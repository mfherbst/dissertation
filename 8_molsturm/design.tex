 \section{Program Design}
\label{sec:MolsturmDesign}
\todoil{Not yet reworked \ldots probably a lot has been mentioned previously}

\todo[inline,caption={}]{
	\begin{itemize}
		\item Make sure it comes across, that we only discuss HF, but
			everything is applicable to Kohn-Sham SCFs as well
			due to their similarity
	\end{itemize}
}
% ----

lazyten layer
An additional advantage of such a layer would be that
modifications of the system matrix can be transparently expressed
to the iterative schemes.
For example adding extra terms to the Fock matrix
describing an electric field or a polarisable embedding
or other contributions does not really require
any change to the \SCF code at all.
A good layer of abstraction between the actual \SCF algorithm
and the implementation of the matrix-vector contractions
is therefore highly desirable to enable flexibility on both sides.

need flexibility in matrix expressions in real-world application
like \SCF (extra terms of varying complexity from external field,
polarisable embedding, fragment approaches, and so on)
non-linearity of Coulomb and Exchange
would be nice to transparently express this

% ----

\lazyten is the linear algebra interface of \molsturm.
It not only implements the lazy matrix datastructures,
which define the common interface of \gint and \gscf,
but further contains code to make standard external
iterative or direct solver implementations available
from a lazy matrices-based setting.

Similarly the eigensolvers can be accessed from \gscf via a common interface,
which abstracts the precise algorithm and allows \lazyten to make an automatic
choice by looking at the precise structure of the Fock matrix at hand.

\todoil{Lazyten common interface for different numerical structure
depending on basis function type}
\todoil{Automatic choice by introspection or alternatively
	supply keywords to influence solver algortihms directly}

% ----


%JA: General criticism: 
%     - Content is good, but a bit verbose (by design, but now we need to condense).
%     - Needs much better separation of "why?" (design goals) and "how" (actual structure).
%     - Also needs more "how". 
%     - Finally, we need to structure the messages/stories: currently, everything is 
%       a bit mixed together. 
% I've copied the original section to 2_desing-mfh.tex and will be working directly with
% the present file.

\subsection{Design Goals}
\label{sec:design-goals}

 % - Small, flexible, low code-complexity

 % - Easy to extend: experimentation with different types of basis
 %   functions (examples); discretization. => SCF and above should not
 %   need to know about shapes of basis functions

 % - Easy to interface with existing quantum chemistry software.
 %   There should be no ``glorified ecosystem'' that users need to buy in to
 %   and that locks them in. Instead it should be small, modular, and limited 
 %   in scope.

 % - Enables rapid development: Especially in the early stages of developing
 %   a new kind of quantum-chemical method it is often not very clear
 %   how it will perform and what methods are required numerically
 %
 % - Interfaces should be well-documented and open source such that
 %   it is easy to integrate with it.

% The main problem that \molsturm aims to solve is to remove the
% difficulty in implementing new types of basis functions and new
% discretizations of $N$-electron Hilbert spaces, and to make it
% possible to easily try out new computational methods in quantum
% chemistry. This requires

%
% Moved from introduction
% Section about automated visualisation and analysis
% which is right now only mentioned in a single sentence
% Perhaps we should not mention this in this paper very much
% but only hint at this in the discussion or the outlook.
% I feel otherwise it becomes too much. This is why I also
% left it out from the introduction section largely (mfh)
%
% TODO Ideas of Tobias Setzer:
% Typical programs are ASCII-in -> ASCII-out
% An idea going in a similar direction is for example TURBOMOLE
%   -> apparently they have very modular modules, where the state of a compuation
%      is incoded in the file on a harddrive
%   -> issue is getting out of the TURBOMOLE ecosystem
% Actual analysis, which is not part of the program package is compartively difficult.
% Especially the transfer of data from one package to another is extremely tough.
%
% Molsturm:
%     fully dynamic, real-time analysis
%     future of molsturm: overcome software package boundaries
%     Journals: JCTC, JCP  (siehe turbomole jungs)
%
% Overall: Focus more on the advantages of the package architecture

\todoil{This paragraph is about the user perspective. Make this more clear}
A large part of the everyday work in quantum chemistry
is the analysis of data which is generated by quantum-chemical programs.
Naturally when investigating a research question it is often
hard to tell what angle is needed to explain what is going on
before doing any preliminary analysis.
In other words the process of understanding what is going on is iterative
and usually supporting calculations and further modelling
needs to be done once more about the problem is known.

\todoil{Reword. This is not the best phrasing}
Therefore a flexible quantum-chemical framework allows to
easily amend calculations which have already been done
at a later stage, making maximal use of what is already known.
Surely in many cases where a more accurate method (e.g. CCSD) is employed
on top of preliminary results (e.g. MP2),
the cost of re-computing the preliminary result is negligible.
In some cases, however,
where the further investigation just involves obtaining
extra quantities like some density plots or similar,
it is rather unnecessary to run the same calculation again,
just because some parameters to request such properties
have been forgotten in the first instance.
Many quantum chemistry packages therefore allow
some mechanism to reuse previous results.
In some cases these mechanism can be rather inconvenient, however.

Furthermore the analysis of obtained data should be easily scriptable.
Most quantum chemistry programs produce human-readable plain-text output
for this very reason.
By the means of standard unix tools like \texttt{grep}, \texttt{awk},
\texttt{bash} or \python or similar those files are then parsed and
the data post-processed.
An alternative and in our opinion more advantageous approach
would be to instead offer a scriptable interface to control
the program package by itself,
along with some utility functions to print summaries
or plot results interactively.
This way results are fully available for a user to analyse
and he himself can decide what information to look at
and what not.
By the means of utility functions
archiving results as well as standard analysis
like printing summary information or plotting some orbitals or densities
should be easy.

Note that on the one hand gives the user much more flexiblity and control
what to look at,
but on the other hand still does not prevent the traditional
mechanism of converting the results into a human-readable output file
by the means of a simple wrapper script.

Especially the ability to archive a large portion of the calculation
furthermore allows to exploit previous results when performing
further calculations as well as delaying the analysis
to a later point when \eg more knowledge about the
problem has been obtained to decide what exactly to look at.
%
% End moved from introduction
%



\subsection{Program structure}
\label{sec:program-structure}


\begin{figure}
	\includeimage{8_molsturm/molsturm_structure}
	\caption{Structure of the \molsturm package: Shown are the five modules of the package,
	along with the set of integrals accessible from \gint and the set of post-HF method,
	which can be used from \molsturm. The greyed-out parts are not yet implemented.
	Only the modules inside the red box are part of \molsturm. The blue boxes are all external packages.}
	\label{fig:structure}
\end{figure}

%# Molsturm structure
%  - Overview of the molsturm package
%  -- Nice molsturm-figure from talk
%  -- How does everything fit together? Rationale and design.
%  -- ...
%
%  - Rationale for and design of the integral library
%  -- How have we achieved basis function independence?
%  --- Different types of basis functions: what differs, what is the same? How to put them under the same roof.
%  --- [All the other design choices]
% 
%
% in this section: Top down bird eye view

% Unlike most existing quantum-chemistry packages
% \molsturm (see \ref{fig:structure}) is not tailoring
% a specific type of basis function.
% Even though this certainly requires
% a certain flexibility with respect to the algorithms
% and program flow,
% the very basic structure of a typical quantum chemical
% calculation is pretty much independent
% of the type of basis function used.


% Typically one first performs a self-consistent field calculation,
% either by solving the Hartree-Fock or the Kohn-Sham equations.
% The obtained self-consistent solution is then used for example in
% linear response calculations or Post-HF methods.
% It is important to note that these steps
% typically do not need any knowledge of the underlying basis function type
% at all, since they are formulated in terms of SCF orbitals only.
% The \molsturm program package therefore concentrates
% on a basis-function independent SCF code
% with interfaces to simplify using the resulting SCF solution
% in third-party code for further processing.

% As can be seen in fig. \ref{fig:structure} this is realised
% by a modular design,
% where each of the five major modules has a very
% distinct responsibility.
% Most importantly only the integral module \gint
% exploits the properties of the type of basis functions.
% \gint for example decides the sparsity structure of the
% repulsion integrals or the Fock matrix.
% This structure is encoded inside the lazy matrices,
% which \gint exposes to the SCF algorithms inside \gscf.

% From the point of view of \gscf,
% on the other hand,
% the lazy matrices for all types of basis functions look alike.
% In other words by the means of the lazy matrices
% the algorithms in \gscf
% can be written in such a high-level manor,
% that they are entirely independent of the precise matrix structure.
% Consequently they are independent of the choice of basis function type as well.

% The lower layers \lazyten and \krims provide
% the lazy matrix implementation as well as other utilities,
% which are needed to pass information between the individual modules.
% \lazyten furthermore contains an interface,
% which allows lazy-matrix based algorithms like the ones in \gscf to
% call standard iterative solver implementations.

% The next sections describe the individual modules of the \molsturm
% package in greater detail together with their interaction
% with external packages.

%
% Move from introduction
%
The \python interface of molsturm for example is deliberately designed
with simplicity in mind.
The downside is that many potential symmetries when storing the data
cannot be exploited,
but on the upside we managed to integrate \molsturm
with some third-party Post-HF libraries rather quickly.
For example the FCI interface to \pyscf was realised in only two days,
but still is general enough to work for Sturmians GTOs and theoretically
all basis function types which are implemented in our integral backend.
Similarly the implementation of a CCD code, as mentioned in the example section,
only took a couple of hours.

The careful reader might have noticed that a well-designed interface
might not only facilitate rapid development of new algorithms
but also prevent one from the need to re-implement the wheel
and much rather exploit the code which has already has been written
for one's purposes.
\todoil{This point that we do not need to re-implement everthing because of interfacing
	and that if we need to reimplement it's quick is really important,
	because many people have mentioned this to me, when I talked about molsturm.
	So something like: Your idea of this flexible framework is great, but then
	you kind of have to start from scratch with everything and can't really try
	out the new methods very much because hardly anything is there.
}

%
% End move from introduction
%
% phd should not spend a year to implement something which might not work
% need flexiblity to try things
% code should be easy and close to the physical formulae
%
% We no not want to re-implement the wheel -> integrate with what exists

%
% ----------
%
\subsection{\molsturm interface layer}

%JA: A note: I think this section should be more a *description* than a *justification*.
%    This is a "meat" section, which the reader will likely read *seeking information*
%    rather than *seeking to be convinced*. After the first pass, I will try to gather 
%    most of the "why" in the top subsection ("Design Goals") or if it's a bit more detailed,
%    the second ("Program Structure"). A little bit of "this is good, because" is OK, but
%    we have to keep the reader -- and what xe is looking for -- in mind.

The topmost \molsturm layer handles calculation set-up 
(e.g., definition of molecules, basis set choice, choice 
of SCF-calculation scheme, et cetera), as well as interfacing 
to the outside world. \molsturm can be controlled in two ways: 
directly as a \cpp library, which is most convenient if it is to 
be called from within a quantum chemistry code written in \cee, \cpp, 
or \fortran; or it can be controlled through a \python interface.

We purposefully do not define a new ``input format'': calculations are 
controlled more cleanly and flexible by simply using
\python scripts (or calling it directly from a host quantum 
chemistry package) to drive calculation.
By calling simple interface functions, an SCF calculation can be set
up, and the appropriate integrals passed on to Post-HF calculations
if desired. The next section shows examples of how this is done.

%JA: The below four paragraphs basically expand on the above. 
%    Can be included, shortened, or cut according to how much
%    space we're left with later.
Compared to the traditional input file approach this has
a couple of advantages:
%
Firstly it makes it easy to interactively work with \molsturm
%JA: Feedback loop?
from a shell. This reduces the feedback loop for small calculations
or during debugging.  If one uses \molsturm from a Jupyter notebook \cite{Jupyter},
one can even perform calculations and view plots interactively from within a web browser.
%
Secondly this gives the user full flexibility
of how to embed \molsturm in his or her workflow.
From the \python interface, all data from \molsturm is available
in plain \numpy arrays, so the user can easily do
any kind of standard processing from \python
after or during a calculation, for example
automatising repetitive analysis work, or repetitive calculations,
as shown in the dissociation-curve example in Section~\ref{sec:ex:data}.
%
Thirdly, because input and output tensors can be accessed as simple
\numpy arrays, or as simple data structures or flat memory from \cpp,
and to experiment with development of new Post-HF methods.
In Section \ref{sec:ex:ccd}, we show a simple coupled cluster CCD
implemented on top of \molsturm in 130 lines of \numpy code.
Similarly, the example in Section~\ref{sec:ex:geo} shows a small
\python script that performs a geometry optimization of water 
numerically in a few lines of code, without needing SCF gradients 
to be implemented already.

%JA: Somehow, these three paragraphs seem like they could be stated
%    in a couple of sentences. Will try my hand at it.
By breaking with the traditional paradigm of plaintext-in, plaintext-out,
which most quantum chemistry programs still employ up to now,
we furthermore gain the flexibility we need to exchange data with
other third-party packages, for example for Post-HF caluclations.
Most packages common in quantum chemistry either use data which is
dumped to disk in the form of plain text or binary files
in order to store their intermediate or final results of
a calculations.
In many cases the programs are not very flexible with regards to
how the data is stored in these files and one hence needs to reproduce
the file format very carefully for interfacing.
In cases where the documentation of these file formats is sparse
or absent,
one might even need to reverse-engeneer in order to work with these programs.
This requires a lot of flexiblity on the calling side.
Luckily \python offers a large range of functions to serialise \numpy
arrays in text or binary form.

Furthermore \numpy arrays have become the \textit{de facto} standard
for storing and manipulating matrices or tensors in \python.
All third-party quantum chemistry codes which
offer a \python interface
either use \numpy arrays themselves or
provide utility functions to convert to and
from \numpy arrays.
\todoil{check if this is true and cite them}
The same is true for typical \python packages used for plotting or
data manipulation (matplotlib, scipy, pandas).

Thirdly by the means of interface generators
like SWIG \todo{cite} \numpy arrays can be converted to plain
\cee arrays for calling \cpp, \cee or \fortran code,
such that even low-level libraries can be employed
from \numpy arrays.

%
% ----------
%
\subsection{The integral interface via \gint}
Similar to the \molsturm interface layer when it comes to Post-HF methods,
\gint does not implement any routine for computing the integrals.
Instead it just acts as a broker,
which presents a common interface for all available basis function types
and third-party integral backend libraries.

When a calculation starts,
\molsturm passes \gint the set of parameters,
which are relevant for the selection and setup of the integral backend,
like for example the Coulomb-Sturmian exponent or the Gaussian basis set information.
In return \gint provides \molsturm with a collection of lazy matrices,
where each lazy matrix represents a single integral for this
type of basis functions.
For example the nuclear attraction matrix, the kinetic operator matrix,
the coulomb matrix and the exchange matrix each exist as separate
lazy matrix objects.
These integral objects may then be combined
in an arbitrary manor to form the Fock matrix or Kohn-Sham matrix
expression by just forming linear combinations.
The resulting Fock matrix expression is then typically passed onto \gscf,
where the SCF problem is solved.

The generality of the lazy matrices allows the interface of \gint
to be entirely independent of the structure
which is used to represent the integral data internally.
This implies that all code inside \molsturm,
which works with the integral objects,
hence becomes independent of the way the integral data is represented
or how the evaluation of the \contraction function is actually performed.
\gint may freely choose the scheme for evaluating a matrix-vector
product or for example the sparsity structure which is used to
represent the data of the integral matrix
without this having any impact towards other parts of \molsturm.

Using lazy matrices furthermore has an advantage for
dealing with the non-linearity of the coulomb and exchange matrices:
The lazy matrix formalism of \lazyten provides the possibility
to implement a special \update function,
which can be used to update internal state of a lazy matrix.
The coulomb and exchange lazy matrices
may use this function to retrieve a new set of coefficients from an
SCF algorithm and update
their internal state accordingly.
As we will discuss in the next section \gscf makes frequent use of this feature.

Currently \gint only offers two types of basis functions,
namely Gaussian integrals and atomic Coulomb-Sturmians,
both available in at least two different implementations.
For example Gaussian integrals for \gint are available from either one of 
\libint by Valeev et al.\cite{Libint2,Libint2_231}
or \libcint by Sun et al.\todo{cite} 
under a common interface.
The design of \gint makes it furthermore very simple to extend the types of integrals
available in \molsturm:
The function calls which compute the integral values only need to be wrapped
inside lazy matrix objects and the resulting integral collection needs to be registered
with \gint.
For example the support for \libcint was added in just two days of work.

Note that this does not even require changing a single line of code inside \gint.
One could achieve this by linking the library into \molsturm at compile time.

%
% ----------
%
\subsection{Design of SCF algorithms in \gscf}
\label{sec:gscf}

\todoil{Fock matrix $\Leftrightarrow$ Kohn-Sham matrix $\Leftrightarrow$ Problem matrix
to make clear that \gscf is not only for HF problems }

%
% Move from introduction
%
Conceptionally an SCF algorithm is entirely independent of the type of basis function.
One could think of it very much as a numerical technique to solve
an non-linear eigenvalue problem by linearisation.
%
% End move from introduction
%


In \gscf all self-consistent field algorithms are assumed to be
iterative processes alternating between two basic steps.
Firstly an \term{Fock-update} step where a new Fock matrix is produced from a current set of
orbital coefficients.
Secondly a \term{coefficient-update} step where the current Fock matrix guess
is used to update the current set of orbital coefficients.
Let us briefly rationalise this choice by 
looking at the two mathematically equivalent
\todo{cite}
ways a
Hartree-Fock or Kohn-Sham self-consistent field problem is typically
viewed in order to solve it.

\begin{figure}
	\centering
	%\input{img/scf_schemes.tex}
	\missingfigure{Where the SCF schemes would have been}
	\caption{Schematic overview of a few exemplary
		self-consistent field~(SCF) algorithms.
		In each case the Fock-update step is highlighted in pale blue
		and the coefficient-update in pale red.
		For further details regarding the algorithms see references
		\cite{Voorhis2002,Roothaan1951,Cances2000a}, respectively.
	}
	\label{fig:SCFs}
\end{figure}

The first viewpoint thinks of it as the variational minimisation
of the ground
state SCF energy with respect to the orbital coefficients making up
the best single Slater determinant approximation to the ground state.
An example would be the Geometric direct minimisation algorithm~(see \fig \ref{fig:SCFs}a).
In these algorithms the Fock matrix occurs in the gradient expression
\todoil{Cite Cances paper and Head-Gordon paper}
of the energy with respect to the coefficients.
An iterative optimisation procedure hence
usually involves the computation of the Fock matrix~(Fock update)
in each iteration in order to determine the search direction
for the next step.
This direction then leads to an updated set of coefficients.

The second viewpoint considers the non-linear Eigenproblem
which results from the aforementioned optimisation problem.
This is solved by linearisation,
i.e. one assumes a guess for the coefficients
in order to build the Fock matrix~(Fock update)
followed by a diagonalisation of this very Fock matrix
in order to obtain new update coefficients~(coefficient update).
This is repeated iteratively until self-consistency is reached.
An example is Roothaan's repeated diagonalisation algorithm~(see fig. \ref{fig:SCFs}b).

In both cases the typical iterative algorithms used for
solving optimisation problems or diagonalisation
can be formulated in a way that they are fully \contraction-based,
i.e. only require matrix-vector products between the Fock matrix
and other iterated vectors.
In \gscf we therefore require only two things from any given Fock matrix:
Firstly a special \update function which updates the internal state of
the matrix from the current set of coefficients
during the Fock-update step of an algorithm.
Secondly the aforementioned \contraction function for the
required matrix-vector products
during the coefficient-update step.

This is exactly the interface offered by the lazy matrix integral objects
which are returned by the \gint integral library.
We can therefore simply use the \gint integral terms to build the appropriate expressions
of the Fock or Kohn-Sham problem matrices
and hand them over to \gscf for finding the self-consistent set of SCF coefficients.
In \gscf itself the SCF algorithms are only implemented in the
high-level language of the lazy-matrix formalism.
Since the lazy matrix interface fully abstracts the internal structure of the matrix
the SCF algorithms of \gscf
are completely agnostic to the structure of the integrals and the Fock matrix,
which may be freely decided by the integral backend of \gint
depending on what suits the particular type of basis function best.

In the discussion about \gscf so far we have focused only on
\term{coefficient-based} SCF algorithms,
i.e. algorithms where the orbital coefficients are iterated to self-consistency.
Yet another way to think about the SCF procedure is to iterate the
density matrix to self-consistency instead.
Mathematically such \term{density-matrix-based} algorithms
are just solving the SCF problem in a different parametrisation
compared to the \term{coefficient-based} algorithms
and are otherwise equivalent.
\todoil{cite papers from Eric and others about this.}
An example for a density-matrix-based algorithm is the optimal damping algorithm
by Eric Cancès and Claude leBris,
\todoil{Cite}
which is shown in \fig \ref{fig:SCFs}c.

Currently all algorithms which are implemented in \gscf are coefficient-based SCF algorithms.
This is not strictly speaking required by the library,
but it has the advantage that for some basis function types
the computational scaling of the \contraction-function
of the Fock matrix expression can be reduced by an order of magnitude.
An example are the Coulomb-Sturmians.
\todoil{Maybe go into this a little in the previous sections and refer up}
Furthermore the density-matrix based algorithms
we have encountered so far could be reformulated as coefficient-based schemes
or were easily approximated to work as such.
For example we have implemented a variant of the original ODA,
which is suitable for a coefficient-based setting by truncating
the history of SCF steps considered in order to compute the optimal damping.

Compared to a scheme where the Fock matrix is held fully in memory
the \contraction-based design has some disadvantages, too.
For example our implementation of Pulay's DIIS conversion accelerator
cannot always form the linear combination of Fock matrices by adding the
individual Fock terms in memory.
Instead the linear combination stays around as a lazy matrix expression
until it is applied to a trial vector.
When this happens during an iterative diagonalisation algorithm
we hence need to call the \contraction-function for each of the
Fock terms separately instead of only once.
This is a trade-off we are happy to pay since it allows to
reduce the computational scaling of the individual \contraction
calls by an order of magnitude as mentioned before.

%
% ----------
%
\subsection{\lazyten: \contraction-based linear algebra backend}
%
% begin move from introduction
%
\todoil{Maybe illustrate the points mentioned here using the coulomb and exchange matrices as actual examples}

% TODO a good paragraph for this part:
Whenever operations are done on a matrix \lazyten does not automatically
evaluate them in all cases.
Much rather it usually just builds up a datastructure,
which keeps track of the operations which \textit{should} be done
at some point in the future.
Whenever an actual call to the \contraction-function happens,
the expression history is considered and evaluated with respect
to the other arguments supplied by the \contraction call.
% end TODO

\todoil{Maybe this section is too long given that we want to publish on this again}

As mentioned above for some basis function types it is advantageous
to not build the coulomb and exchange matrices in memory,
but instead only use them in the form of matrix-vector applications.
Since iterative solvers like Krylov-subspace based methods or Davidson's algorithm
only need the matrix-vector product in order to find a few of the eigenvalues
of a particular matrix,
this is not an issue with respect to the eigenproblem which needs to be solved
as part of the SCF.
Note, that this approach is furthermore not new in quantum chemistry at all.
Other examples where one typically avoids to place the matrix to diagonalise
into memory are the configuration interaction~(CI) matrix
or the algebraic-diagrammatic construction~(ADC) matrix to name a few.

We shall refer to algorithms which focus on implementing a matrix-vector product
instead of the full matrix as \contraction-based algorithms.

The latter name is supposed to indicate that such a scheme is in principle
not only restricted to the case of multiplying a matrix with a vector,
much rather general tensor contractions could be expressed implicitly
by the means of a function computing the contraction rather than
by explicitly performing the contraction form tensor elements
which reside in memory.

Note that a disadvantage of \contraction-based algorithms is that the expressions
computing the tensor contraction can become rather complicated.
Furthermore it usually is more intuitive to think of the
numerical modelling in terms of tensors, matrices and vectors
rather than \contraction functions.
For this purpose we generalise the concept of a matrix
to so-called \term{lazy matrix} objects.
Whilst a normal or \term{stored matrix} is dense and has all its elements
residing in memory,
a lazy matrix is more general.
It may follow a particular sparse storage scheme
like compressed-row storage
or even more general all its elements may just be arbitrary expressions.
The values of lazy matrices are hence only computed upon request
or whenever a contraction with another object is required.
As such the lazy matrix may have, \ie a special \update function
may be used to modify the expression of the lazy matrix
at a later point.
Note, however, that special storage schemes for storing
sparse matrices are similarly just lazy matrices 
\todoil{
	Updating lazy matrices is a bit like reactive programming
	\url{https://en.wikipedia.org/wiki/Reactive_programming}.
	In fact one could use them to get reactive programming
	into C++ in a simple way.
	Mention this here (or in the \lazyten paper)
}

This on the one hand makes obtaining the lazy matrix elements
expensive, but on the other hands gives a nice matrix-like
interface to more complicated objects.
All evaluation between lazy matrices
(i.e. addition or matrix-matrix multiplication)
is usually delayed until for example a contraction of the resulting
expression with a vector is performed.
One typically refers to this strategy as \term{lazy evaluation}.

Since lazy matrices are a straight generalisation
of usual matrices,
all algorithms written in terms of lazy matrices
are at the same time applicable to dense matrices,
sparse matrices or special matrices like in the case of Sturmians
where a lazy evaluation scheme is needed.
So high-level code written in terms of lazy matrices
does not need to be changed if the low level matrix implementation
is changed from one basis function type to another.
%
% end move from introduction
%



\begin{figure}
	%\resizebox{.5\textwidth}{!}{%
	%\input{img/lazyten_structure.tex}%
	%}
	\missingfigure{Where lazyten would have been}
	\caption{Overview of \lazyten}
	\label{fig:lazyten}
\end{figure}



\lazyten is the linear algebra interface of \molsturm.
It not only implements the lazy matrix datastructures,
which define the common interface of \gint and \gscf,
but further contains code to make standard external
iterative or direct solver implementations available
from a lazy matrices-based setting.

As can be seen in figure \ref{fig:lazyten} \lazyten provides
a common interface for the \contraction-based algorithms in \gscf,
regardless of the linear algebra~(LA) backend or the solver implementation,
which is used to solve a particular problem.
Lazy matrices like the integrals from \gint as well as built-in structures
like for example a lazy matrix representing the inverse of a matrix
can be used transparently and even in combination with stored matrices
by the means of the automatic bookkeeping done by \lazyten.
Whenever a call to for example the \contraction function enforces
evaluation of a lazy matrix expression,
\lazyten is flexible with respect to the LA backend which is used for
this step as well.
Both armadillo as a LAPACK-based backend as well as Bohrium as a array-operation based backend
are currently available and by recompiling \molsturm with the appropriate configure options,
the backend can be switched.

When it comes to the choice of the solver algorithms, \lazyten is even more flexible.
All algorithms for solving linear problems or eigenproblems are available as 
convenient high-level interface from \gscf.
For example a linear problem $A x = b$ with a lazy matrix $A$, known right-hand side $b$
and unknown $x$ can either be solved by a call to a \texttt{solve} function or by
the means of calling \texttt{inverse(A)} on $A$ and then subsequently
multiplying the inverse matrix with $b$, quite literally coding it as $x = A^{-1} b$.
In both cases \lazyten will perform some introspection regarding the properties of
the lazy matrix expression behind $A$ and consider the number of solution
vectors as well as the required accuracy in order to automatically determine the algorithm to use
for solving the linear problem at hand.
By the means of keyword arguments the user can influence or override the automatic
choice made by \lazyten.

Similarly the eigensolvers can be accessed from \gscf via a common interface,
which abstracts the precise algorithm and allows \lazyten to make an automatic
choice by looking at the precise structure of the Fock matrix at hand.
If all eigenpairs are desired or the matrix is stored in memory anyway,
\lazyten will favour the dense eigensolver algorithms from LAPACK,
whereas if only few eigenpairs are desired,
Arnoldi methods from ARPACK~\cite{Lehoucq1998} are used.
In either case the precise algorithm can also me chosen from the \python interface
of \molsturm by supplying an appropriate keyword string.

%  - L10 (but short-ish, because there will be a full paper)
%  -- Idea: Write high-level formula similar to the equations on paper, generate efficient
%     streamed/lazy tensor-code.
%     (currently only supports matrices/linear operators and vectors).
%  -- Transparent combination of dense, sparse, and matrix-free
%  -- Solver-switching on the fly
%  -- How is multi-backend support solved
%  -- Design towards automatic generation of efficient (Only simple evaluation currently supported)
%  -- Include simplified L10-figure from talk

%
% ----------
%
\subsection{The \molsturm test suite}
A very important subsidiary to good software design is a flexible testing framework.
A test suite which is simple to execute, fast and easy to expand not only allows to verify
that the current status of a piece of software is correct,
but it also allows to verify that all future changes do not break anything.
This includes of cause the potential adaption of the design in the future,
which might involve changing any of the already existing interfaces.
A strong test suite aids with this procedure,
since one can perform these changes in a sequence of many small steps,
verifying the correctness of the software on the way.

For this reason \molsturm comes with an extensive test framework
with roughly four types of tests.
Firstly there are \term{unit tests},
which test the functionality of a single function or code unit in a couple of hard-coded examples.
Further we have \term{functionality tests},
which test a larger portion of code and are meant to ensure that
the results of \molsturm agree between different versions.
Thirdly the \term{reference tests} compare
the results of \molsturm to other quantum-chemistry packages.
Especially in \lazyten we furthermore make use of yet another type of tests,
namely so-called \term{property-based tests}.
This testing technique uses the expected properties of a code unit
to randomly generate test cases and to verify the result.
On failure the generated test cases are simplified until the most simple,
failing test case is found.
This is really helpful for debugging and in order to reduce the human aspect in testing.

Next to combining various different types of tests
other key principles in the design of our test suite were that our tests should be simple
to execute and take only a couple of minutes.
This not only makes it easier to motivate others to use the tests, too,
but it also allows to fully automate the whole testing process as well.
\todoil{I feel we should explicitly mention github and Travis-CI to show them some credit for this awesome work.}
In our case any commit to our repositories starts up a couple of virtual machines,
which build \molsturm and run the test suite for a couple of common compilers and
build configurations.
A commit into the stable source code branch is only accepted if the test suite passes.

In our test suite even the work of adding a new test is somewhat automated.
For example in order to add a new reference test for a new feature
or a case where a bug was discovered,
one simply adds a small configuration file to a special directory,
which defines the testing parameters.
A \python script, which is part of the test suite,
then automatically executes the third party program and extracts the reference results in a format,
which is understood by the \molsturm test suite.
When the test suite is executed all such reference results are read and
\molsturm is executed and tested for each of them.

\todoil{Mention test coverage ???}

\todoil{We probably should not keep this next paragraphs here}
On the side of the \molsturm code we employ \term{assertive programming} to assure that
\molsturm crashes with a very detailed backlog whenever an unexpected behaviour happened.
By the means of a dual build system setup, we achieve that a Debug version
(including heavy assertive checks) and a Release version
(without heavy assertive checks) are compiled side-by-side.
This allows to use the Debug version with a lot of assertions,
potentially even in the inner loops,
to be used for trial runs and the Release version with less assertions for productive runs.
Switching between Debug and Release from \molsturm's python interface is very simple and
only requires to recompile a few files,
since in normal build conditions Release and Debug have already been built.

By the means of checkpointing one could further use the Release version to get an erroneous
part of \molsturm and the Debug version to step-by-step test what goes wrong.
The detailed traceback on failure of an assertion can then be used to analyse the problem quickly.
% end TODO

% correctness testing(continuous integration)
% unit testing
% property-based testing ???

%\krims is a tiny utility package which is used by all parts of \molsturm.
%It provides a very handy exception system for error handling.
%By overriding the terminate handler in a way that a very verbose backtrace
%is produced, a developer of \molsturm becomes 
% Parameter map

%% High-productivity
% Exception system / error handling
% Build system

%% Finding bugs with checkpointing


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "paper"
%%% End:
