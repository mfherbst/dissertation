\section{Lazy matrices}
\label{sec:lazymat}
The idea of lazy matrices is to encapsulate the coding \contraction-based
in a domain-specific language,
which makes it feel as if one was dealing with actual matrices
instead of \contraction expressions.
Even though not all complications can be hidden,
the resulting syntax allows to write algorithms in a high-level manner
being agnostic to the underlying implementation of the \contraction expression.
This will turn out to be the key aspect
leading to the basis-type of the quantum-chemistry package \molsturm.

For this purpose we generalise the concept of a matrix
to objects we call a \newterm{lazy matrix}.
Whilst a conventional or \newterm{stored matrix} is dense
and has all its elements residing in a continuous chunk of memory,
this restriction does no longer hold for a lazy matrix.
It may for example follow a particular sparse storage scheme
like compressed-row storage,
but does not even need to be associated to any kind of storage at all.
In the most general sense it can be thought of as an
arbitrary \contraction expression for computing the matrix elements,
which is dressed to look like an ordinary matrix from the outside.

In other words one may still obtain individual matrix elements,
add, subtract or multiply such lazy matrix objects together
or apply them to a bunch of vectors or a stored matrix.
Not all of these operations may be equally fast
than there counterparts on stored matrices, however.
Most importantly obtaining individual elements of such a matrix
can become rather costly,
since they might involve a computation as well
and not just a lookup into memory.

On the upside one gains a much more flexible data structure
where a familiar matrix-like interface
can be added to more complicated objects.
Most notably a lazy matrix may well be non-linear or can have state,
which may be changed by a \update function
in order to influence the represented values at a later point.
An example where this is sensible would be the Coulomb and Exchange matrices,
where the values of these matrices depend on the set of occupied coefficients,
which have been obtained in the previous iterations.
Other examples include the \update of an accuracy threshold for a \contraction expression,
which might change between iterations.

\begin{figure}
	\centering
	\includeimage{6_contraction_lazy_matrices/expression_tree} \\[0.8em]
	\includeimage{6_contraction_lazy_matrices/expression_tree2}
	\caption[Examples for lazy matrix expression trees]
	{
		Examples for lazy matrix expression trees.
		The upper represents the instruction
		$\mat{D} = \mat{A} + \mat{B}$
		and the lower the multiplication of the result $\mat{D}$
		with $\mat{C}$.
	}
	\label{fig:LazyMatrixExpressionTree}
\end{figure}
All evaluation between lazy matrices
like addition, subtraction or matrix-matrix multiplication
is usually delayed until a contraction of the resulting
expression with a vector or a stored matrix is performed
and thus the represented values are unavoidably needed.
This evaluation strategy is called \newterm{lazy evaluation}
in programming language theory~\todo{cite https://dl.acm.org/citation.cfm?doid=72551.72554}, explaining the name of these data structures.
To make this more clear consider the following instructions
\begin{align*}
	\mat{D} &= \mat{A} + \mat{B}, \\
	\mat{E} &= \mat{D} \mat{C} \\
\intertext{followed by}
	\vec{y} &= \mat{E} \vec{x}
\end{align*}
where $\mat{A}$, $\mat{B}$ and $\mat{C}$ are lazy matrices
and $\vec{x}$ is a vector stored in memory.
The first two instructions do not give rise to any computation being done,
they only amount to build an extended expression tree in the resulting
lazy matrix $\mat{E}$, see figure \vref{fig:LazyMatrixExpressionTree}.
The last line instructs the formation of a matrix-vector product.
Now the complete expression tree needs to be worked upon in appropriate order
to return the resulting vector $\vec{y}$.
This implies that
\[ \mat{E} \vec{x} = \left( \mat{A} + \mat{B} \right) \mat{C} \vec{x} \]
is evaluated in steps.
First $\tilde{\vec{x}} \equiv \mat{C} \vec{x}$ is formed,
by evaluating the matrix-vector-product expression of the lazy matrix
$\vec{C}$,
then $\mat{A} \tilde{\vec{x}}$ and $\mat{B} \tilde{\vec{x}}$ is formed
by considering the matrix-vector-product expressions of $\mat{A}$
and $\mat{B}$ and finally the result is added to give $\vec{y}$.






Let us introduce the terminology of a \newterm{primary lazy matrix expression},
which is a lazy matrix
that is directly associated with a \contraction expression for their values,
\ie where the expression tree only consists of one node.







% TODO Maybe have this paragraph in a later context after lazyten is introduced
%      to make reader more aware of the role lazyten plays in molsturm
Since lazy matrices are a straight generalisation of usual matrices,
all algorithms written in terms of lazy matrices
are at the same time applicable to dense matrices,
sparse matrices or special matrices like in the case of Sturmians
where a lazy evaluation scheme is needed.
In the context of an \SCF this implies that high-level code
written in terms of lazy matrices
does not need to be changed if the low level matrix implementation
is changed from one basis function type to another.





introduce lazyten (statistics: number of lines of code)
basic ideas behind lazyten / structures
examples (davidson, exchange / coulomb <-- update function)
-> show that it is more intuitive to think of \contraction expressions as lazy matrices

advantage for scf and quantum chemistry
hint at lazy tensors?
transition to molsturm



% ---
the \SCF algorithm
and the implementation of the matrix-vector \contraction expressions.
An additional advantage of such a layer would be that
modifications of the system matrix can be transparently expressed
to the iterative schemes.
For example adding extra terms to the Fock matrix
describing an electric field or a polarisable embedding
or other contributions does not really require
any change to the \SCF code at all.
A good layer of abstraction between the actual \SCF algorithm
and the implementation of the matrix-vector contractions
is therefore highly desirable to enable flexibility on both sides.

need flexibility in matrix expressions in real-world application
like \SCF (extra terms of varying complexity from external field,
polarisable embedding, fragment approaches, and so on)
non-linearity of Coulomb and Exchange
would be nice to transparently express this

%----

Additionally for these more complicated cases
it is not exactly clear what the best answer will be,
such that the flexibility to experiment with different aspects is key.
Another aspect, which is often forgotten,
but strongly emphasises the need for flexibility
is that hardware frequently changes.
So the best implementation to the current state of hardware,
will be outdated in a few years time,
calling for frequent changes to the implemented \contraction expression.

Such changes should not influence the outer iterative process
like the iterative diagonalisation or the actual \SCF algorithm,
since not that much effort has to be put into these parts
of the program for optimisation
and thus these are not that much influenced by the hardware changes for example.
This calls for a good layer of abstraction between the
iterative routine and the implementation of the matrix-vector \contraction expressions.

Hard to think about problem in terms of expressions.

% ---


Lazy matrices can deal with the non-linearity of Coulomb and exchange
and it makes it easy to add terms (maybe show example ??)

Let $\mat{U}$ and $\mat{A}$ be lazy matrices and $\vec{x}$ be a normal vector.
We wish to compute the expression
\[ (\mat{I} - \mat{U} \tp{\mat{U}}) \mat{A} (\mat{I} - \mat{U} \tp{\mat{U}}) \vec{x} \]
which occurs in Jacobi-Davidson procedure.
A typical code is
\begin{lstlisting}
DiagonalMatrix<matrix_type> I(std::vector<scalar_type>(100, 1));
auto projector = I - U * transpose(U);
auto mat = projector * A * projector;
auto res = mat * x
\end{lstlisting}


% See for example slide 15 of iwr school talk
% Code example
% Resulting expression tree
% Evaluation order


\todoil{Maybe illustrate the points mentioned here using the coulomb and exchange matrices as actual examples}

% TODO a good paragraph for this part:
Whenever operations are done on a matrix \lazyten does not automatically
evaluate them in all cases.
Much rather it usually just builds up a datastructure,
which keeps track of the operations which \textit{should} be done
at some point in the future.
Whenever an actual call to the \contraction-function happens,
the expression history is considered and evaluated with respect
to the other arguments supplied by the \contraction call.
% end TODO




\section{\lazyten lazy matrix library}

\begin{figure}
	\centering
	\includeimage{6_contraction_lazy_matrices/lazyten_structure}
	\caption{Overview of \lazyten}
	\label{fig:structureLazyten}
\end{figure}




\begin{itemize}
	\item Builds up DAG instead of evaluating
	\item Evaluation is done delayed upon \contraction
	
\end{itemize}

Fazit
\begin{itemize}
	\item contraction based algorithms may lead to lower memory footprint and
		hence to improved scaling
	\item Code can be more readable if if proper abstraction is used
	\item Good for teaching and for experimentation
	\item Abstraction between integrals and SCF algorithms
	\item Plug and play integrals libraries (see next chapter)
	\item Basis-type independent SCF / quantum chemistry
\end{itemize}


designed to add lazy matrix support to existing
linear algebra packages.
It effectively provides a layer of abstraction between the code
describing the algorithms


In the previous chapter we already mentioned
how similar ideas can reduce the formal
computational scaling of the \SCF procedure
for certain kinds of discretisations
and additionally could even lead to a basis-function
agnostic \SCF.



\lazyten is the linear algebra interface of \molsturm.
It not only implements the lazy matrix datastructures,
which define the common interface of \gint and \gscf,
but further contains code to make standard external
iterative or direct solver implementations available
from a lazy matrices-based setting.

As can be seen in figure \ref{fig:lazyten} \lazyten provides
a common interface for the \contraction-based algorithms in \gscf,
regardless of the linear algebra~(LA) backend or the solver implementation,
which is used to solve a particular problem.
Lazy matrices like the integrals from \gint as well as built-in structures
like for example a lazy matrix representing the inverse of a matrix
can be used transparently and even in combination with stored matrices
by the means of the automatic bookkeeping done by \lazyten.
Whenever a call to for example the \contraction function enforces
evaluation of a lazy matrix expression,
\lazyten is flexible with respect to the LA backend which is used for
this step as well.
Both armadillo as a LAPACK-based backend as well as Bohrium as a array-operation based backend
are currently available and by recompiling \molsturm with the appropriate configure options,
the backend can be switched.

When it comes to the choice of the solver algorithms, \lazyten is even more flexible.
All algorithms for solving linear problems or eigenproblems are available as 
convenient high-level interface from \gscf.
For example a linear problem $A x = b$ with a lazy matrix $A$, known right-hand side $b$
and unknown $x$ can either be solved by a call to a \texttt{solve} function or by
the means of calling \texttt{inverse(A)} on $A$ and then subsequently
multiplying the inverse matrix with $b$, quite literally coding it as $x = A^{-1} b$.
In both cases \lazyten will perform some introspection regarding the properties of
the lazy matrix expression behind $A$ and consider the number of solution
vectors as well as the required accuracy in order to automatically determine the algorithm to use
for solving the linear problem at hand.
By the means of keyword arguments the user can influence or override the automatic
choice made by \lazyten.

Similarly the eigensolvers can be accessed from \gscf via a common interface,
which abstracts the precise algorithm and allows \lazyten to make an automatic
choice by looking at the precise structure of the Fock matrix at hand.
If all eigenpairs are desired or the matrix is stored in memory anyway,
\lazyten will favour the dense eigensolver algorithms from LAPACK,
whereas if only few eigenpairs are desired,
Krylov-subspace based methods from ARPACK are used.
In either case the precise algorithm can also me chosen from the \python interface
of \molsturm by supplying an appropriate keyword string.


\section{Examples}
An example showing the performance and how we can run things with bohrium as well
