\section{Lazy matrices}
\label{sec:lazymat}
The idea of lazy matrices is to encapsulate the coding \contraction-based
in a domain-specific language,
which makes it feel as if one was dealing with actual matrices
instead of \contraction expressions.
Even though not all complications can be hidden,
the resulting syntax allows to write algorithms in a high-level manner
being agnostic to the underlying implementation of the \contraction expression.
This will turn out to be the key aspect
leading to the basis-type of the quantum-chemistry package \molsturm.

For this purpose we generalise the concept of a matrix
to objects we call a \newterm{lazy matrix}.
Whilst a conventional or \newterm{stored matrix} is dense
and has all its elements residing in a continuous chunk of memory,
this restriction does no longer hold for a lazy matrix.
It may for example follow a particular sparse storage scheme
like compressed-row storage,
but does not even need to be associated to any kind of storage at all.
In the most general sense it can be thought of as an
arbitrary \contraction expression for computing the matrix elements,
which is dressed to look like an ordinary matrix from the outside.

In other words one may still obtain individual matrix elements,
add, subtract or multiply such lazy matrix objects together
or apply them to a bunch of vectors or a stored matrix.
Not all of these operations may be equally fast
than there counterparts on stored matrices, however.
Most importantly obtaining individual elements of such a matrix
can become rather costly,
since they might involve a computation as well
and not just a lookup into memory.

On the upside one gains a much more flexible data structure
where a familiar matrix-like interface
can be added to more complicated objects.
Most notably a lazy matrix may well be non-linear or can have state,
which may be changed by a \update function
in order to influence the represented values at a later point.
An example where this is sensible would be the Coulomb and Exchange matrices,
where the values of these matrices depend on the set of occupied coefficients,
which have been obtained in the previous iterations.
Other examples include the \update of an accuracy threshold for a \contraction expression,
which might change between iterations.

\begin{figure}
	\centering
	\includeimage{6_contraction_lazy_matrices/expression_tree} \\[0.8em]
	\includeimage{6_contraction_lazy_matrices/expression_tree2}
	\caption[Examples for lazy matrix expression trees]
	{
		Examples for lazy matrix expression trees.
		The upper represents the instruction
		$\mat{D} = \mat{A} + \mat{B}$
		and the lower the multiplication of the result $\mat{D}$
		with $\mat{C}$.
	}
	\label{fig:LazyMatrixExpressionTree}
\end{figure}
All evaluation between lazy matrices
like addition, subtraction or matrix-matrix multiplication
is usually delayed until a contraction of the resulting
expression with a vector or a stored matrix is performed
and thus the represented values are unavoidably needed.
This evaluation strategy is called \newterm{lazy evaluation}
in programming language theory~\todo{cite https://dl.acm.org/citation.cfm?doid=72551.72554}, explaining the name of these data structures.
To make this more clear consider the lazy matrix instructions
\begin{equation}
	\begin{aligned}
		\label{eqn:LazyMatrixInstructions}
		\mat{D} &= \mat{A} + \mat{B}, \\
		\mat{E} &= \mat{D} \mat{C}, \\
		\vec{y} &= \mat{E} \vec{x},
	\end{aligned}
\end{equation}
where $\mat{A}$, $\mat{B}$ and $\mat{C}$ are lazy matrices
and $\vec{x}$ is a vector stored in memory.
The first two do not give rise to any computation being done.
They only amount to build an expression tree in the returned
lazy matrix $\mat{E}$ as illustrated in figure \vref{fig:LazyMatrixExpressionTree}.
The final line is a matrix-vector product with a stored vector,
where an actual stored result should be returned in the vector $\vec{y}$.
In the lazy matrix sense this triggers the complete expression tree to be
evaluated in appropriate order,
leading effectively to an evaluation of the expression
\begin{equation}
	\vec{y} = \left( \mat{A} + \mat{B} \right) \mat{C} \vec{x}
	\label{eqn:LazyMatrixFinalExpression}
\end{equation}
at once at this very instance.
\eqref{eqn:LazyMatrixFinalExpression} can be evaluated entirely only using
matrix-vector \contraction expressions.
For example one could first form the product $\tilde{\vec{x}} \equiv \mat{C} \vec{x}$
using the  matrix-vector-product expression of the lazy matrix $\vec{C}$.
Afterwards one would form $\mat{A} \tilde{\vec{x}}$ and $\mat{B} \tilde{\vec{x}}$
again by appropriate \contraction expressions
and finally add the result to give $\vec{y}$.
This is just one way to perform the evaluation.
An implementation of the lazy matrix language is free to choose
a different route for evaluating \eqref{eqn:LazyMatrixFinalExpression}
by reordering the expression if it considers this useful.
If $\mat{C}$ for example was made up of a sum $\mat{F} + \mat{G}$,
it could use distributivity to write
\[ \left( \mat{A} + \mat{B} \right) \left( \mat{F} + \mat{G} \right) \vec{x}
	= \mat{A} \left( \mat{F} \vec{x} \right) + \mat{A} \left( \mat{G}\vec{x}\right)
	+ \mat{B} \left( \mat{F} \vec{x} \right) + \mat{B} \left( \mat{G}\vec{x}\right).
\]
Which of these routes is best differs very much on the structure
of the lazy matrices being part of the expression to evaluate.
But other factors like the operating system or hardware on which
the program code is executed are not unimportant either.
Since the evaluation is delayed
until the call to $\mat{E}\vec{x}$ gets executed at the actual program runtime,
all of this can in theory be taken into account for deciding
which route to take.
Naturally the design of an appropriate cost function
will be in practice all but easy
as previous attempts have shown~\todo{cite a few}.

In either case such decision happen in the evaluation back end
and are well-abstracted by the lazy matrix language
from the instructions \eqref{eqn:LazyMatrixInstructions},
which stay intelligible and understandable.
Furthermore if the structure of the matrices changes,
since for example the discretisation scheme changes,
the evaluation route will automatically adapt
given that the cost function is sensibly chosen.

\section{\lazyten lazy matrix library}




Nothings stops us to directly evaluate and keep expressions like $\mat{A} + \mat{B}$
in memory if both matrices are stored once this result is needed for an apply.

Since lazy matrices are a straight generalisation of usual matrices,
all algorithms written in terms of lazy matrices
are at the same time applicable to dense matrices,
sparse matrices or special matrices like in the case of Sturmians
where a lazy evaluation scheme is needed.
In the context of an \SCF this implies that high-level code
written in terms of lazy matrices
does not need to be changed if the low level matrix implementation
is changed from one basis function type to another.






introduce lazyten (statistics: number of lines of code)
basic ideas behind lazyten / structures
examples (davidson, exchange / coulomb <-- update function)
-> show that it is more intuitive to think of \contraction expressions as lazy matrices

advantage for scf and quantum chemistry
hint at lazy tensors?
transition to molsturm



% ----
the \SCF algorithm
and the implementation of the matrix-vector \contraction expressions.
An additional advantage of such a layer would be that
modifications of the system matrix can be transparently expressed
to the iterative schemes.
For example adding extra terms to the Fock matrix
describing an electric field or a polarisable embedding
or other contributions does not really require
any change to the \SCF code at all.
A good layer of abstraction between the actual \SCF algorithm
and the implementation of the matrix-vector contractions
is therefore highly desirable to enable flexibility on both sides.

need flexibility in matrix expressions in real-world application
like \SCF (extra terms of varying complexity from external field,
polarisable embedding, fragment approaches, and so on)
non-linearity of Coulomb and Exchange
would be nice to transparently express this

% ----

Additionally for these more complicated cases
it is not exactly clear what the best answer will be,
such that the flexibility to experiment with different aspects is key.
Another aspect, which is often forgotten,
but strongly emphasises the need for flexibility
is that hardware frequently changes.
So the best implementation to the current state of hardware,
will be outdated in a few years time,
calling for frequent changes to the implemented \contraction expression.

Such changes should not influence the outer iterative process
like the iterative diagonalisation or the actual \SCF algorithm,
since not that much effort has to be put into these parts
of the program for optimisation
and thus these are not that much influenced by the hardware changes for example.
This calls for a good layer of abstraction between the
iterative routine and the implementation of the matrix-vector \contraction expressions.

Hard to think about problem in terms of expressions.

% ---

designed to add lazy matrix support to existing
linear algebra packages.
It effectively provides a layer of abstraction between the code
describing the algorithms


In the previous chapter we already mentioned
how similar ideas can reduce the formal
computational scaling of the \SCF procedure
for certain kinds of discretisations
and additionally could even lead to a basis-function
agnostic \SCF.
% ---


Lazy matrices can deal with the non-linearity of Coulomb and exchange
and it makes it easy to add terms (maybe show example ??)

Let $\mat{U}$ and $\mat{A}$ be lazy matrices and $\vec{x}$ be a normal vector.
We wish to compute the expression
\[ (\mat{I} - \mat{U} \tp{\mat{U}}) \mat{A} (\mat{I} - \mat{U} \tp{\mat{U}}) \vec{x} \]
which occurs in Jacobi-Davidson procedure.
A typical code is
\begin{lstlisting}
DiagonalMatrix<matrix_type> I(std::vector<scalar_type>(100, 1));
auto projector = I - U * transpose(U);
auto mat = projector * A * projector;
auto res = mat * x
\end{lstlisting}


% See for example slide 15 of iwr school talk
% Code example
% Resulting expression tree
% Evaluation order


\todoil{Maybe illustrate the points mentioned here using the coulomb and exchange matrices as actual examples}



\begin{figure}
	\centering
	\includeimage{6_contraction_lazy_matrices/lazyten_structure}
	\caption{Overview of \lazyten}
	\label{fig:structureLazyten}
\end{figure}




Fazit
\begin{itemize}
	\item contraction based algorithms may lead to lower memory footprint and
		hence to improved scaling
	\item Code can be more readable if if proper abstraction is used
	\item Good for teaching and for experimentation
	\item Abstraction between integrals and SCF algorithms
	\item Plug and play integrals libraries (see next chapter)
	\item Basis-type independent SCF / quantum chemistry
\end{itemize}





\lazyten is the linear algebra interface of \molsturm.
It not only implements the lazy matrix datastructures,
which define the common interface of \gint and \gscf,
but further contains code to make standard external
iterative or direct solver implementations available
from a lazy matrices-based setting.

As can be seen in figure \ref{fig:lazyten} \lazyten provides
a common interface for the \contraction-based algorithms in \gscf,
regardless of the linear algebra~(LA) backend or the solver implementation,
which is used to solve a particular problem.
Lazy matrices like the integrals from \gint as well as built-in structures
like for example a lazy matrix representing the inverse of a matrix
can be used transparently and even in combination with stored matrices
by the means of the automatic bookkeeping done by \lazyten.
Whenever a call to for example the \contraction function enforces
evaluation of a lazy matrix expression,
\lazyten is flexible with respect to the LA backend which is used for
this step as well.
Both armadillo as a LAPACK-based backend as well as Bohrium as a array-operation based backend
are currently available and by recompiling \molsturm with the appropriate configure options,
the backend can be switched.

When it comes to the choice of the solver algorithms, \lazyten is even more flexible.
All algorithms for solving linear problems or eigenproblems are available as 
convenient high-level interface from \gscf.
For example a linear problem $A x = b$ with a lazy matrix $A$, known right-hand side $b$
and unknown $x$ can either be solved by a call to a \texttt{solve} function or by
the means of calling \texttt{inverse(A)} on $A$ and then subsequently
multiplying the inverse matrix with $b$, quite literally coding it as $x = A^{-1} b$.
In both cases \lazyten will perform some introspection regarding the properties of
the lazy matrix expression behind $A$ and consider the number of solution
vectors as well as the required accuracy in order to automatically determine the algorithm to use
for solving the linear problem at hand.
By the means of keyword arguments the user can influence or override the automatic
choice made by \lazyten.

Similarly the eigensolvers can be accessed from \gscf via a common interface,
which abstracts the precise algorithm and allows \lazyten to make an automatic
choice by looking at the precise structure of the Fock matrix at hand.
If all eigenpairs are desired or the matrix is stored in memory anyway,
\lazyten will favour the dense eigensolver algorithms from LAPACK,
whereas if only few eigenpairs are desired,
Krylov-subspace based methods from ARPACK are used.
In either case the precise algorithm can also me chosen from the \python interface
of \molsturm by supplying an appropriate keyword string.


\section{Examples}
An example showing the performance and how we can run things with bohrium as well
