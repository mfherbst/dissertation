\section{Lazy matrices}
\label{sec:lazymat}
The idea of lazy matrices is to encapsulate the coding \contraction-based
in a domain-specific language,
which makes it feel as if one was dealing with actual matrices
instead of \contraction expressions.
Even though not all complications can be hidden,
the resulting syntax allows to write algorithms in a high-level manner
being agnostic to the underlying implementation of the \contraction expression.
This will turn out to be the key aspect
leading to the basis-type of the quantum-chemistry package \molsturm.

For this purpose we generalise the concept of a matrix
to objects we call a \newterm{lazy matrix}.
Whilst a conventional or \newterm{stored matrix} is dense
and has all its elements residing in a continuous chunk of memory,
this restriction does no longer hold for a lazy matrix.
It may for example follow a particular sparse storage scheme
like compressed-row storage,
but does not even need to be associated to any kind of storage at all.
In the most general sense it can be thought of as an
arbitrary \contraction expression for computing the matrix elements,
which is dressed to look like an ordinary matrix from the outside.

In other words one may still obtain individual matrix elements,
add, subtract or multiply such lazy matrix objects together
or apply them to a bunch of vectors or a stored matrix.
Not all of these operations may be equally fast
than there counterparts on stored matrices, however.
Most importantly obtaining individual elements of such a matrix
can become rather costly,
since they might involve a computation as well
and not just a lookup into memory.

On the upside one gains a much more flexible data structure
where a familiar matrix-like interface
can be added to more complicated objects.
Most notably a lazy matrix may well be non-linear or can have state,
which may be changed by a \update function
in order to influence the represented values at a later point.
An example where this is sensible would be the Coulomb and Exchange matrices,
where the values of these matrices depend on the set of occupied coefficients,
which have been obtained in the previous iterations.
Other examples include the \update of an accuracy threshold for a \contraction expression,
which might change between iterations.

\begin{figure}
	\centering
	\includeimage{6_contraction_lazy_matrices/expression_tree} \\[0.8em]
	\includeimage{6_contraction_lazy_matrices/expression_tree2}
	\caption[Examples for lazy matrix expression trees]
	{
		Examples for lazy matrix expression trees.
		The upper represents the instruction
		$\mat{D} = \mat{A} + \mat{B}$
		and the lower the multiplication of the result $\mat{D}$
		with $\mat{C}$.
	}
	\label{fig:LazyMatrixExpressionTree}
\end{figure}
All evaluation between lazy matrices
like addition, subtraction or matrix-matrix multiplication
is usually delayed until a contraction of the resulting
expression with a vector or a stored matrix is performed
and thus the represented values are unavoidably needed.
This evaluation strategy is called \newterm{lazy evaluation}
in programming language theory~\todo{cite https://dl.acm.org/citation.cfm?doid=72551.72554}, explaining the name of these data structures.
To make this more clear consider the lazy matrix instructions
\begin{equation}
	\begin{aligned}
		\label{eqn:LazyMatrixInstructions}
		\mat{D} &= \mat{A} + \mat{B}, \\
		\mat{E} &= \mat{D} \mat{C}, \\
		\vec{y} &= \mat{E} \vec{x},
	\end{aligned}
\end{equation}
where $\mat{A}$, $\mat{B}$ and $\mat{C}$ are lazy matrices
and $\vec{x}$ is a vector stored in memory.
The first two do not give rise to any computation being done.
They only amount to build an expression tree in the returned
lazy matrix $\mat{E}$ as illustrated in figure \vref{fig:LazyMatrixExpressionTree}.
The final line is a matrix-vector product with a stored vector,
where an actual stored result should be returned in the vector $\vec{y}$.
In the lazy matrix sense this triggers the complete expression tree to be
evaluated in appropriate order,
leading effectively to an evaluation of the expression
\begin{equation}
	\vec{y} = \left( \mat{A} + \mat{B} \right) \mat{C} \vec{x}
	\label{eqn:LazyMatrixFinalExpression}
\end{equation}
at once at this very instance.
\eqref{eqn:LazyMatrixFinalExpression} can be evaluated entirely only using
matrix-vector \contraction expressions.
For example one could first form the product $\tilde{\vec{x}} \equiv \mat{C} \vec{x}$
using the matrix-vector-product expression of the lazy matrix $\vec{C}$.
Afterwards one would form $\mat{A} \tilde{\vec{x}}$ and $\mat{B} \tilde{\vec{x}}$
again by appropriate \contraction expressions
and finally add the result to give $\vec{y}$.
This is just one way to perform the evaluation.
An implementation of the lazy matrix language is free to choose
a different route for evaluating \eqref{eqn:LazyMatrixFinalExpression}
by reordering the expression if it considers this useful.
If $\mat{C}$ for example was made up of a sum $\mat{F} + \mat{G}$,
it could use distributivity to write
\[ \left( \mat{A} + \mat{B} \right) \left( \mat{F} + \mat{G} \right) \vec{x}
	= \mat{A} \left( \mat{F} \vec{x} \right) + \mat{A} \left( \mat{G}\vec{x}\right)
	+ \mat{B} \left( \mat{F} \vec{x} \right) + \mat{B} \left( \mat{G}\vec{x}\right).
\]
Which of these routes is best differs very much on the structure
of the lazy matrices being part of the expression to evaluate.
But other factors like the operating system or hardware on which
the program code is executed are not unimportant either.
Since the evaluation is delayed
until the call to $\mat{E}\vec{x}$ gets executed at the actual program runtime,
all of this can in theory be taken into account for deciding
which route to take.
Naturally the design of an appropriate cost function
will be in practice all but easy
as previous attempts have shown~\todo{cite a few}.

In either case such decision happen in the evaluation back end
and are well-abstracted by the lazy matrix language
from the instructions \eqref{eqn:LazyMatrixInstructions},
which stay intelligible and understandable.
Furthermore if the structure of the matrices changes,
since for example the discretisation scheme changes,
the evaluation route will automatically adapt
given that the cost function is sensibly chosen.

%
% --------
%
\defineabbr{LA}{LA\xspace}{linear algebra}
\section{\lazyten lazy matrix library}
\begin{figure}
	\centering
	\includeimage{6_contraction_lazy_matrices/lazyten_structure}
	\todoil{Citations for Lapack, Eigen and Anasazi}
	\caption[Structure of the \lazyten lazy matrix library]{%
		Structure of the \lazyten lazy matrix library~\cite{lazytenWeb}
		and its interfaces to the 3rd party codes
		armadillo~\cite{Armadillo}, Bohrium~\cite{Kristensen2016array,Kristensen2016streaming},
		LAPACK~\cite{LAPACK} and ARPACK~\cite{Lehoucq1998}.
		Support for Eigen~\cite{Eigen} and Anasazi~\cite{Anasazi} is planned.
	}
	\label{fig:structureLazyten}
\end{figure}
An initial implementation of the lazy matrix language has been
achieved in the \cpp library \lazyten~\cite{lazytenWeb}.
Not all aspects of lazy matrices are yet available in this library, however.
For example many opportunities to exploit the possible performance improvements
due to freely reordering the lazy matrix expression tree are currently missed.
On the other hand \lazyten goes a bit beyond the lazy matrix interface
in the sense that it is a full abstraction layer for linear algebra.
As can be seen in figure \vref{fig:structureLazyten}
the overall aim of \lazyten is to provide a common interface
for \contraction-based methods like for example a \contraction-based \SCF algorithm
with access to different linear algebra back ends or solver implementations.

Inside the framework of \lazyten combining custom lazy matrices
as well as built-in structures,
like a lazy matrix representing the inverse of a matrix,
can be achieved transparently.
Even a combination with stored matrices in any of these expressions is possible.
Whenever an operation like the matrix-vector product in the third line
of \eqref{eqn:LazyMatrixInstructions}
enforces actual computation to be performed
\lazyten is flexible with respect to the \LA back end,
which is used to compute the sums and products between the
stored quantities that appear along the way.
In the example above the \LA back end would for example be
responsible for evaluating the final sum of the vectors
$\mat{A} \tilde{\vec{x}}$ and $\mat{B} \tilde{\vec{x}}$.
Other places where the \LA back end is important
are the operations of stored matrices or vectors amongst themselves.
At the present stage armadillo~\cite{Armadillo}
as a LAPACK\todo{cite}-based back end
as well as Bohrium~\cite{Kristensen2016array,Kristensen2016streaming}
as an array-operation-based back end are currently available.
Switching back end right now requires to recompile \lazyten
with the appropriate configure options.

The interface of \lazyten furthermore allows
to access the available solver algorithms
in the form of high-level routines.
For example consider a linear problem
\[\mat{A} \vec{x} = \vec{b}\]
with known right-hand side $\vec{b}$ and unknown $\vec{x}$.
Let the system matrix $\mat{A}$ be represented by the \lazyten matrix object \texttt{A}
and the right-hand side $\vec{b}$ by the object \texttt{b}
of type \code{SmallVector<double>}.
Then the linear system can be solved either by the code
\begin{lstlisting}[style=c++]
SmallVector<double> x(b.size());
solve(A, x, b);
\end{lstlisting}
or equivalently by
\begin{lstlisting}[style=c++]
auto invA = inverse(A);
auto b = invA * b;
\end{lstlisting}
\ie quite literally coding the application of the inverse.
In each case only the last line does any real computation
by currently calling a hard-coded linear solver algorithm from LAPACK.
In the future we plan to add a step which performs some introspection
regarding the properties of
the lazy matrix expression encoded inside the object \texttt{A}
as well as the number of right-hand side vectors or the required accuracy
for the solution in order to determine an
ideal algorithm for solving the linear problem at hand.
For solving eigenproblems such a
selection mechanism has already been implemented
and mediates between the dense eigensolver algorithms from LAPACK~\todo{cite}
and Arnoldi methods from ARPACK~\cite{Lehoucq1998}.
Generally dense methods will be favoured if many eigenpairs
or eigenpairs from a dense matrix are requested,
whereas the Arnoldi methods are selected for large matrices and obtaining few eigenpairs.
In each case the automatic selection can be overwritten
by supplying an appropriate keyword arguments.
Similarly many details of the underlying solver can be set by parameters
from the high-level interface.

%
%----
% TODO from here

\begin{figure}
	\centering
	\loadnt[firstline=40,lastline=80,style={c++},showstringspaces=false]
		{6_contraction_lazy_matrices/rhf_code.cpp}
	\caption{Code fragment of a very simple closed-shell Roothaan repeated diagonalisation
	\SCF in \lazyten.}
	\label{fig:LazytenRhfCode}
\end{figure}
To illustrate the user-friendliness of \lazyten
let us consider the code fragment depicted in figure \vref{fig:LazytenRhfCode}.


\texttt{n\_alpha} number of alpha electrions
\texttt{n\_orb} number of orbitals to compute
\texttt{outer\_prod\_sum} routine to compute the sum of outer products
between multivectors.
Essentially equivalent to a matrix-matrix multiplication with transposition.


easy to add terms

-> show that it is more intuitive to think of \contraction expressions as lazy matrices
advantage for scf and quantum chemistry
\lazyten as mutual abstraction layer between integrals and \SCF routines
This simple code is an example of a basis-function agnostic \SCF.

incorporate the next few paragraphs

hint at lazy tensors?
transition to molsturm









% ----


Coded in the high-level language provided by \lazyten
the precise way how the evaluation of these instructions proceeds
can still be decided at a later point
once insight into matrix structures as well as
parameters like the number of eigenpairs to retrieve or the required accuracy are known.


Since lazy matrices are a straight generalisation of usual matrices,
all algorithms written in terms of lazy matrices
are at the same time applicable to dense matrices,
sparse matrices or special matrices like in the case of Sturmians
where a lazy evaluation scheme is needed.
In the context of an \SCF this implies that high-level code
written in terms of lazy matrices
does not need to be changed if the low level matrix implementation
is changed from one basis function type to another.


% ----
the \SCF algorithm
and the implementation of the matrix-vector \contraction expressions.
An additional advantage of such a layer would be that
modifications of the system matrix can be transparently expressed
to the iterative schemes.
For example adding extra terms to the Fock matrix
describing an electric field or a polarisable embedding
or other contributions does not really require
any change to the \SCF code at all.
A good layer of abstraction between the actual \SCF algorithm
and the implementation of the matrix-vector contractions
is therefore highly desirable to enable flexibility on both sides.

need flexibility in matrix expressions in real-world application
like \SCF (extra terms of varying complexity from external field,
polarisable embedding, fragment approaches, and so on)
non-linearity of Coulomb and Exchange
would be nice to transparently express this

% ----

Additionally for these more complicated cases
it is not exactly clear what the best answer will be,
such that the flexibility to experiment with different aspects is key.
Another aspect, which is often forgotten,
but strongly emphasises the need for flexibility
is that hardware frequently changes.
So the best implementation to the current state of hardware,
will be outdated in a few years time,
calling for frequent changes to the implemented \contraction expression.

Such changes should not influence the outer iterative process
like the iterative diagonalisation or the actual \SCF algorithm,
since not that much effort has to be put into these parts
of the program for optimisation
and thus these are not that much influenced by the hardware changes for example.
This calls for a good layer of abstraction between the
iterative routine and the implementation of the matrix-vector \contraction expressions.

Hard to think about problem in terms of expressions.

% ---


some words about statistics,
number of lines of code





Fazit or put into molsturm design
\begin{itemize}
	\item contraction based algorithms may lead to lower memory footprint and
		hence to improved scaling
	\item Code can be more readable if if proper abstraction is used
	\item Good for teaching and for experimentation
	\item Abstraction between integrals and SCF algorithms
	\item Plug and play integrals libraries (see next chapter)
	\item Basis-type independent SCF / quantum chemistry
\end{itemize}




\subsection{Examples}
\todoil{If time}
An example showing the performance and how we can run things with bohrium as well


Lazy matrices can deal with the non-linearity of Coulomb and exchange
and it makes it easy to add terms (maybe show example ??)

Let $\mat{U}$ and $\mat{A}$ be lazy matrices and $\vec{x}$ be a normal vector.
We wish to compute the expression
\[ (\mat{I} - \mat{U} \tp{\mat{U}}) \mat{A} (\mat{I} - \mat{U} \tp{\mat{U}}) \vec{x} \]
which occurs in Jacobi-Davidson procedure.
A typical code is
\begin{lstlisting}
DiagonalMatrix<matrix_type> I(std::vector<scalar_type>(100, 1));
auto projector = I - U * transpose(U);
auto mat = projector * A * projector;
auto res = mat * x
\end{lstlisting}
