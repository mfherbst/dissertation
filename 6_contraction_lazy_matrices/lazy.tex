\section{Lazy matrices}



% ---
the \SCF algorithm
and the implementation of the matrix-vector \contraction expressions.
An additional advantage of such a layer would be that
modifications of the system matrix can be transparently expressed
to the iterative schemes.
For example adding extra terms to the Fock matrix
describing an electric field or a polarisable embedding
or other contributions does not really require
any change to the \SCF code at all.
A good layer of abstraction between the actual \SCF algorithm
and the implementation of the matrix-vector contractions
is therefore highly desirable to enable flexibility on both sides.

need flexibility in matrix expressions in real-world application
like \SCF (extra terms of varying complexity from external field,
polarisable embedding, fragment approaches, and so on)
non-linearity of Coulomb and Exchange
would be nice to transparently express this
% ---



contraction functions less intuitive to think about
than matrices
So ideas to make a matrix that is a contraction



Lazy matrices can deal with the non-linearity of Coulomb and exchange
and it makes it easy to add terms (maybe show example ??)


\todo[inline,caption={}]{
	\begin{itemize}
		\item Define
		\item how it works
		\item what it solves
		\item In the context of HF
	\end{itemize}
}

\begin{itemize}
	\item Lazy matrices defined in contrast to stored matrices
	\item Stored matrices: Everything resides in memory
	\item Lazy matrices: Generalisation which lifts this restriction
	\item Only a shell which looks like a matrix, but internally only contains expressions
	\item Subject to lazy evaluation
	\item Everything is delayed until \contraction with vector
	\item For convenience: All matrix operations (including element retrieval) allowed.
\end{itemize}

Let $\mat{U}$ and $\mat{A}$ be lazy matrices and $\vec{x}$ be a normal vector.
We wish to compute the expression
\[ (\mat{I} - \mat{U} \tp{\mat{U}}) \mat{A} (\mat{I} - \mat{U} \tp{\mat{U}}) \vec{x} \]
which occurs in Jacobi-Davidson procedure.
A typical code is
\begin{lstlisting}
DiagonalMatrix<matrix_type> I(std::vector<scalar_type>(100, 1));
auto projector = I - U * transpose(U);
auto mat = projector * A * projector;
auto res = mat * x
\end{lstlisting}


% See for example slide 15 of iwr school talk
% Code example
% Resulting expression tree
% Evaluation order


\label{sec:lazymat}
\todoil{Maybe illustrate the points mentioned here using the coulomb and exchange matrices as actual examples}

% TODO a good paragraph for this part:
Whenever operations are done on a matrix \lazyten does not automatically
evaluate them in all cases.
Much rather it usually just builds up a datastructure,
which keeps track of the operations which \textit{should} be done
at some point in the future.
Whenever an actual call to the \contraction-function happens,
the expression history is considered and evaluated with respect
to the other arguments supplied by the \contraction call.
% end TODO

% For the design chapter it is important to mention here that \contraction-based methods
% are a generalisation of non-contraction based methods.

\todoil{Maybe this section is too long given that we want to publish on this again}

Note that a disadvantage of \contraction-based algorithms is that the expressions
computing the tensor contraction can become rather complicated.
Furthermore it usually is more intuitive to think of the
numerical modelling in terms of tensors, matrices and vectors
rather than \contraction functions.
For this purpose we generalise the concept of a matrix
to so-called \term{lazy matrix} objects.
Whilst a normal or \term{stored matrix} is dense and has all its elements
residing in memory,
a lazy matrix is more general.
It may follow a particular sparse storage scheme
like compressed-row storage
or even more general all its elements may just be arbitrary expressions.
The values of lazy matrices are hence only computed upon request
or whenever a contraction with another object is required.
As such the lazy matrix may have, \ie a special \update function
may be used to modify the expression of the lazy matrix
at a later point.
Note, however, that special storage schemes for storing
sparse matrices are similarly just lazy matrices 
\todoil{
	Updating lazy matrices is a bit like reactive programming
	\url{https://en.wikipedia.org/wiki/Reactive_programming}.
	In fact one could use them to get reactive programming
	into C++ in a simple way.
	Mention this here (or in the \lazyten paper)
}

This on the one hand makes obtaining the lazy matrix elements
expensive, but on the other hands gives a nice matrix-like
interface to more complicated objects.
All evaluation between lazy matrices
(i.e. addition or matrix-matrix multiplication)
is usually delayed until for example a contraction of the resulting
expression with a vector is performed.
One typically refers to this strategy as \term{lazy evaluation}.

Since lazy matrices are a straight generalisation
of usual matrices,
all algorithms written in terms of lazy matrices
are at the same time applicable to dense matrices,
sparse matrices or special matrices like in the case of Sturmians
where a lazy evaluation scheme is needed.
So high-level code written in terms of lazy matrices
does not need to be changed if the low level matrix implementation
is changed from one basis function type to another.

\todoil{One could mention the  processor-memory performance gap \ldots
	    but I would leave it for the \lazyten paper}

% 1/2 paragraph: Introducing the problem of large tensors in QM - apply based algorithms
% 1/2 paragraph: One of the flexibilities of Molsturm is that we can switch between
%                apply-based, sparse, and dense methods without changing high-level
%                code (which simply mimic the formul√¶). 
%
% define stored vs. lazy matrix
% lazy: sparse or expressions
% lazy matrix may have state
% other key features of lazy matrices



\section{\lazyten lazy matrix library}

\begin{figure}
	\centering
	\includeimage{6_contraction_lazy_matrices/lazyten_structure}
	\caption{Overview of \lazyten}
	\label{fig:structureLazyten}
\end{figure}




\begin{itemize}
	\item Builds up DAG instead of evaluating
	\item Evaluation is done delayed upon \contraction
	
\end{itemize}

Fazit
\begin{itemize}
	\item contraction based algorithms may lead to lower memory footprint and
		hence to improved scaling
	\item Code can be more readable if if proper abstraction is used
	\item Good for teaching and for experimentation
	\item Abstraction between integrals and SCF algorithms
	\item Plug and play integrals libraries (see next chapter)
	\item Basis-type independent SCF / quantum chemistry
\end{itemize}


designed to add lazy matrix support to existing
linear algebra packages.
It effectively provides a layer of abstraction between the code
describing the algorithms


In the previous chapter we already mentioned
how similar ideas can reduce the formal
computational scaling of the \SCF procedure
for certain kinds of discretisations
and additionally could even lead to a basis-function
agnostic \SCF.



\lazyten is the linear algebra interface of \molsturm.
It not only implements the lazy matrix datastructures,
which define the common interface of \gint and \gscf,
but further contains code to make standard external
iterative or direct solver implementations available
from a lazy matrices-based setting.

As can be seen in figure \ref{fig:lazyten} \lazyten provides
a common interface for the \contraction-based algorithms in \gscf,
regardless of the linear algebra~(LA) backend or the solver implementation,
which is used to solve a particular problem.
Lazy matrices like the integrals from \gint as well as built-in structures
like for example a lazy matrix representing the inverse of a matrix
can be used transparently and even in combination with stored matrices
by the means of the automatic bookkeeping done by \lazyten.
Whenever a call to for example the \contraction function enforces
evaluation of a lazy matrix expression,
\lazyten is flexible with respect to the LA backend which is used for
this step as well.
Both armadillo as a LAPACK-based backend as well as Bohrium as a array-operation based backend
are currently available and by recompiling \molsturm with the appropriate configure options,
the backend can be switched.

When it comes to the choice of the solver algorithms, \lazyten is even more flexible.
All algorithms for solving linear problems or eigenproblems are available as 
convenient high-level interface from \gscf.
For example a linear problem $A x = b$ with a lazy matrix $A$, known right-hand side $b$
and unknown $x$ can either be solved by a call to a \texttt{solve} function or by
the means of calling \texttt{inverse(A)} on $A$ and then subsequently
multiplying the inverse matrix with $b$, quite literally coding it as $x = A^{-1} b$.
In both cases \lazyten will perform some introspection regarding the properties of
the lazy matrix expression behind $A$ and consider the number of solution
vectors as well as the required accuracy in order to automatically determine the algorithm to use
for solving the linear problem at hand.
By the means of keyword arguments the user can influence or override the automatic
choice made by \lazyten.

Similarly the eigensolvers can be accessed from \gscf via a common interface,
which abstracts the precise algorithm and allows \lazyten to make an automatic
choice by looking at the precise structure of the Fock matrix at hand.
If all eigenpairs are desired or the matrix is stored in memory anyway,
\lazyten will favour the dense eigensolver algorithms from LAPACK,
whereas if only few eigenpairs are desired,
Krylov-subspace based methods from ARPACK are used.
In either case the precise algorithm can also me chosen from the \python interface
of \molsturm by supplying an appropriate keyword string.


\section{Examples}
An example showing the performance and how we can run things with bohrium as well
