\chapter{Mathematical structure of quantum mechanics}
\chapquote{
	It is a well-known experience that the only truly
	enjoyable and profitable way of studying mathematics
	is the method of ``filling in details'' by one's
	own efforts.
}{Cornelius Lanczos~(1893--1974)}

At the turn of the 19th century it was discovered
that classical mechanics in the formulations provided
by Lagrange, Hamilton and Poission
are not able to capture all effects experimentially observed.
This lead to the alternative theory of Quantum Mechanics,
which was developed in close analogy to the classical formulation.

In this chapter we will discuss the mathematical structure
of quantum mechanics in light of the problems of atomic phyics
and quantum chemistry.
Our discussion will focus on explaining both the mathematical
peculiarities of quantum mechanics as well as hinting
at the tricks required to make progress in a phyisical setting.
We follow mostly the excellent materical by \citet{Mueller2000},
\citet{Davies2007} and \citet{Helffer2013} is our discussion.

\input{1_qm/correspondence.tex}

\section{Hilbert spaces}
The setting of Quantum Mechanics are the Hilbert spaces
as all states 


Next we turn our attention to Hilbert spaces, which will turn out to be the abstract counterparts of Euclidean space. \\

In order to know what we are talking about, it is necessary to properly define some mathematical structures.
Usually one does this by listing the axioms which those structures should satisfy in a very abstract way.
Whilst this surely can be quite boring, note that the key points here are only that you get the general idea why things are defined the way they are. 
The precise mathematical formulation is only of minor importance.

\begin{defn}
	A \newterm{group} $(G, \circ)$ is a \emph{non-empty} set $G$
	together with a binary operation
	\[ \circ: M \times M \to M \]
	satisfying the three axioms
	\begin{itemize}
		\item \newterm{Associativity}: 
			$\forall k, m, n \in G: \left( k \circ m \right) \circ n = k \circ \left( m \circ n \right)$
		\item \newterm{Identity element}: $\exists e \in M$ such that $\forall g \in G:  e \circ g = g$
		\item \newterm{Inverse}:
			$\forall g \in G: \exists g' \in G$ such that $g \circ g' = e$
		where $e$ is exactly the identity element.
		Often one denotes the inverse of $g$ as $g^{-1}$ as well.
	\end{itemize}
\end{defn}

\begin{lem}
	Let $(G, \circ)$ be a group. It holds
	\begin{itemize}
		\item The identity element $e$ acts as such both to the left and the right:
			\[ e \circ m = m = m \circ e \qquad \forall m \in G \]
		\item The inverse of the inverse is the element by itself, i.e.
			\[ (g^{-1})^{-1} = g \]
	\end{itemize}
	\begin{proof}
		\todo[inline]{Add a reference or do it}
	\end{proof}
\end{lem}

\begin{defn}
	A group $(G, \circ)$ is called \newterm{abelian} if it satisfies
	\[ \forall g, h \in G: g \circ h = h \circ g \]
\end{defn}

\nomenclature{$\F$}{Either the field of real numbers $\R$ or the field of complex numbers $\C$. See also definition \vref{defn:Field}.}
\begin{defn}
	\label{defn:Field}
	A \newterm{field} $\F$ is a non-empty set with the binary operations
	\begin{itemize}
		\item Multiplication: $\cdot: \F \times \F \to \F$
		\item Addition $+: \F \times \F \to \F$
	\end{itemize}
	such that
	\begin{itemize}
		\item $(\F, +)$ is an abelian group with identity element $0 \in \F$.
		\item $\left( \F \backslash \{ 0 \}, \cdot \right)$ is an abelian group
			with identity element $1 \in \F$.
		\item \newterm{Distributivity}: $\forall a,b,c \in \F: a \cdot (b + c) = (a \cdot b) + (a \cdot c)$
	\end{itemize}
\end{defn}

\begin{exmp}
	In this work the most important fields are the set of real numbers $\R$
	and the set of complex numbers $\C$ with their usual addition and multiplication.
\end{exmp}

\begin{defn}
	A vector space over a field $\F$ is a non-empty set $V$
	with the binary operations
	\begin{itemize}
		\item Vector addition: $V \times V \to V : (x,y) \mapsto x+y$
		\item Scalar multiplication $\F \times V \to V: (\alpha,x) \mapsto \alpha x$
	\end{itemize}
	that satisfy some axioms listed below. The elements of $V$ are referred to as vectors
	and the elements of the field $\F$ are called scalars.

	The axioms to be satisfied are:
	\begin{itemize}
		\item $(V,+)$ is an abelian group, i.e. it holds
		\begin{align}
			\label{eqn:vectorAss}        \forall x,y,z \in V: &\quad x + (y + z) = (x + y) + z		&&\text{"associativity of addition"} \\
			\label{eqn:vectorZero}       \exists 0 \in V: &\quad \forall x \in V : x + 0 = x		&&\text{"existence of zero vector"} \\
			\label{eqn:vectorInverse}    \forall x \in V: &\quad \exists -x \in V : x + (-x) = 0	&&\text{"existence of additive inverses"} \\
			\label{eqn:vectorComm}       \forall x,y \in V: &\quad x + y = y + x			&&\text{"commutativity of addition"}
		\end{align}
		\item The so-called compatibility of scalar multiplication and
			field multiplication:
			\begin{equation}
				\forall \alpha,\beta \in \F, x \in V: \quad (\alpha \beta) x = \alpha (\beta x)
				\label{eqn:vectorCompField}
			\end{equation}
		\item The scalar multiplication with the multiplicative identity of
			$\F$ (i.e. the scalar $1$) should satisfy
			\begin{equation}
				\forall x \in V: \quad 1\, x = x
				\label{eqn:vectorMultIdent}
			\end{equation}
		\item The following distributivity relations:
			\begin{align}
				\label{eqn:vectorDistrVecAdd}\forall \alpha \in \set{F}, x,y \in V: & \quad \alpha (x+y) = \alpha x + \alpha y\\
				\label{eqn:vectorDistrFldAdd}\forall \alpha,\beta \in \set{F}, x \in V: & \quad (\alpha + \beta) x = \alpha x + \beta x
			\end{align}
	\end{itemize}
\end{defn}

\begin{nte}
	As stated above, this is all rather technical, so let us sum briefly up the rationale and the key ideas:
	\begin{itemize}
		\item Recall the picture of \eqref{eqn:vectorEuclidAdd} where we pasted arrows together in order to add them. 
			It is obvious that such an operation should satisfy \eqref{eqn:vectorAss} and \eqref{eqn:vectorComm}, just because the precise order in which we apply the vectors does not matter.
			Surely if one adds the zero vector to any arrow, that arrow is unchanged, rationalising \eqref{eqn:vectorZero}.
			\eqref{eqn:vectorInverse} is also easy to understand if one sees the inverse as the original arrow with head and tail swapped.
		\item Condition \eqref{eqn:vectorCompField} establishes a formal consistency when scaling vectors multiple times.
		\item \eqref{eqn:vectorMultIdent} gives us a uniquely defined scalar multiplication, since it defines the outcome of the multiplication operator for one specific scalar.
			The result for other scalars can than be deduced by applying distributivity and \eqref{eqn:vectorCompField}. 
			It is remotely comparable to choosing the potential offset when calculating the energy.
		\item The distributivity relations \eqref{eqn:vectorDistrVecAdd} and \eqref{eqn:vectorDistrFldAdd} just give us the intuitive result that the order of vector/scalar addition and scaling of vectors does not matter.
	\end{itemize}
\end{nte}

\noindent
As a nice example we are now going to show (at least partly)
\begin{prop}
	Euclidean space $\R^3$ is a vector space over $\R$.
\end{prop}

\noindent
The essential idea behind the proof will be to use the well-known properties from $\R$ in order to deduce the corresponding axioms in $\R^3$.
Since this is a pretty boring task we are only going to formally proof some of the axioms, illustrating this key point.
The rest is left as an exercise for the reader.
\begin{proof}
	First note, that $\R^3$ is non-empty, since $(0~0~0)^T \in \R^3$. 
	Now choose arbitrary $x,y \in \R^3$ and arbitrary $\alpha, \beta \in \R$.
	This implies (recall \eqref{eqn:vectorR3}) that we can find some $x_1, x_2, x_3,~y_1, y_2, y_3 \in \R$ with
	\[ x = \left( \begin{array}{c} x_1 \\ x_2 \\ x_3 \end{array} \right)
		\hspace{20pt}
		y = \left( \begin{array}{c} y_1 \\ y_2 \\ y_3 \end{array}  \right)
	\]
	\begin{itemize}
		\item Now use the previously established rule \eqref{eqn:vectorEuclidAdd} and commutativity in $\R$ to write
			\begin{equation}
				x + y = \left( \begin{array}{c} x_1 + y_1 \\ x_2 + y_2 \\ x_3 + y_3 \end{array} \right)
					= \left( \begin{array}{c} y_1 + x_1 \\ y_2 + x_2 \\ y_3 + x_3 \end{array} \right)
					= y + x,
				\label{eqn:vectorEuclidProofComm}
			\end{equation}
			which shows that we satisfy \eqref{eqn:vectorComm}.
		\item From \eqref{eqn:vectorEuclidScalFac} and associativity of the multiplication in $\R$ we show the compatibility condition \eqref{eqn:vectorCompField}.
			\begin{equation}
				(\alpha \beta) x = \left( \begin{array}{c} (\alpha \beta) x_1 \\ (\alpha \beta) x_2 \\ (\alpha \beta) x_3 \end{array} \right)
					= \left( \begin{array}{c} \alpha (\beta x_1) \\ \alpha (\beta x_2) \\ \alpha (\beta x_3) \end{array} \right)
					= \alpha (\beta x)
				\label{eqn:vectorEuclidProofComp}
			\end{equation}
		\item Apply both rules and use distributivity in $\R$ to see that the axiom \eqref{eqn:vectorDistrVecAdd} holds.
			\begin{equation}
				\alpha (x + y) = \left( \begin{array}{c} \alpha (x_1 + y_1) \\ \alpha (x_2 + y_2) \\ \alpha (x_3 + y_3) \end{array} \right)
					= \left( \begin{array}{c} \alpha x_1 + \alpha y_1 \\ \alpha x_2 + \alpha y_2 \\ \alpha x_3 + \alpha y_3 \end{array} \right)
					= \alpha x + \alpha y
				\label{eqn:vectorEuclidProofDist}
			\end{equation}
		\item and so on 
	\end{itemize}
\end{proof}

% --------------------------------------

\noindent
The next step is to add a scalar product, also known as an inner product.
\begin{defn}
	An \emph{inner product space} over $\set{F}$ is a vector space $V$ (over the same field) that is further equipped with an inner product, i.e. a map
	\[ (\cdot,\cdot) : V \times V \to \set{F} \]
	that satisfies (for all vectors $x,y,z \in V$ and all $\alpha \in \set{F}$)
	\begin{align}
		\label{eqn:innProdConjSym}  &(x,y)^\ast = (y,x) &&                                                 \text{"Conjugate symmetry"} \\
		\label{eqn:innProdLinLeft}  &(\alpha x + y,z) = \alpha (x,z) + (y,z) &&                            \text{"Linearity in the first argument"} \\
		\label{eqn:innProdPosDef}   &(x,x) \ge 0 \quad \text{and} \quad (x,x) = 0 ~\Rightarrow~ x = 0  &&  \text{"Positive-definiteness"}
	\end{align}
	here the asterisk $^\ast$ denotes complex conjugation.
\end{defn}

\begin{rem}
	%TODO: have that?
	Some other well-known properties of the inner product are in fact not axioms but can be easily deduced by combining two of the above:
	\begin{itemize}
		\item Conjugate symmetry \eqref{eqn:innProdConjSym} implies \emph{antilinearity} (or \emph{conjugate linearity}) in the second argument
			\begin{equation}
				(z, \alpha x + y) = \alpha^\ast (z,x) + (z,y)
				\label{eqn:innProdAntiLinRight}
			\end{equation}
			In other words the inner product is what is called a \emph{sesquilinear form}.

		\item If $\set{F} = \R$, then Conjugate symmetry \eqref{eqn:innProdConjSym} simplifies to proper symmetry, ie
			\[ (x,y) = (y,x). \]
			The antilinearity in the second argument then also reduces to proper linearity
			\begin{equation}
				(z, \alpha x + y) = \alpha (z,x) + (z,y),
				\label{eqn:innProdLinRight}
			\end{equation}
			such that this time we get a so-called \emph{bilinear form}.
	\end{itemize}
\end{rem}

\begin{nte}
	From your previous encounters with Dirac notation $\langle \cdot | \cdot \rangle$ you might think that it satisfies all these properties and should be regarded as an inner product as well.
	In a loose sense this is not actually a bad description and it probably suffices as working knowledge.
	From a mathematically precise perspective this is, however, not correct. 
	Nevertheless both objects, the inner product $(\cdot,\cdot)$ and the Dirac bra-c-ket $\langle \cdot | \cdot \rangle$, are highly related. This will become clear from the formal definition of the latter, which we will present later.
\end{nte}

\noindent
As expected one can show:
\begin{prop}
	Euclidean space $\R^3$ is an inner product space over $\R$.
\end{prop}
\begin{proof}
	Left as an exercise for the reader.
\end{proof}

\noindent
Next we proceed to discuss the norm:
\begin{defn}
	Given a vector space over the field $\set{F}$, a \emph{norm} is a map
	\[ \norm{\cdot} : V \to \R \]
	such that the following axioms hold for all vectors $x,y \in V$ and all $\alpha \in \set{F}$:
	\begin{align} 
		\label{eqn:normScalability}  &\norm{\alpha x} = \abs{\alpha} \norm{x} &&                           \text{"Absolute scalability"} \\
		\label{eqn:normTriaIneq}     &\norm{x + y} \le \norm{x} + \norm{y}    &&                           \text{"Triangle inequality"} \\
		\label{eqn:normPointSep}     &\text{If}~\norm{x} = 0 \quad \Rightarrow \quad \text{$x$ is the zero vector} &&  \text{"Norm separates points"}
	\end{align}
\end{defn}

\begin{rem}
	Whilst absolute scalability \eqref{eqn:normScalability} seems like a reasonable thing to ask for when defining the norm (again think about scaling arrows), the other two might not be as obvious from the start.
	\begin{itemize}
		\item The abstract formulation of the triangle inequality \eqref{eqn:normTriaIneq} is --- as the name suggests --- a direct generalisation of the geometric constraints for a basic triangle:
			One side always has to be shorter than the sum of the two others. 
			From the triangular structure that vector addition gives (see fig. \vref{fig:TriangleInequality}) the axiom can be easily illustrated in the case of Euclidean vectors.
		\item The norm separates points condition again is one of those axioms to give a uniquely defined structure, like \eqref{eqn:vectorMultIdent} for the scaling of vectors.
			The choice is the most natural as it leads to a positive norm which can be seen as follows.
			From absolute scalability we have $\norm{-x} = \norm{x}$ for each vector $x \in V$ and furthermore for the zero vector $\norm{0} = 0$.
			By the triangle inequality $0 = \norm{0} = \norm{x-x} \le \norm{-x} + \norm{x} = 2 \norm{x}$. 
			Dividing both sides by 2 gives the result $\norm{x} \ge 0$.
	\end{itemize}
\end{rem}


\begin{prop}
	For every inner product space there exists a norm given by 
	\begin{equation}
		\norm{x} = \sqrt{ (x,x) } \qquad \forall x \in V,
		\label{eqn:normInduced}
	\end{equation}
	the so-called \emph{induced norm}.
\end{prop}
\begin{proof}
	Consider arbitrary $x,y \in V$ and $\alpha \in \set{F}$
	\begin{itemize}
		\item The point separating property \eqref{eqn:normPointSep} is a trivial consequence of \eqref{eqn:innProdPosDef}.
		\item We get absolute scalability from linearity \eqref{eqn:innProdLinLeft} and antilinearity \eqref{eqn:innProdAntiLinRight} of the scalar product
			\[ \norm{\alpha x} = \sqrt{ (\alpha x,\alpha x) } = \sqrt{ \alpha \alpha^\ast (x,x) } = \abs{\alpha} \norm{x} \]
		\item Using \eqref{eqn:normPointSep} and again linearity we find
			\begin{align}
				\nonumber
				0 &\le \norm{x \pm y}^2 \\
				\nonumber
				  &= (x \pm y, x \pm y) = (x,x) \pm (x,y) \pm (y,x) + (y,y) \\
				\nonumber
				  &= (x \pm y, x \pm y) = (x,x) \pm (x,y) \pm (x,y)^\ast + (y,y) \\
				  &= \norm{x}^2 + \norm{y}^2 \pm 2\Re (x,y)
				\label{eqn:normInducedProof1}
			\end{align}
			Now define normalised vectors $\tilde{x} = \frac{x}{\norm{x}}$ and $\tilde{y} = \frac{y}{\norm{y}}$.
			In an analogous way to the negative branch in \eqref{eqn:normInducedProof1} above, we have
			\[ 0 \le \norm{\tilde{x}}^2 + \norm{\tilde{y}}^2 - 2\Re (\tilde{x},\tilde{y}) = 2 - 2 \Re (\tilde{x},\tilde{y}) ,\]
			such that $\Re  (\tilde{x},\tilde{y}) \le 1$.
			Now consider
			\begin{align}
				\nonumber
				\left(\norm{x} + \norm{y}\right)^2 - \norm{x + y}^2 &\stackrel{\eqref{eqn:normInducedProof1}}{=} 2 \norm{x} \norm{y} - 2 \Re (x,y) \\
				\nonumber
				&= 2 \norm{x} \norm{y} - 2 \Re ( \norm{x} \tilde{x}, \norm{y} \tilde{y} )  \\
				\nonumber
				&= 2 \norm{x} \norm{y} \left[ 1 - \Re (\tilde{x},\tilde{y}) \right] \\
				&\ge 0
				\label{eqn:normInducedProof2}
			\end{align}
			where we used linearity in the penultimate step, to get the (real) factors $\norm{x}$ and $\norm{y}$ out front. 
			Taking the square root proves the triangle inequality \eqref{eqn:normTriaIneq}.
	\end{itemize}
\end{proof}

\begin{cor}[Cauchy-Schwarz Inequality]
	\label{eqn:CS}
	Let $V$ be an inner product space with inner product $(\cdot,\cdot)$ and induced norm $\norm{\cdot}$. Then for all $x,y \in V$
	\[ \norm{x} \norm{y} \ge \Re (x,y) \]
	and furthermore
	\[ \sqrt{2} \norm{x} \norm{y} \ge \abs{(x,y)}. \]
\end{cor}
\begin{proof}
	The first part has been implicitly achieved in \eqref{eqn:normInducedProof2}. For the second part, note
	\[ \abs{(x,y)}^2 = \Big( \Re (x,y) \Big)^2 + \Big( \Im (x,y) \Big)^2  = \Big( \Re (x,y) \Big)^2 + \Big( \Re (y,x) \Big)^2 \le 2 \norm{x}^2 \norm{y}^2.\]
	Taking the square-root shows the claim.
\end{proof}

\noindent
Finally, we arrive at the definition of a \emph{Hilbert space}:
\begin{defn}
	A Hilbert space $V$ is an inner product space over the field $\set{F}$, that is complete with respect to the induced norm.
\end{defn}

\begin{nte}
	A formal discussion of completeness is omitted here.
	The picture we developed in section \vref{ideaCompleteness} will be sufficient for our purposes and shall serve as an intuitive guideline whenever the term \emph{complete} arises. \\

	As we said in the last chapter, Euclidean space is indeed complete under its induced norm and can hence be regarded as a Hilbert space.
	We therefore succeeded in finding an abstract formalism that represents large parts of the internal mathematical structure of Euclidean space.
\end{nte}

\section{Lebesgue space}
In this subsection we will try to get a little insight into the space of square-integrable functions $L^2$.
As it will turn out later, this space is the mathematical object on which Dirac Notation lives in most practical scenarios.
In a similar fashion to completeness, the precise formulation of \emph{integrability} is well out of scope here\footnote{In fact the whole topic of measure theory deals with just this definition.}. 
For us this section is merely meant to scratch the surface and introduce the formalism we will need later on. \\

\noindent
The first problem one runs into very quickly when considering square-integrable functions is that our familiar Riemann integral definition is too tight. 
This means that there are too few functions that can be properly Riemann-integrated in order to prove the theorems in this subsection.
The way to make progress is to broaden the definition of the integral and integrability. 
Hence we consider

\begin{rem}[The Lebesgue integral] 
	\label{intLebesgue}
	Instead of a formal definition of the Lebesgue integral, we will just state a few of its properties in order to get a feeling for how it works.
	\begin{itemize}
		\item Whenever the Riemann integral exists, the values of the Riemann and the Lebesgue integrals agree. 
			The only difference between the two integral definitions is that there exist some ``problematic cases'' where the Lebesgue integral still allows integration, but the Riemann integral does not.
			So if the Riemann integral exists (i.e. if we can do the usual integration and the result is a finite value), we can evaluate it and we are done for the calculation of the integral in both definitions.
		\item For a Lebesgue integral over a domain $\Omega \subset \R^1$, changing the value of the integrand at up to countably infinite number of points (in an arbitrary way) does not change the value of the integral. 
			Note, that this means for example 
			\begin{align}
				\nonumber
				&\int_\R \left\{
				\begin{array}{ll}
					1 & \text{for $x \in \set{Q}$} \\
					0 & \text{for $x \in \R \backslash \set{Q}$}
				\end{array}
				\right\} \D[x] &&\text{only Lebesgue-integrable} \\
				%
				\nonumber
				= &\int_\R 0 \D[x] && \text{Lebesgue- and Riemann-integrable}\\
				= &\int_\R \left\{
				\begin{array}{ll}
					1 & \text{for $x \in \{\pi, e, 3, 15, 42\}$} \\
					0 & \text{else}
				\end{array}
				\right\} \D[x] &&\text{Lebesgue- and Riemann-integrable}, 			
				\label{exampleLebesqueIntegral}
			\end{align}
			which trivially evaluates to zero (Consider the Riemann-integrable second term).
			For higher dimensions analogous statements are true:
			e.g. for integrals over subsets of $\R^2$, we can change the value of the integrand at up to countably infinite number of lines or points; 
			similarly for 3D integrals at planes, lines or points, \ldots. 
			Often this allows us to alter an integrand in a way that integration in a Riemann sense is possible. 
			For example in the first integral of \eqref{exampleLebesqueIntegral}, we changed the points where the function is 1 to 0 in order to get a Riemann-integrable version. 
			This is possible since $\set{Q}$ is a set with only up to countably infinite number of elements\footnote{It is not necessarily intuitive to understand, what this means. Just imagine there are two kinds of infinities: For one kind one can abstractly speaking find a way to enumerate each element (the ``countable infinity''), for the other one this is not possible.}.
		\item A direct consequence of the above point is that for each $\Omega \subset \R^n$ there exist an infinitely large number of functions $f : \Omega \to \C$, such that $\int_\Omega f \D[x] = 0$. This will be important very soon.
		\item From now on all integrals in this script are to be understood as Lebesgue integrals.
	\end{itemize}
\end{rem}

\noindent
That being said, let us consider complex integrability. 
\begin{defn}
We call a \emph{complex-valued} function $g : \Omega \to \C$ \emph{integrable} on the domain $\Omega \subset \R^n$ if
	\begin{equation}
		\abs{ \int_\Omega g(x) \D x } < \infty,
		\label{defIntegrable}
	\end{equation}
i.e. if the modulus of the integral over the domain remains finite. 
\end{defn}
\begin{defn}
	A complex-valued function $f : \Omega \to \C$ is called \emph{square-integrable} on the domain $\Omega \subset \R^n$ if
	the function $\abs{f}^2 : \Omega \to \R : x \mapsto \abs{f(x)}^2$ is integrable on that very same domain. 
	This means nothing else but 
	\[ \int_\Omega \abs{f}^2 \D x < \infty. \]
\end{defn}

\nomenclature{$L^2(\Omega, \C)$}{The Hilbert space of square-integrable complex-valued functions, see \vref{lem:L2HilbertSpace}}
\begin{nte}
	Usually one denotes the set of square-integrable functions as
	\begin{equation} 
		L^2(\Omega, \C) := \big\{ f : \Omega \to \C~\big|~\text{$f$ is square-integrable} \big\}.
		\label{defL2}
	\end{equation}
\end{nte}

\pagebreak[2]
\noindent
Before we will eventually proceed to show that $L^2(\Omega)$ is actually a Hilbert space we need a few more intermediate results.
\begin{lem}[Minkowski’s inequality in $L^2$]
	\label{lemMinkowski}
	Let $f,g \in L^2(\Omega)$, i.e. square-integrable, then
	\begin{itemize}
		\item $f + g$ is square-integrable on $\Omega$ as well
		\item $\sqrt{\int_\Omega \abs{f + g}^2 \D[x]} \le \sqrt{\int_\Omega \abs{f}^2 \D[x]} + \sqrt{\int_\Omega \abs{g}^2 \D[x]}$
	\end{itemize}
\end{lem}
\begin{proof}
	See for example \cite{Adams2003}.
\end{proof}

\begin{nte}
	When we say the resulting function $f \circ g$ of an operation $\circ$ is to be \emph{defined pointwise},
	we mean that $f\circ g$ can be defined by considering the action of $\circ$ in each point $x \in \Omega$ on the function values $f(x)$ and $g(x)$.
	For example the function $f+g$ from lemma \ref{lemMinkowski} could be defined as the function for which
	\[ \Big(f+g\Big)(x) = f(x) + g(x) \qquad \forall x \in \Omega.\]
	Similarly $\Re f$ is the function with
	\[ \Big( \Re f \Big)(x) = \Re \Big( f(x) \Big) \qquad \forall x \in \Omega.\]
\end{nte}

\begin{lem}
	\label{aLittleHoelder}
	For all complex-valued functions $f,g$, which are defined and integrable on $\Omega \subset \R^n$
	\[ \int_\Omega \abs{ f g } \D x \le \sqrt {\int_\Omega \abs{f}^2 \D x} \cdot \sqrt{\int_\Omega \abs{g}^2 \D x}. \]
\end{lem}
\begin{proof}
	This is a special case of the so-called Hölder's inequality, a proof of which can be found in \cite{Adams2003}.
\end{proof}
\noindent
From this we deduce
\begin{lem}
	\label{WellDefL2InnProd}
	For all complex-valued functions $f,g$ integrable on $\Omega \subset \R^n$, it holds
	\begin{align*}
		&\abs{ \int_\Omega f g^\ast \D x } \le \sqrt{ \int_\Omega \abs{f}^2 \D x} \cdot \sqrt{\int_\Omega \abs{g}^2 \D x} \\
		\intertext{and}
		&f,g \in L^2(\Omega) \quad \Rightarrow \quad \text{$f g^\ast$ is integrable},
	\end{align*}
	where the function $f g^\ast$ is to be constructed from $f$ and $g$ in a pointwise sense.
\end{lem}
\begin{proof}
	Consider first an integrable function $h$ which is defined on $\Omega \subset \R^n$, we have
	\[ \int_\Omega \abs{h} \D x = \int_\Omega \sqrt{ \left( \Re h \right)^2 + \left( \Im h \right)^2 } \D x \ge \int_\Omega \abs{ \Re h } \D x.\]
	Noting that $\abs{ \Re h } \ge \pm \Re h$ and thus 
	\[ \int_\Omega \abs{ \Re h } \D x \ge \pm \int_\Omega \Re h \D x\]
	we get
	\begin{equation}
		\int_\Omega \abs{h} \D x \ge \abs{\int_\Omega \Re h \D x}.
		\label{ProofWellDefL2InnProd1}
	\end{equation}
	Now note that for each complex number $z$ we may find a $\phi \in \R$ such that $z\exp(\I \phi)$ is a real number. 
	So specifically we can choose $\phi$ such that
	\begin{align*}
		\exp(\I \phi) \int_\Omega h \D x &= \Re \left( \exp(\I \phi) \int_\Omega h \D x \right) \\
		&=\Re \left(  \int_\Omega \exp(\I \phi) h \D x \right) \\
		&=\int_\Omega \Re \Big( \exp(\I \phi) h \Big) \D x \\
	\end{align*}
	and therefore
	\[ \abs{\int_\Omega h \D x} = \abs{\exp(\I \phi) \int_\Omega h \D x} = \abs{\int_\Omega \Re \Big( \exp(\I \phi) h \Big) \D x} \stackrel{\eqref{ProofWellDefL2InnProd1}}{\le} \int_\Omega \abs{\exp(\I \phi)  h} \D x = \int_\Omega \abs{h} \D x \]
	Now set $h = f g^\ast$ and use lemma \ref{aLittleHoelder} to obtain the first result
	\begin{equation}
		\abs{ \int_\Omega f g^\ast \D x } \le \int_\Omega \abs{f g^\ast}\D x = \int_\Omega \abs{f g}\D x \le  \sqrt{\int_\Omega \abs{f}^2 \D x} \cdot \sqrt{\int_\Omega \abs{g}^2 \D x}.
		\label{ProofWellDefL2InnProd2}
	\end{equation}
	If furthermore $f,g \in L^2(\Omega)$ then the integrals in the RHS of \eqref{ProofWellDefL2InnProd2} are finite (since the functions are square-integrable) and thus
	\[ \abs{ \int_\Omega f g^\ast \D x } \le \int_\Omega \abs{f g}\D x \le  \sqrt{\int_\Omega \abs{f}^2 \D x} \cdot \sqrt{\int_\Omega \abs{g}^2 \D x} < \infty, \]
	which implies the second result, that $f g^\ast$ is integrable.
\end{proof}

%TODO
\begin{rem} For this lemma it is easy to see that an equivalent version cannot be constructed if the Riemann integral definition is to be used. 
	To see this first define the two complex-valued functions $f,g : \R \to \C$ with
	\begin{align*}
		f(x) &= \left\{
		\begin{array}{ll}
			-1 & \text{for $x \in \set{Q}$} \\
			1 & \text{for $x \in \R \backslash \set{Q}$}
		\end{array}
		\right.
		&
		g(x) &= \left\{
		\begin{array}{ll}
			\I & \text{for $x \in \set{Q}$} \\
			1 & \text{for $x \in \R \backslash \set{Q}$}
		\end{array}
		\right. .
	\end{align*}
	Clearly 
	\[ \forall x \in \R:\qquad \abs{f(x)} = \abs{g(x)} = 1,\]
	such that in both the Riemann and the Lebesgue sense
	\[ \int_{[0,1]} \abs{f}^2 \D x = \int_{[0,1]} 1 \D x = 1 = \int_{[0,1]} 1 \D x = \int_{[0,1]} \abs{g}^2 \D x.\]
	This makes $f$ and $g$ square-integrable functions on the domain $\Omega = [0,1]$. 
	On the other hand 
	\[ \big(f g^\ast\big)(x) = \left\{
		\begin{array}{ll}
			\I & \text{for $x \in \set{Q}$} \\
			1 & \text{for $x \in \R \backslash \set{Q}$}
		\end{array}
		\right., \]
	which makes $(f g^\ast)$ Lebesgue, but not Riemann integrable. 
	For lemma \ref{WellDefL2InnProd} this provides a counterexample if the Riemann integral definition is used.
\end{rem}

\begin{lem}
	\label{lem:L2HilbertSpace}
	For $\Omega \subset \R^n$, $L^2(\Omega)$ is an inner product space over $\C$ with the inner product given by the form
	 \begin{equation}
		(u,v)_\Omega = \int_\Omega u v^\ast \D x.
		\label{L2innerProduct}
	\end{equation}
	and a pointwise definition of vector addition and scalar multiplication, i.e. for functions $f$ and $g$
	\[ \Big(f+g\Big)(x) = f(x) + g(x) \qquad \forall x \in \Omega\]
	and a scalar multiple of $f$ with $\alpha \in \C$
	\[ \Big(\alpha f\Big)(x) = \alpha f(x)\qquad \forall x \in \Omega.\]

\end{lem}
\begin{proof}
	First of all we need to verify that the pointwise vector operations are well-defined. 
	This is the case if for all $f,g \in L^2(\Omega)$ and all $\alpha \in \C$ uniquely defined functions $\alpha f$ and $f+g$ exist that are also a member of $L^2(\Omega)$.
	Uniqueness follows from the properties of $\C$. 
	For the sum $f+g$ the first part of Minkowski’s inequality \eqref{lemMinkowski} guaranties that $f+g \in L^2(\Omega)$ if $f,g \in L^2(\Omega)$.
	For the scalar product this is a consequence of the linearity of the integral, i.e.
	\begin{align*}
				&\text{$f$ is square-integrable on $\Omega$} \\
		\Leftrightarrow \quad &\int_\Omega \abs{f(x)}^2 \D x < \infty \\
		\Leftrightarrow \quad& \int_\Omega \abs{\alpha f(x)}^2 \D x = \abs{\alpha}^2~\underbrace{\int_\Omega \abs{f(x)}^2 \D x}_{<\infty} < \infty \qquad \forall \alpha \in \C \\
		\Leftrightarrow \quad& \text{$\alpha f$ is square-integrable on $\Omega$} \qquad \forall \alpha \in \C
	\end{align*}
	Showing now that $L^2(\Omega)$ is a vector space is pretty simple.
	The zero vector is just the zero function and the additive inverse to $f$ is $-f$, i.e. the function with the signs of all function values reversed. 
	To illustrate the argument we will show that the distributivity relation \eqref{vectorDistrVecAdd} holds. 
	The remaining steps are left as an exercise for the reader.
	
	\noindent
	So consider arbitrary $\alpha \in \C$ and arbitrary $f,g \in L^2(\Omega)$. We have 
	\[ \forall x \in \Omega: \qquad \Big( \alpha (f+g) \Big)(x) = \alpha (f(x) + g(x)) = \alpha f(x) + \alpha g(x) = \Big( \alpha f + \alpha g \Big)(x). \]
	Therefore the functions given by $\alpha (f+g)$ and $\alpha f + \alpha g$ have to be identical (since they are identical at all points $x$), which indeed proves \eqref{vectorDistrVecAdd}. \\

	\noindent
	Now we try to show that $L^2(\Omega)$ is an inner product space. 
	The integral in the definition of the inner product \eqref{L2innerProduct} exists due to the second part of lemma \ref{WellDefL2InnProd}, which gives that $u v^\ast$ is integrable.
	Conjugate symmetry \eqref{innProdConjSym} and linearity in the first argument \eqref{innProdLinLeft} are trivially satisfied by \eqref{L2innerProduct}, due to the properties of the integral itself.
	Furthermore for all $f \in L^2(\Omega)$
	\[(f,f)_\Omega = \int_\Omega f f^\ast \D x = \int_\Omega \abs{f}^2 \ge 0.\]
	The remaining point to show in order to verify that \eqref{L2innerProduct} gives indeed a positive-definite inner product is
	\begin{equation}
		(f,f)_\Omega = 0 \Rightarrow f = 0.
		\label{eqnL2PosDef}
	\end{equation}
	Recall the properties of the Lebesgue integral from remark \ref{intLebesgue}. 
	If the integral in $(f,f)_\Omega$ is zero for one specific function $f$ then we can construct arbitrarily many functions for which this is the case as well.
	In a strict sense statement \eqref{eqnL2PosDef} can hence never be satisfied. 
	The mathematical trick we employ here to overcome this limitation is to redefine the meaning of the equality $=$ when comparing functions from $L^2(\Omega)$.
	One calls this new meaning of $=$ ``equality almost everywhere'' and it describes something along the lines\footnote{The mathematical precise formalism uses the notion of an equivalence relation and factor groups for the proper definition of $L^2(\Omega)$.} of
	\[ f = g \quad \text{almost everywhere} \qquad \Leftrightarrow \qquad \int_\Omega f \D x = \int_\Omega g \D x .\]
	In this weaker sense of equality between the $L^2(\Omega)$ functions $f$ and $0$, the relation \eqref{eqnL2PosDef} can indeed be proved, which makes $L^2(\Omega)$ a proper inner product space.
\end{proof}

\noindent
The last step --- showing that $L^2(\Omega)$ is a Hilbert space --- is pretty involved and we will just state the result here:
\begin{thm}
	\label{l2hilbert}
	For (most) $\Omega \subset \R^n$, $L^2(\Omega)$ is a Hilbert space under the induced norm
	\begin{equation}
		\norm{f}_{L^2(\Omega)} = \sqrt{(f,f)_\Omega} = \sqrt{\int_\Omega \abs{f}^2 \D x} .
		\label{L2norm}
	\end{equation}
\end{thm}
\begin{proof}
	See \cite{Adams2003}
\end{proof}

\begin{rem}
	Now what have these 4 pages of (more or less) rigorous mathematical labour given us?
	The important point is that we were able to show how the infinite dimensional function space $L^2(\Omega)$ behaves abstractly very similar to the finite dimensional Euclidean space.
	In other words one could well do analysis or linear algebra with functions instead of numbers or vectors. 
	This might seem a little odd in the first place, but the fact that $L^2(\Omega)$ is a Hilbert space lets us deal with this in a fairly intuitive way.
	For Quantum Mechanics, where the wave functions are pretty much all members of (subspaces of) $L^2(\Omega)$, getting an idea of the structure of $L^2(\Omega)$ and the operations we can perform in this space can become quite important.
\end{rem}

\section{Sobolev spaces}
\nomenclature{$H^1(\Omega, \C)$}{The Hilbert space of complex-valued functions with square-integrable first derivative}
\nomenclature{$H^2(\Omega, \C)$}{The Hilbert space of complex-valued functions with square-integrable second derivative}



Sobolev spaces lift 



\begin{defn}
	% bounded
\end{defn}

\begin{defn}
	A metric space $M$ is complete if every sequence in $M$
	which is Cauchy converges to a limit, which is in $M$ as well.
\end{defn}

\begin{defn}
	A set is compact if it is bounded and complete.
\end{defn}

\begin{defn}
	A subset $S$ is dense in a complete metric space $M$ if it is a proper subset
	and each sequence in $S$ which is Cauchy converges to a point in $M$.
\end{defn}

$C^\infty_0$ is dense in $L^2$ (Hachbusch 1986)
and dense in $H^1$

also define $H_0^n(\Omega, \C)$
Give inclusions including $C^\infty_0$
Mention what is dense


\section{Spectral theory}
\todo[inline,caption={}]{
	\begin{itemize}
		\item Elaborate on the spectral theory of bounded operators
		\item Sturm-Liouville theory
		\item See Erics notes
		\item Keep it short, just what is needed for HF
	\end{itemize}
}

\url{https://en.wikipedia.org/wiki/Self-adjoint_operator}
\url{https://en.wikipedia.org/wiki/Spectral_theorem}
\url{https://en.wikipedia.org/wiki/Extensions_of_symmetric_operators}

% Discuss the spectral properties of the Laplace and Schrödinger operators


% Important:
%https://en.wikipedia.org/wiki/Self-adjoint_operator
%If A is symmetric and D o m ( A ) = H {\displaystyle \mathrm {Dom} (A)=H} {\displaystyle \mathrm {Dom} (A)=H}, then A is necessarily bounded.
%Hence if the operator is not bounded (as most operators in QM) in cannot be self-adjoint.


self-adjoint operator
(show that it has real eigenvalues)

symmetric operator


% This section should make clear
% The importance of the Sobolev space $H_2$ for quantum mechanics

\subsection{The Laplace operator}
\label{sec:SpectrumLaplace}
Let us consider the $N$-dimensional \newterm{Laplace operator}
in Cartesian coordinates,
\[ \Delta = \sum_{i=1}^d \frac{\partial^2}{\partial x_i^2}. \]
This operator plays a crucial role in quantum mechanics.
We will see later\todo{refer} that this operator is part of the
kinetic operator of the Schrödinger equation.
For a physical equation we expect that the operator has
real, i.e. measurable, eigenvalues.
This typically requires the operator to be self-adjoint.
In other words from a physical standpoint we would expect
the Laplace operator to have real eigenvalues.

As it turns out,
the Laplacian operator $T_0 = \Delta$ with the domain $D(T_0) = L^2(\R^d)$
does not have \emph{any}
eigenfunctions $u \in L^2(\R^d)$,
not even in the weak sense of distributions~\cite{Helffer2013}.
In other words we are unable to find an eigenfunctions $u \in L^2(\R^d)$
and a $\lambda \in \R$,
such that it holds
\[ \forall \phi \in C^\infty_0(\R^d):  \int_{\R^d}\phi(x) (\Delta u)(x) \D x = \lambda \int_{\R^d} \phi(x) u(x) \D x. \]
Furthermore the Laplacian operator $T_0$ is not self-adjoint.

It turns out that the domain of the operator is the culprit here.
Converse to the problem we posed it is actually possible
to find \emph{for each} $\lambda \in \C$ an eigenfunction from the
slightly different so-called space of distributions $D'(\R^d)$.
This suggests that the construction of an appropriate, related
operator $T_1$, for which we can find an eigenspectrum.
In fact one may do so in a way, that $T_1$ is even self-adjoint
by the means of the \newterm{Friedrichs extension}.
In the particular case we discussed, there are actually
multiple ways to achieve this.
One way assumes a Dirichlet boundary, where the potential
eigenfunctions go to zero
one obtains the restricted domain
$D(T_1) = H^2(\R^d) \cap H^1_0(\R^d)$
for the self-adjoint Laplacian extension.
Unlike for other operators such an extension for the Laplacian
is not unique.
Another construction assuming a Neumann boundary is possible, too.
It again yields a subset of $H^2(\R^d)$ just with an appropriate
so-called trace condition enforcing the derivative to go to zero
at the boundary.
Most notably this implies that only under specification
of the boundary condition the operator has an eigenspectrum.
This is an observation which is true for many operators in quantum mechanics.

If $u \in H^2(\R^d)$ then $\Delta u \in L^2(\R^d)$

\subsection{The Laplace-Beltrami operator on the unit sphere}
\label{sec:SpectrumLaplaceBeltrami}
\nomenclature{$Y_l^m(\theta, \varphi)$}{Spherical harmonic function with angular
momentum quantum number $l$ and azimuthal quantum number $m$}
\nomenclature{$P^{m}_l$}{Associated Legendre polynomial with orders $l$ and $m$.}

In the previous section, we saw that the Laplace operator $\Delta$
is self-adjoint and has hence real eigenvalues provided that
we choose for its domain a subdomain of $D(\Delta) = H^2(\R^3)$.

In this section we want to discuss the solution of Laplace's equation
on the surface of the unit sphere
\[
	\set{S}^2 := \{ \vec{r} \in \R^3 | x^2 + y^2 + z^2 = 1 \}.
\]
For this it is most convenient to consider
the spherical polar coordinate system,
\ie instead of specifying the vector $\vec{r} = (x, y, z)^T$,
we specify the parameters $r, \theta, \phi$ or
\[ \vec{r} = \mm{ r \, \sin \theta \, \cos \phi \\ r \, \sin \theta \sin \phi \\
r \, \cos \theta }. \]
The condition for the unit sphere than reduces to $r=1$

Since the sphere is no longer a Euclidean geometry
but much rather a curved manifold some of the treatment presented
in the previous chapter no longer holds.
Nevertheless an equivalent to the Friedrichs extension can be found
in order to make the Laplace-Beltrami operator
\[ \Delta_{\set{S}^2} u = \frac{1}{\sin \theta} \frac{\partial}{\partial \varphi}
\left( \sin \varphi \frac{\partial u}{\partial \varphi}  \right)
+ \frac{1}{(\sin \varphi)^2} \frac{\partial^2}{\partial \theta^2} u\]
self-adjoint.
\todoil{Insert details}

The above polar expression for the Laplace-Beltrami operator
allows to explicitly solve the equation
\newcommand{\laplaceSphere}{\Delta_{\set{S}^2}}
\[ \laplaceSphere u = \lambda u \]
for eigenfunctions $u$ and eigenvalues $\lambda$,
which results in the spherical harmonics
\[ Y_l^m(\theta, \varphi) = N_{lm} P_l^m(\cos \theta) e^{im\varphi} \]
where $N_{lm}$ is a normalisation constant
and eigenvalues $-l (l+1)$:
\begin{equation}
	-\laplaceSphere Y_l^m(\theta, \varphi) = l (l+1) Y_l^m(\theta, \varphi)
	\label{eqn:Laplace}
\end{equation}
Importantly the quantum number $l$ is positive and $m$ is restricted by
\[ -l \le m \le l, \]
such that there are exactly $(2l+1)$ spherical harmonics with the same eigenvalue
$-l (l+1)$.

For our following arguments it will be important to note
that the 3d Laplacian in spherical polar coordinates
and the Laplace-Beltrami operator for the unit sphere
are related by
\newcommand{\laplaceRadial}{\frac{\partial}{\partial r} \left( r^2 \frac{\partial}{\partial r} \right)}
\begin{equation}
r^2 \Delta = \laplaceRadial + \laplaceSphere
	\label{eqn:LaplaceCorrespondance}
\end{equation}
such that
\begin{equation}
	- r^2 \Delta Y_l^m(\theta, \varphi) = -\laplaceSphere Y_l^m(\theta, \varphi) = l (l+1) Y_l^m(\theta, \varphi)
	\label{eqn:LaplaceSphericalHarmonic}
\end{equation}


\subsection{The Schrödinger operator for a Hydrogenic atom}
We will discuss in the next chapter the physical origin.
For now we will just state that the Schrödinger operator for the Hydrogenic atom
(in so-called atomic units)
is given by the expression
\begin{equation}
	\Op{H} = - \frac12 \Delta - \frac{Z}{r} = -\frac{1}{2r^2} \laplaceRadial -\frac{1}{2r^2} \laplaceSphere - \frac{Z}{r}
	\label{eqn:HydrogenOperator}
\end{equation}
in other words the scaled Laplace operator and a term for a radial-symmetric Coulombic
potential.

The Hilbert space for this operator is $L^2(\R^3, \C)$
and the domain is $C^\infty_0(\R^3, \C)$
By a similar argument to section \vref{sec:SpectrumLaplace} based on the
Friedrichs extension one notices that this operator is self-adjoint
if the domain is chosen to be $H^2(\R^3, \C$.
Let us now find the analytic expressions of the eigenfunctions of this operator,
i.e. solve the time-independent Schrödinger equation
\begin{equation}
	( \Op{H} - E ) \Psi = 0
	\label{eqn:HydrogenEigenproblem}
\end{equation}
where $\Psi \in L^2(\R^3, \C)$ and $E \in \R$.
\todoil{More details on this later}

Without jumping ahead too far let us assume that both the energy as well
as the state $\Psi$ are characterised by three quantum numbers $n$, $l$ and $m$.
A further careful inspection of \eqref{eqn:HydrogenEigenproblem}
in contrast with \eqref{eqn:Laplace} suggests a product ansatz
\[ \Psi_{nlm}(\vec{r}) = R_{nl}(r) Y_l^m(\theta, \phi) \]
This yields to the radial equation
\begin{equation}
	- \left( \frac{1}{2r^2} \laplaceRadial + \frac{l (l+1)}{2 r^2} - \frac{Z}{r} - E \right) R_{nl}(r) = 0
	\label{eqn:HydrogenRadial}
\end{equation}
which has the solutions~\cite{Mueller2000}
\begin{equation}
	 R_{nl}(r) = N_{nl} \left(\frac{2Zr}{n}\right)^l \exp\left(-\frac{Zr}{n} \right)
\;_1F_1\left(l+1-n | 2l+2|\frac{2Zr}{n}\right)
	\label{eqn:HydrogenRadialSolution}
\end{equation}
where $_1F_1\left(a|b|\zeta\right)$ is a confluent hypergeometric function,
namely~\cite{Avery2006}
\[ _1F_1\left(a \middle| b \middle| \zeta\right) =
\sum_{k=0}^\infty \frac{a^{\bar{k}}}{k! \, b^{\bar{k}}} \zeta^k =
1 + \frac{a}{b} \zeta + \frac{a(a+1)}{2b(b+1)} \zeta^2 + \cdots \]
where $a^{\bar{k}}$ is the rising factorial of $a$.
The normalisaton constant is
\[ N_{nl} = \frac{2 \left( \frac{Z}{n} \right)^{3/2}}{(2l+1)!} \sqrt{ \frac{(l+n)!}{n (n-l-1)!}} \]
The energy eigenvalues are
\[ E_{nlm} = - \frac{1}{2n^2}. \]
\todoil{One should mention that these are all to be found,
	since the radial functions are complete in the radial sense
and the spherical harmonics are complete in the angular sense.}

If one walks through the derivation properly,
one notices that the principle quantum number $n$ is always larger than zero
and that $l < n$ applies as well.
\todoil{If I remember correctly this is a result of Sturm-Liouville theory}
Together with the restrictions on $l$ and $m$ itself this results in the
following restrictions on the quantum numbers:
\begin{align*}
	n &> 0 & 0 \leq l &< n & -l \leq &m \leq l
\end{align*}
Taking a look at the confluent hypergeometric function we use, it is easy to see
that the power of the polynomial in $r$ is between zero and
\[
	\deg \;_1F_1\left(l + 1 -m \middle| 2l+2 \middle| \frac{2Zr}{n} \right) = n - l -1 \]
such that overall the polynomial part of the radial function
has a power between $l$ and
\[ \deg \frac{R_{nl}(r)}{\exp(2Zr/n)} = n - 1. \]

We will now argue that the solutions $\Psi \in H^1(\R^3, \C)$.
It is easy to see that the derivative of $\Psi$ is continuous and bound
everywhere but the origin.
See \cite{Kato1957} and references therein for details.
Since the smoothness at the origin is better the larger
the larger the exponents of the polynomial in $r$,
let us consider the worst case, i.e. the $s$-functions with $l = 0$.
Taking a look at the definition of $_1F_1(a,b,\zeta)$
one notices that the smallest exponent, irrespective of the value of $n$ is $0$.
Thus we will discuss the smoothness properties of
\[
	\Psi_{1s}(r, \theta, \phi) = \Psi_{100}(r, \theta, \phi)
	= \sqrt{\frac{Z^3}{\pi}} \exp(-Z r)
\]
at the origin,
which in fact is the least smooth of all hydrogen solutions.

Since this function is radially symmetric,
we only need to consider the derivative along one direction.
Without loss of generality we choose the $x$ direction.
Taking the first derivative we find
\[
	\frac{\partial \Psi_{1s}}{\partial x} = -\sqrt{\frac{Z^5}{\pi}} \frac{x}{r} \exp(-Z r)
\]
As expected at the origin we encounter a discontinuity in the derivative.
Inserting $x = r \sin\theta \cos \phi$ we do notice
\[
	\lim_{r \to 0} \frac{\partial \Psi_{1s}}{\partial x} = -\sqrt{\frac{Z^5}{\pi}} \sin\theta \cos \phi,
\]
which stays finite. The derivative is thus bounded at zero as well
and as a result square-integrable.
On the other hand the second derivative
\[
	\frac{\partial^2 \Psi_{1s}}{\partial x^2}
	= \left( -\frac{Z x^2}{r^2} -\frac{x^2}{r^3} + \frac{1}{r} \right)
		\sqrt{\frac{Z^5}{\pi}} \exp(-Z r)
\]
is unbound at the origin.
Careful inspection shows that it is not square-integrable.
\todoil{Do that}
Therefore $\Psi_{1s} \in H^1(\R^3,\C)$, but $\Psi_{1s} \not\in H^2(\R^3, \C)$.







Show that Solutions are in $H^2$ by considering
the least smooth (i.e. the 1s orbital) and showing that this
is in $H^2$.
That way we know that $H^1$ is a good space for approximating the solutions
$\Rightarrow$ FE formalism can be applied.

\todoil{Try to check whether a productansatz for regularising the nuclear potential
	in combination with CS for the nuclear cusp factor and FE for the rest
could actually work}


% Derivative is smooth and bounded everywhere but at origin
% Argue that the worst function is for l = 0.
% Take derivative wrt. x
% Then show that it's absolute value at the origin just contains a hole
% hence derivative is L^2 integrable in the weak sense


\section{Optimisation theory}
\todo[inline,caption={}]{
	\begin{itemize}
		\item Elaborate on Optimisation theory
		\item See Erics notes
		\item Keep it short, just what is needed for HF
	\end{itemize}
}

\section{Numerical solution of partial differential equations}
We can relax $H^2(\R^3)$ to $H^1(\R^3)$ if we use test functions
from $H^1(\R^3)$ as well in Ritz-Galerkin scheme.
\todo{Ask guido}
Is this the energetic extension
\url{https://en.wikipedia.org/wiki/Energetic_space}

\todo[inline,caption={}]{
	\begin{itemize}
		\item Define the problem
		\item Galerkin projection
	\end{itemize}
}

\subsection{Introduction to Finite Elements}
\todo[inline,caption={}]{
	\begin{itemize}
		\item Basic ideas and properties
	\end{itemize}
}

% Use as an example to explain the ideas of finite elements
An important equation from classical electrodynamics which
involves the Laplace operator is Poisson's equation
\begin{equation}
	\Delta V_H(\vec{r}) = \rho(\vec{r})
	\label{eqn:Poisson}
\end{equation}
relating the potential to the charge density $\rho$ which generates it.


\subsection{Diagonalisation algorithms}
\todo[inline,caption={}]{
	\begin{itemize}
		\item Have this section from 99algos here or elsewhere?
	\end{itemize}
}

% TODO 
%  Note that norm wrt fock operator F is equivalent to H1 norm due to Laplace part
%  Lipschitz continuity
%  well-posedness

% TODO Talk about convergence rates?

