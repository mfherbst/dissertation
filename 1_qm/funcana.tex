\section{Elements of functional analysis}
\label{sec:FunAna}

The mathematical field of functional analysis
is concerned with the study of Banach and Hilbert spaces
as well as the properties of mappings between such structures.
In this work we will neglect Banach spaces
and focus on Hilbert spaces only
due to their exceptional importance in the mathematical structure
of quantum mechanics, see the previous section \ref{sec:IntroQM}.
After some general remarks, we will take a closer look
at the Lebesgue space $L^2(\R^d, \C)$
as well as Sobolev spaces in the context of \QM.

In this section we assume familiarity with the concept of a vector space
as well as some intuitive understanding of the Lebesgue integral.
For a more detailed discussion developing such concepts
by generalising standard Euclidean geometry,
see~\cite{DiracNotation}.

\subsection{Definition of Hilbert spaces}
\label{sec:Hilbert}
\nomenclature{$\F$}{Either the field of real numbers $\R$ or the field of complex numbers $\C$.}

Hilbert spaces are generalising some concepts of
two- or three-dimensional Euclidean space to
larger vector spaces of possibly infinite dimensions.
Most notably taking limits or computing lengths and angles
is possible in the same way as for Euclidean geometry,
thus allowing to perform vector calculus or
to numerically approximate in a sound way.
Same as vector spaces, Hilbert spaces are defined with respect
to a field $\F$, see definition below.
In our case $\F$ can be typically identified
with the field of all complex numbers $\C$ or the real numbers $\R$.

The first ingredients to a Hilbert space are ways to measure
angles and distances, \ie an inner product and a norm.
\begin{defn}
	\label{def:InnerProduct}
	An \newterm{inner product space} over a field $\F$
	is a vector space $V$ (over the same field)
	that is further equipped with an \newterm{inner product}, i.e. a map
	\[ \braket{\slot}{\slot}_V : V \times V \to \F \]
	that satisfies%
	(for all vectors $x,y,z \in V$ and all $\alpha \in \set{F}$)
	\begin{align}
		\label{eqn:innProdConjSym}
			&\braket{x}{y}_V^\ast = \braket{y}{x}_V &&
			\text{\textit{(conjugate symmetry)}} \\
		\label{eqn:innProdLinRight}
			&\braket{x}{\alpha y + z}_V = \alpha \braket{x}{y}_V + \braket{x}{y}_V &&
			\text{\textit{(linearity in the last argument)}} \\
		\label{eqn:innProdPosDef}
			&\braket{x}{x}_V \ge 0 \quad \text{and}
			\quad \braket{x}{x}_V = 0 ~\Rightarrow~ x = 0  &&
			\text{\textit{(positive-definiteness)}},
	\end{align}
	where the asterisk ``$^\ast$'' denotes complex conjugation.
	We typically drop the ``$V$'' subscript from the notation of the inner
	product if the underlying vector space is clear from context.
\end{defn}

\begin{rem}
	Some literature uses a deviating definition for the inner product,
	where not the second,
	but the first argument in \eqref{eqn:innProdLinRight} is linear,
	\ie where \eqref{eqn:innProdLinRight} would be replaced by
	\[ \braket{\alpha y + z}{x}_V = \alpha \braket{y}{x}_V + \braket{z}{x}_V. \]
	Our definition is in better agreement with the usual convention
	of quantum physics and quantum chemistry
	due to the resemblance of Dirac notation~\cite{DiracNotation}.
\end{rem}

\begin{defn}
	Given a vector space $V$ over the field $\F$, a \newterm{norm} is a map
	$\norm{\slot} : V \to \R$
	such that the following axioms hold for all vectors $x,y \in V$ and all $\alpha \in \set{F}$:
	\begin{align}
		\label{eqn:normScalability}
			&\norm{\alpha x} = \abs{\alpha} \, \norm{x} &&
			\text{\textit{(absolute scalability)}} \\
		\label{eqn:normTriaIneq}
			&\norm{x + y} \le \norm{x} + \norm{y} &&
			\text{\textit{(triangle inequality)}} \\
		\label{eqn:normPointSep}
			&\text{If}~\norm{x} = 0 \quad \Rightarrow
			\quad \text{$x$ is the zero vector} &&
			\text{\textit{(norm separates points)}}
	\end{align}
	If such a norm can be found for a particular vector space $V$,
	one typically refers to $V$ as a \newterm{normed vector space} as well.
\end{defn}

\begin{prop}
	For every inner product space exists the so-called \newterm{induced norm}
	\begin{equation}
		\norm{x}_V = \sqrt{ \braket{x}{x}_V } \qquad \forall x \in V.
		\label{eqn:normInduced}
	\end{equation}
	One may drop the subscript on the norm
	if it is clear from context.
	\begin{proof}
		See~\cite{DiracNotation}.
	\end{proof}
\end{prop}

The second ingredient for a Hilbert space is a property
called \newterm{completeness}. Formally it is defined as such:
\begin{defn}
	\label{def:Completeness}
	A vector space $V$ is called \newterm{complete} if every
	\newterm{Cauchy sequence} of vectors in $V$ has a limit in $V$.
\end{defn}
Let us first recall, that a sequence $(x_n)_{n\in\N} \in V$ is called Cauchy if
\[ \forall \varepsilon > 0 \quad
	\exists M \in \N \quad \text{such that} \quad
	\norm{x_n - x_m}_V < \varepsilon \quad \forall n,m > M.
\]
One can show that every converging sequence is Cauchy.
A roughly equivalent way of phrasing definition \vref{def:Completeness}
is therefore,
that a space $V$ is complete iff every sequence $(x_n)$
of elements which come arbitrarily close at large enough $n$
tend towards an element, which is from $V$ \emph{as well}.

\begin{exmp}
	\label{exmp:QdenseR}
	To make the concept of completeness more clear,
	let us consider a counterexample.
	For this let us leave the setting of vector spaces
	and more broadly think about sequences defined on sets of numbers%
	\footnote{This is fine, since completeness is in fact a property on
		so-called metric spaces, which are related to normed vector
		spaces, but have much less structure.},
	where the concept of completeness applies as well.

	\noindent
	It is well known that the sequence
	\[ x_n = \sum_{k=0}^n \frac{1}{k!} \in \set{Q} \]
	converges to Euler's number $e$, \ie
	\[ \lim_{n\to\infty} x_n = e \not\in \set{Q}. \]
	In other words $\set{Q}$ is not complete.

	One may, however, build the \newterm{completion} of $\set{Q}$
	by just including all limiting points of all sequences in $\set{Q}$.
	In fact this is one way of defining the set of real numbers $\R$.
\end{exmp}

\begin{rem}
	A subtle point about completeness is that it depends
	on the norm which is used to determine whether a sequence
	is Cauchy or not.
	In other words a vector space may be complete with respect to one norm,
	but not with respect to another.
	Similarly the completion of a space with respect to two different
	norms may yield different spaces.

	In practice the choice of the norm is only of importance for
	infinite-dimensional vector spaces,
	since for finite-dimensional real or complex vector spaces
	all norms are equivalent\footnote{That is they induce the same topology.}
	anyway.
\end{rem}

\noindent
Finally we can state
\begin{defn}
	A \newterm{Hilbert space} $\hilbert$ is an inner product space,
	which is complete with respect to the induced norm.
\end{defn}
In other words a Hilbert space is a space,
where the inner product naturally defines a way to measure distances
and take limits, that is perform calculus.
Thinking ahead towards the integral and differential operators
we will define on such Hilbert spaces,
this is exactly what we will need.

Before we look into some Hilbert spaces relevant for \QM,
let us first clarify the concept of \newterm{denseness} and \newterm{separability}.
\begin{defn}
	A subspace $S$ of a vector space $V$ is called \newterm{dense in $V$}
	if each vector $x \in V$ either is a member of $S$ or one may find
	a Cauchy sequence in $S$ for which $x$ is the limit point.
\end{defn}
In other words $S$ is dense in $V$ if we can --- for each element of $V$
--- construct a sequence of approximations inside the smaller space $S$,
representing the desired element up to arbitrary accuracy.
Denseness is therefore one of the fundamental properties required for approximation.

\begin{exmp}
	Returning to example \vref{exmp:QdenseR} we note,
	that $\set{Q}$ is dense in $\R$.
	This guarantees that we may approximate any real number
	up to arbitrary accuracy by an appropriate sum of fractions,
	which is one of the assumptions behind any floating point operation
	performed on the computer.
\end{exmp}

\begin{defn}
	A Hilbert space is \newterm{separable}%
	\footnote{In the broader context of metric spaces,
	a separable space has a countable, dense subset.}
	iff it admits a countable orthonormal basis.
\end{defn}

\todoil{Go into the subtle differences between
	a Schauder basis (infinite liner combinations,
	finite set) and a Hamel basis (finite linear combinations)
	and a Hilbert basis (both infinite).
	See wikipedia on Schauder_basis, Orthonormal_basis and Separable_space.
}

\begin{rem}
	\label{rem:HilbertCd}
	If a Hilbert space is separable we can find a basis set%
	\footnote{This remark sketches the construction of a so-called
		\textit{Schauder basis},
		which is related, but not identical to the concept
		of a \textit{Hamel basis},
		which is usually employed in finite-dimensional linear algebra.
	}
	$\{ \varphi_\mu \}_{\mu \in \Ibas}$
	of at most countably infinite cardinality,
	\ie where $\Ibas \subseteq \N$.
	With this we can write for each $\Psi \in \hilbert$:
	\begin{equation}
		\Psi = \sum_{\mu \in \Ibas} c_\mu \varphi_\mu.
		\label{eqn:HilbertBasisExpansion}
	\end{equation}
	This in turn uniquely identifies each $\Psi$ with a sequence
	$(c_\mu)_{\mu\in\N}$ of complex numbers.
	By this means each complex, separable Hilbert space is
	isomorphic to the space of complex-valued, square-summable sequences $l^2(\N, \C)$.
	One can easily show that this isomorphism is even an isometry, \ie
	\[ \norm{\Psi}_\hilbert = \norm{(c_\mu)}_{l^2}
		= \sqrt{ \sum_{\mu=0}^\infty \abs{c_\mu}^2 }. \]
	By transitivity all separable Hilbert spaces are isometrically isomorphic.

	In our remaining discussion we will only encounter complex,
	separable Hilbert spaces. This implies:
	\begin{itemize}
		\item If $\Psi \in \hilbert$ is a vector in a Hilbert space,
			we can always identify it with a
			(possibly infinite) column vector of complex coefficients.
		\item Finite-dimensional Hilbert spaces are isomorphic to $\C^d$,
			where $d$ is the dimensionality.
			Their vectors are thus identified by a column of complex numbers
			of finite size.
	\end{itemize}
\end{rem}

\begin{rem}
	\label{rem:HilbertApproximation}
A consequence of remark \ref{rem:HilbertCd} is that we can
numerically approximate all separable Hilbert spaces rather naturally.
For example by restricting the sum in \eqref{eqn:HilbertBasisExpansion}
to only a finite number of $d$ basis functions,
we can make sure that the resulting $\Psi$ is located
in only a $d$-dimensional subspace $\hilbert^{(d)} \subset \hilbert$.
Moreover this subspace is dense, since in the limit of taking
all basis functions, we get exactly $\hilbert$.
In turn since $\hilbert^{(d)}$ is finite-dimensional,
we can identify each approximation to $\Psi$ with a vector in $\C^d$,
which can be represented numerically on the computer,
regardless of the structure of $\hilbert$.
\end{rem}

\subsection{The Hilbert spaces of quantum mechanics}
\nomenclature{$L^2(\R^d, \C)$}{
The Hilbert space of square-integrable complex-valued functions.
}

Now that we have the required basic concepts at hand,
let us discuss the question which Hilbert space to take for quantum mechanics.
In section \ref{sec:IntroQM} we said that the state functions
$\Psi : \R^d \to \C$ are taken from a complex, separable Hilbert space.
In our treatment we adhere to the
\newterm{Copenhagen interpretation} or \newterm{Born interpretation}
of the quantum-mechanical state $\Psi$,
which associates the meaning of a probability density
with the square
of the state function $\abs{\Psi(x_1, x_2, \ldots x_d)}^2$
at each point in space $(x_1, \ldots x_d)$.
A more detailed analysis in light of this probabilistic meaning of $\Psi$
suggests to take these functions from the Hilbert space of square-integrable functions
$L^2(\R^d, \C)$,
which we will define now.

\begin{defn}
	\label{def:L2Norm}
	Given two suitable%
	\footnote{%
		In order for the inner product \eqref{eqn:defL2InnProd}
		to be well-defined, the integrand needs to be Lebesgue-measurable,
		\ie $f$ and $g$ need to be chosen such that the integral
		over $f^\ast g$ can be performed in the Lebesgue sense.%
	}
	functions $f, g : \R^d \to \C$,
	we can define an inner product
	\begin{equation}
		\braket{f}{g}_{L^2} := \int_{\R^d} \cc{f}(\vec{x}) g(\vec{x}) \D \vec{x}
		\label{eqn:defL2InnProd}
	\end{equation}
	and the corresponding induced norm function
	\begin{equation}
		\abs{f}_{L^2} := \sqrt{\braket{f}{f}_{L^2}} = \left( \int_{\R^d} \abs{f(\vec{x})}^2 \D \vec{x} \right)^{1/2},
		\label{eqn:defL2Norm}
	\end{equation}
	where the integral --- in both cases --- is to be understood in the Lebesgue sense
	and we identified
	\[ \vec{x} \equiv (x_1, \ldots, x_d) \]
	for ease of notation.
\end{defn}
Notice, that \eqref{eqn:defL2Norm} is not yet a norm,
but almost\footnote{It is a so-called \textit{semi-norm}.}.
What is missing is the norm-separates-points-condition \eqref{eqn:normPointSep}.
The reason has to do with the subtleties
of the Lebesgue integrals are defined.
In fact the result of a Lebesgue integral
\[ \int_{R^d} f \D x \]
is unchanged if we replace the integrand $f$ by a function $f'$,
which is identical \emph{almost everywhere}.
This is meant to say,
that changing $f$ at infinitely many, well-separated places in order yield $f'$
does not change the outcome of the integration.
More mathematically one says that the Lebesgue integral is only uniquely defined
up to \newterm{sets of measure zero}.
Coming back to \eqref{eqn:normPointSep},
the problem is hence that there is more than one function satisfying
$\abs{f}_{L^2} = 0$,
in fact infinitely many.
To circumvent this problem one performs a trick,
namely one puts all functions which are equivalent almost everywhere
in one class and henceforth only thinks of them as one entity.
In the language of mathematics,
we form the quotient group under the equivalence relation of functions being identical
almost everywhere.
Under this procedure $\abs{\slot}_{L^2}$ becomes a full norm,
denoted as $\norm{\slot}_{L^2}$ and we can define:
\begin{prop}
	\label{prop:L2HilbertSpace}
	The set
	\[ L^2(\R^d, \C) := \big\{ f : \R^d \to \C ~\big|~ \norm{f}_{L^2} < \infty \big\}, \]
	where the norm $\norm{\slot}_{L^2}$ stays finite
	is the set of \newterm{square-integrable functions}.
	It forms a Hilbert space over the field $\C$.
	\begin{proof}
		See~\cite{Adams2003}
	\end{proof}
\end{prop}
\begin{prop}
	$L^2(\R^d, \C)$ is separable.
	\begin{proof}
		See~\cite{Werner2011}
	\end{proof}
\end{prop}
In other words $L^2(\R^d, \C)$ truly satisfies all the requirements
for being a suitable Hilbert space of \QM
as introduced in section \vref{sec:IntroQM}.

\begin{rem}
% TODO OPTIONAL
%	\to doil{Here spinor and spin are introduced rather ad hoc,
%	a reader without knowledge in any quantum or relativistic
%	physics will find this hard}
We did not introduce the most general theory of \QM
in this section, but only  \emph{non-relativistic, spin-free} \QM.
In a fully relativistic \QM treatment
each state is not a function returning
a single complex value, but rather a function returning a \newterm{spinor},
which for relativistic \QM has $4$ spin components per particle.
In other words for an $N$-particle system
the corresponding space would be $L^2(\R^{3N}, \C^{4N})$.
This work does not treat relativistic \QM at all,
much rather only deal with \textit{non-relativistic, spin-adapted} \QM,
which only has $2$ spin components per particle,
thus states from $L^2(\R^{3N}, \C^{2N})$.

For simplifying the mathematical treatment we will nevertheless
assume $\hilbert = L^2(\R^d, \C)$ for most of our analysis
in the next few chapters and only introduce spin \textit{ad hoc} in the form
of the space $L^2(\R^{3N}, \C^{2N})$ once this is needed.
We can do this without any loss of generality
because of remark \vref{rem:HilbertCd},
where we pointed out that all infinite-dimensional
Hilbert spaces are isometrically isomorphic.
This implies that all of the properties we have showed or will show based
on the Hilbert space $L^2(\R^d, \C)$ can be generalised
to $L^2(\R^d, \C^s)$ with $s \geq 1$ with ease.
\end{rem}

\subsection{Sobolev spaces}
\label{sec:Sobolev}
Many operators of quantum mechanics including all the operators,
which we will discuss in detail, involve taking derivatives of states.
Whilst Lebesgue spaces are suitable for doing statistics,
their mathematical structure does not make sure
that derivatives of functions from $L^2(\R^d, \C)$ stay in $L^2(\R^d, \C)$.
For example $1/r$ is square-integrable on $\R^3$,
whilst its radial derivative $-1/r^2$ is not.
One way to tackle this, is to take an appropriate subspace of $L^2(\R^d, \C)$,
which allows taking a certain number of derivatives.
As it turns out the appropriate treatment for the numerical solution
of partial differential equations,
does not require the usual or \newterm{strong derivatives},
weak derivatives are sufficient.
These are defined as such:

\begin{defn}
	A function $f \in L^2(\R^d, \C)$ has a \newterm{weak partial derivative}
	$g \in L^2(\R^d, \C)$ with respect to $x_i$ if
	\[ \forall \varphi \in C^\infty_0(\R^d, \C): \quad \braket{g}{\varphi}_{L^2} = - \braket{f}{\frac{\partial}{\partial x_i}\varphi}_{L^2}, \]
	where $C^\infty_0(\R^d, \C)$ is the space of all
	infinitely differentiable complex-valued functions with compact support.
	To denote the weak derivative
	one may write $g = \frac{\partial}{\partial x_i} f$ like in the strong case.
	It can further be shown that if $f$ has a strong derivative
	then it also has a weak derivative, which coincides with the strong derivative.
	For ease of notation we also write
	\[ D^{\vec{\alpha}} f = \frac{\partial^{\norm{\vec{\alpha}}_1}}{ \prod_{i=1}^d \partial x_i^{\alpha_i} }, \]
	where $\vec{\alpha} \in \N^d$ and as usual for the $l_1$-norm
	\[ \norm{\vec{\alpha}}_1 = \sum_{i=1}^d \abs{\alpha_i}. \]
\end{defn}

\noindent
With the weak derivative at hand we can define the so-called Sobolev spaces,
which allow to make certain guarantees about the number of (weak)
derivatives, which can be taken.
A full family of such spaces exist. We will only present two kinds here.

\nomenclature{$H^1(\R^d, \C)$}{The Sobolev space of complex-valued functions with square-integrable first derivative.}
\nomenclature{$H^2(\R^d, \C)$}{The Sobolev space of complex-valued functions with square-integrable second derivative.}
\begin{defn}
	The \newterm{Sobolev space} defined by
	\begin{equation}
		H^k(\R^d, \C) := \left\{ f \in L^2(\R^d, \C) ~\middle|~ D^{\vec{\alpha}} f \in L^2(\R^d, \C) \text{ for $\norm{\vec{\alpha}}_1 \le k$} \right\}
		\label{eqn:defSobolev}
	\end{equation}
	with inner product
	\begin{equation}
		\braket{f}{g}_{H^k} := \sum_{\norm{\vec{\alpha}}_1 \le k} \braket{D^{\vec{\alpha}} f\,}{\,D^{\vec{\alpha}} g}_{L^2}
		\label{eqn:defSobolevInnProd}
	\end{equation}
	and induced norm
	\begin{equation}
		\norm{f}_{H^k} = \sum_{\norm{\vec{\alpha}}_1 \le k} \norm{D^{\vec{\alpha}} f}_{L^2}
		\label{eqn:defSobolevNorm}
	\end{equation}
	is a Hilbert space~\cite{Adams2003}.
\end{defn}

\begin{defn}
	The completion of $C^\infty_0(\R^d, \C)$
	with respect to the norm $\norm{\slot}_{H^k}$
	is the Sobolev space $H^k_0(\R^d, \C)$.
	It is a proper subspace of $H^k$ and a Hilbert space as well~\cite{Adams2003}.
\end{defn}
Colloquially speaking if a function is a member of $H^k(\R^d, \C)$ or $H^k_0(\R^d, \C)$,
we can assume that the $k$-th derivative of this function remains square-integrable.
These spaces will become rather important in the next section \vref{sec:spectral},
where we will need them to define self-adjoint operators upon them.
As a summary the relationships between the spaces we discussed
in this section have been summarised in figure \vref{fig:sobolevRelations}.
Note, that by definition
\[ L^2(\R^d, \C) = H^0(\R^d, \C) = H^0_0(\R^d, \C). \]

\begin{figure}
	\centering
	\includeimage{1_qm/sobolev}
	\caption
		[Relationships between the function spaces discussed in this section]
		{Overview of the spaces discussed in this section.
		Apart from $C^\infty_0(\R^d, \C)$ all mentioned spaces are Hilbert spaces.
		In each case $A \subset B$ denotes that $A$ is a proper, dense
		subspace of $B$.}
	\label{fig:sobolevRelations}
\end{figure}

To finish our discussion of Sobolev spaces let us determine
in which Sobolev space the function
\begin{equation}
	\label{eqn:H1sFunctionalForm}
	\Psi_{1s}(\vec{r}) = \exp\left(- \sqrt{x^2 + y^2 + z^2} \right) = \exp(-r)
\end{equation}
is located.
This function and trivial generalisations thereof
will be of relevance for our future treatment,
since it arises naturally as an eigenfunction of the hydrogen-like
Hamiltonian \eqref{eqn:OpHydrogen}~(see section \vref{sec:HydrogenAtom})
and is furthermore an important building block of the Coulomb-Sturmians%
~(see section \vref{sec:BasisCS}).

\begin{exmp}
	\label{exmp:H1sH1}
	The function $\Psi_{1s}$ of \eqref{eqn:H1sFunctionalForm} belongs to $H^1(\R^3, \C)$.
	\begin{proof}
	Since the function is Riemann-integrable over $\R^3$,
	it is Lebesgue-integrable as well
	and as a result $\Psi_{1s} \in L^2(\R^3, \C)$.
	Furthermore for any $\alpha \in \{x, y, z\}$:
	\begin{equation}
		\norm{ \frac{\partial \Psi_{1s}}{\partial \alpha} }_{L^2}
		= \norm{ - \frac{\alpha}{r} \exp(-r) }_{L^2}
		= \int_{\R^3} \frac{\alpha^2}{r^2} \exp(-2r) \D\vec{r}
		\leq \int_{\R^3} \frac{r^2}{r^2} \exp(-2r) \D\vec{r}
		\label{eqn:ProofH1sH1}
	\end{equation}
	Due to the properties of the Lebesgue integral,
	we may ignore the removable discontinuity at $\vec{r} = \vec{0}$
	and instead write
	\[ \norm{ \frac{\partial \Psi_{1s}}{\partial \alpha} }_{L^2}
		\leq \int_{\R^3} \exp(-2r) \D\vec{r} = \norm{\Psi_{1s}}_{L^2} < \infty. \]
	This shows that $\exp(-r) \in H^1(\R^3, \C)$,
	since each term of \eqref{eqn:defSobolevNorm} is bound.
	\end{proof}
\end{exmp}

For the next step, showing $\Psi_{1s} \in H^2(\R^3, \C)$,
we need two results relating $H^1(\R^3, \C)$ and $L^2(\R^3, \C)$.
\begin{prop}[Hardy's inequality]
	\label{prop:Hardy}
	For all $u \in H^1(\R^3, \C)$, we have
	\[ \int_{\R^3} \norm{\nabla u}_2^2 \D\vec{r}
		\geq \frac{1}{4} \int_{\R^3} \frac{\abs{u}^2}{r^2} \D\vec{r} \]
	\begin{proof}
		For a proof of the special case $u \in C_0^\infty(\R^3, \C)$
		see \cite[p. 30]{Helffer2013}.
		The more general case we claim here,
		follows from the denseness of $C_0^\infty(\R^3, \C)$ in $H^1(\R^3, \C)$
		and continuity of the integrands on both sides
		with respect to the $H^1$ norm.
	\end{proof}
\end{prop}

\begin{cor}
	\label{cor:Hardy}
	If $u \in H^1(\R^3, \C)$, then $\frac{u}{r} \in L^2(\R^3, \C)$.
	\begin{proof}
		One easily rewrites Hardy's inequality to
		\[
			\norm{u}_{H^1} \geq  \sum_{\alpha\in\{x,y,z\}} \int_{\R^3} \abs{\frac{\partial u}{\partial \alpha}} \D\vec{r}
			\stackrel{\text{(triangle)}}{\geq}
			\int_{\R^3} \norm{\nabla u}_2^2 \D\vec{r}
			\stackrel{\text{(Hardy)}}{\geq}
			\frac{1}{4} \int_{\R^3} \frac{\abs{u}^2}{r^2} \D\vec{r}
			= \frac{1}{4} \norm{\frac{u}{r}}_{L^2}
		\]
		which proves the claim.
	\end{proof}
\end{cor}

\begin{exmp}
	\label{exmp:H1sH2}
	We now want to use corollary \vref{cor:Hardy} to prove that
	$\Psi_{1s} \in H^2(\R^3,\C)$.
	\begin{proof}
		Considering our result from \eqref{eqn:ProofH1sH1}
		we find that for all $\alpha, \beta \in \{x,y,z\}$:
		\begin{align*}
			\norm{\frac{\partial^2 \Psi_{1s}}{\partial \alpha \partial \beta}}_{L^2}
			&\leq \norm{ \frac{\delta_{\alpha\beta}}{r} \exp(-r)}_{L^2}
			+ \norm{\frac{\alpha\beta}{r^3}\exp(-r)}_{L^2}
			+ \norm{\frac{\alpha\beta}{r^2}\exp(-r)}_{L^2}
		\end{align*}
		Noting $\abs{\alpha\beta} \le r^2$
		and ignoring the removable singularities in the Lebesgue integral,
		we arrive at
		\begin{align*}
			\norm{\frac{\partial^2 \Psi_{1s}}{\partial \alpha \partial \beta}}_{L^2}
			&\leq \norm{\frac{1}{r} \exp(-r)}_{L^2}
			+ \norm{\frac{r^2}{r^3}\exp(-r)}_{L^2}
			+ \norm{\frac{r^2}{r^2}\exp(-r)}_{L^2} \\
			&= 2\norm{\frac{1}{r} \exp(-r)}_{L^2} + \norm{\exp(-r)}_{L^2} \\
			&< \infty,
		\end{align*}
		where in the last line we used
		$\exp(-r) \in H^1(\R^3, \C)$ and corollary \vref{cor:Hardy}.
\end{proof}
\end{exmp}

\begin{rem}
	Analogously to what we sketched in examples \vref{exmp:H1sH1} and \vref{exmp:H1sH2},
	one could attempt to probe whether the one-dimensional function
	$f(x) = \exp(-\abs{x})$ is in $H^1(\R,\C)$ or $H^2(\R,\C)$.
	Whilst the former can be easily verified,
	one finds $f \not\in H^2(\R,\C)$.
\end{rem}

This rather surprising result is a consequence of the second part of
the Sobolev embedding theorem
of which we only present a slightly specialised form here.
\begin{thm}[Sobolev embedding]
	Given $r, k, d \in \N$ with
	\[ k > \frac{d}{2} > 0 \quad \text{and} \quad k -\frac{d}{2} > r \]
	one may find an embedding
	\[ H^k(\R^d) \subset C^r(\R^d) \]
	between the Sobolev space $H^k(\R^d)$ and the space of the $r$ times
	continuously differentiable functions, $C^r(\R^d)$.
\end{thm}
This embedding theorem allows to get an idea what is to be expected about
the smoothness of a function in $H^k$.
Interestingly the smaller the dimensionality the smoother such a function
has to be.
% TODO OPTIONAL
% \to doil{One gets lost in the mathematics by now \ldots add some forward references}

% TODO OPTIONAL
%\subsection{Representation theorems}
%\to doil{If there is time talk about the Ritz and the Lax-Milgram}
%
%\begin{thm}[Riesz's representation theorem]
%	\label{thm:Riesz}
%	bla
%	\begin{proof}
%		See \cite{Werner2011}
%	\end{proof}
%\end{thm}
%
%\begin{lem}[Lax-Milgram]
%	\label{thm:LaxMilgram}
%	bla
%\end{lem}
