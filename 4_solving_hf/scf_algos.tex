\section{Self-consistent field algorithms}
\label{sec:SCFAlgorithms}

In this section we want to discuss a few standard self-consistent field algorithms
in the light of the various types of basis functions we discussed in the previous section.
Even though it is my hope that the selection of algorithms discussed here
is representative,
the vast number of methods,
which has been developed over the years,
makes it impossible to be exhaustive.

Most \SCF algorithms are designed only with a \cGTO-based
discretisation of the \HF and Kohn-Sham \DFT problem in mind.
The deviating numerical properties of the finite-element method
or a \CS-based discretisation,
therefore often call for minor modifications of the schemes.
For example both finite elements as well as Coulomb-Sturmians
favour \contract-based methods
due to the better scaling of equations like \eqref{eqn:ExchangeApply}
and \eqref{eqn:ApplicationKcs}
compared to building the full matrix.
Therefore the Fock matrix might not be built in memory any more,
which implies that a linear combination of Fock matrices
cannot be computed in memory either.
This does not imply that \SCF schemes,
which form linear combinations of Fock matrices
are completely ruled out,
but they might become less favourable compared to other schemes.
See section \ref{sec:DIIS} below for details.

On the other hand in \FE-based approaches all quantities,
which scale quadratically in $\Nbas$ cannot be stored in memory.
This applies not only to the iterated Fock matrix $\mat{F}^{(n)}$,
but to the density matrix $\mat{D}^{(n)} \in \R^{\Nbas\times \Nbas}$ as well.
Even though clever low-rank approximation methods
like hierarchical matrices~\cite{Hackbusch1999,Hackbusch2002,Grasedyck2003,Hackbusch2015}
or tensor decomposition methods
\todo{cite}
could reduce the memory footprint of the density matrix,
this work will try to indicate ways
by which building the density matrix in an \SCF can be avoided.
Naturally this implies
a focus on coefficient-based \SCF schemes as well,
where the number of iterated parameters
--- the coefficient matrix $\mat{C} \in \R^{\Nbas\times\Norb}$ ---
scales only linearly in $\Nbas$.
Furthermore coefficient-based \SCF schemes have the advantage,
that iterating the density matrix
destroys the possibility to perform the
linear-scaling application of $\mat{K}$ for \CS-based methods,
see equation \eqref{eqn:ApplicationKcs} for details.

\todoil{Pagebreak here?}
\pagebreak
It was already pointed out in section \vref{sec:OverviewSCF}
that focusing on coefficient-based schemes
is hardly a restriction in terms of the number of possible approaches,
since coefficient-based and density-based schemes can be interconverted,
at least approximately.
We will suggest modifications of the
optimal damping algorithm~(ODA)~\cite{Cances2000a} in section \ref{sec:tODA}
and Pulay's direct inversion of the iterative subspace~\cite{Pulay1982}
in section \ref{sec:DIIS},
which bring these methods to the coefficient-based setting.

Most of the \SCF algorithms we will consider here
only converge the \HF equations \eqref{eqn:HFiterated}
until the Pulay error \eqref{eqn:PulayError} vanishes
following our general description in remark \vref{rem:SCFcoeff}.
Regarding the \HF optimisation problem \eqref{eqn:HFOptCoeff}
this is only the necessary condition for a stationary point on the Stiefel
manifold $\mathcal{C}$.
Only some \SCF algorithms
termed \newterm{second-order self-consistent field method}s
guarantee (usually only appropriately)
that the stationary point they find is truly a minimum.
They are briefly considered in section \ref{sec:SOSCF}.

\subsection{Roothaan repeated diagonalisation}
\label{sec:RoothaanRepeatedDiag}

Roothaan's repeated diagonalisation~\cite{Roothaan1951}
approach to the \HF problem \eqref{eqn:HFiterated} is by far the simplest.
In the formalism of remark \vref{rem:SCFcoeff}
this algorithm can be described by building the next Fock matrix
$\tilde{\mat{F}}^{(n)}$ only by considering
the current occupied coefficients $\mat{C}^{(n)}$,
\ie $\tilde{\mat{F}}^{(n)} = \matFnfull$.
The two-step iteration procedure of figure \vref{fig:RoothanRepeatedDiag} results.
\begin{figure}
	\centering
	\missingfigure{Roothaan algorithm diagram}
	\caption{Roothaan algorithm diagram}
	\label{fig:RoothanRepeatedDiag}
\end{figure}

Even though Roothaan's algorithm already works for a few simple cases,
it is far from being reliable.
For example one can show~\cite{Cances2000,Cances2000b} that it
either converges to a stationary point of the
discretised \HF problem \eqref{eqn:HFOptCoeff}
or alternatively it numerically oscillates between two states,
where none of them are a stationary point of \eqref{eqn:HFOptCoeff}.
In practice which of these cases occurs is dependent on the system
as well as the basis set.
Furthermore there is no guarantee that the resulting stationary point
the Roothaan's algorithm finds is the \HF ground state.
All these cases are already found for \HF calculations
on the first three periods of the periodic table~\cite{Cances2000}.

\subsection{Level-shifting modification}
If one uses essentially the same \SCF scheme as \ref{fig:RoothanRepeatedDiag}
but instead diagonalises the matrix
\[ \tilde{\mat{F}}^{(n)} = \matFnfull
	- b \mat{S} \, \mat{C}^{(n)}\!\tp{\left(\mat{C}^{(n)}\right)} \, \mat{S} \]
where $b > 0$,
already a much better convergence is achieved.
This modification is called \newterm{level shifting}%
~\cite{Saunders1973,Guest1974},
where $b$ is the \textbf{level-shifting parameter},
typically chosen in the range between $0.1$ and $0.5$.
Effectively this modification increases the energy gap between occupied and virtual
orbital energies.
To see this, let us consider the converged case, where
\[ \mat{F} \mat{C}_F = \mat{S} \mat{C}_F \mat{E} \]
exactly and let us partition the full coefficient matrix%
\footnote{We assume \RHF here and furthermore only consider the $\alpha$ block.
	For \UHF the analysis is exactly the same with
	the relevant equations just replicated in $\alpha$ and $\beta$ block.}
\[
	\mat{C}_F = \mm{\mat{C} & \mat{C}_\text{virt}}
\]
into occupied and virtual parts.
Now let $\tilde{\mat{F}} = \mat{F} - b \mat{S} \, \mat{C}\tp{\mat{C}} \, \mat{S}$
such that
\begin{align*}
	\tilde{\mat{F}} \mat{C}_F
	&= \left( \mat{F} - b \mat{S} \, \mat{C}\tp{\mat{C}} \, \mat{S} \right) \mat{C}_F \\
	&= \mat{S} \mat{C}_F \mat{E} - b \mat{S} \mm{
		\mat{C}\tp{\mat{C}} \mat{S} \mat{C} &
		\mat{C}\tp{\mat{C}} \mat{S} \mat{C}_\text{virt}
	} \\
	&= \mat{S} \mat{C}_F \mat{E} - b \mat{S} \mm{ \mat{C} & 0 } \\
	&= \mat{S} \mat{C}_F \mat{E} + \mat{S} \mat{C}_F \mm{ -b  & 0 } \\
	&= \mat{S} \mat{C}_F \tilde{\mat{E}}
\end{align*}
where
\[ \tilde{\mat{E}} = \diag\left(\varepsilon_1 - b, \varepsilon_2 - b, \ldots,
	\varepsilon_{\Nelec} - b, \varepsilon_{\Nelec+1}, \ldots, \varepsilon_{\Norb}\right).
\]
In other words the virtual orbitals are unaffected
whereas the occupied orbitals are shifted downwards in energy by an amount $b$.

The effect of this is that coupling between both orbital spaces is reduced,
which tends to lead to faster convergence
especially if the gap between $\varepsilon_{\Nelec}$ and $\varepsilon_{\Nelec+1}$ is small.
This empirical observation is backed up by more sophisticated
mathematical analysis by \citet{Cances2000b}.
Their result shows that for large enough $b$,
the level-shifted Roothaan procedure is guaranteed to converge to a stationary
point of the \HF problem \eqref{eqn:HFOptCoeff}.
The also provide an expression for the lower bound of $b$.
In this manner convergence to a stationary point
can be forced even for cases where the original \HF equations \eqref{eqn:HFMO}
have no solution (like the negative ions with $N > 2Z + M$).
In such a case the result is no physical ground state, however.

One can show~\cite{Saunders1973}
that the level-shifting modification is mathematically equivalent
to another modification of Roothaan's repeated diagonalisation,
called \newterm{damping}.
In this procedure one chooses a \newterm{damping factor} $0 < \alpha < 1$
and sets
\begin{equation}
	\tilde{\mat{F}}^{(n)} = (1-\alpha) \tilde{\mat{F}}^{(n-1)} + \alpha \matFnfull,
	\label{eqn:FockDamping}
\end{equation}
such that the new Fock matrix to diagonalise contains still a share
of the old Fock matrix.

\defineabbr{ODA}{ODA\xspace}{Optimal damping algorithm}
\subsection{Optimal damping algorithm}
\label{sec:ODA}
The optimal damping algorithm~(\ODA) was proposed by \citet{Cances2000a}
based on their analysis of the Roothaan algorithm
including the level-shifting modification.

\begin{figure}
	\centering
	\missingfigure{Optimal damping algorithm}
	\caption{Optimal damping algorithm}
	\label{fig:OptimalDampingAlgorithm}
\end{figure}
In unmodified form~\cite{Cances2000,Cances2000a} it is a density-based \SCF algorithm.
Starting from an initial density $\tilde{\mat{D}}^{(0)} = \mat{D}^{(0)}$,
the procedure is roughly~(compare figure \ref{fig:OptimalDampingAlgorithm})
for $n=1, 2, 3, \ldots$
\begin{itemize}
	\item Build the Fock matrix
	\begin{equation}
	\tilde{\mat{F}}^{(n-1)} = \mat{F}\left[ \tilde{\mat{D}}^{(n)} \right]
		\label{eqn:ODAupFock}
	\end{equation}
	and diagonalise it to obtain the new coefficient $\mat{C}_F^{(n)}$.
	Form the new density $\mat{D}^{(n)}$ according to the Aufbau principle from these
	as
	\[ \mat{D}^{(n)} = \mat{C}^{(n)} \tp{\left( \mat{C}^{(n)} \right)}. \]
	%
	\item Evaluate the Pulay error $\mat{e}^{(n)}$ \eqref{eqn:PulayError}
		and end the process if $\norm{\mat{e}^{(n)}}_\text{frob} \leq \epsilonconv$.
	%
	\item Set
		\begin{equation}
			\tilde{\mat{D}}^{(n+1)}
			= \arginf_{\tilde{\mat{D}} \in
			\text{Seg}\left[\tilde{\mat{D}}^{(n)}, \mat{D}^{(n+1)}\right]}
			\mathcal{E}_D^\text{HF}[\tilde{\mat{D}}]
			\label{eqn:ODAupDens}
		\end{equation}
		where
		\[
			\text{Seg}\left[\mat{D}_1, \mat{D}_2\right]
				= \left\{ (1-\lambda) \mat{D}_1 + \lambda \mat{D}_2 \, \Big|\,
				\lambda \in [0,1] \right\}
		\]
		is a line segment of density matrices
		and the energy functional $\mathcal{E}_D^\text{HF}$
		is defined as in \eqref{eqn:HFEnergyFunctionalDens}
		and repeat the process.
\end{itemize}
The remaining question to complete the picture of the \ODA
is how to obtain minimal density $\tilde{\mat{D}}^{(n+1)}$ along the density segment.
First notice that in general the density matrix segment
\[\text{Seg}\left[\mat{D}_1, \mat{D}_2\right] \not\subset \mathcal{P}\]
even if $\mat{D}_1, \mat{D}_2 \in \mathcal{P}$.
Much rather this line segment is fully contained only in a superset
$\tilde{\mathcal{P}} \supset \mathcal{P}$,
where we relax the constraint $\mat{D}^2 = \mat{D}$ to%
\footnote{Let $\mat{A}, \mat{B} \in \R^{n\times n}$,
then $\mat{A} \leq \mat{B}
\Leftrightarrow \forall \vec{x} \in \R^n \
\tp{\vec{x}}\mat{A} \vec{x} \leq \tp{\vec{x}}\mat{B}\vec{x}$}
$\mat{D}^2 \leq \mat{D}$.
See \cite{Cances2000} for details.
For ease of notation let us define
%
\newcommand{\Gfct}[1]{  \mat{G}\left[ #1 \right]   }
\newcommand{\Pet}[1]{\textcolor{dkred}{\tilde{\mat{D}}^{(#1)}}}
\newcommand{\Pe}[1]{\textcolor{dkblue}{\mat{D}^{(#1)}}}
\newcommand{\Fot}[1]{\textcolor{dkred}{\tilde{\mat{F}}^{(#1)}}}
\newcommand{\Fo}[1]{\textcolor{dkblue}{\mat{F}^{(#1)}}}
\newcommand{\la}[1]{\lambda^{(#1)}}
\newcommand{\ila}[1]{\left( 1 - \la{#1} \right)}
\newcommand{\EHFD}[1]{\mathcal{E}_D^\text{HF}\left[#1\right]}
%
\newcommand{\Pn}{\Pet{n}}
\newcommand{\Pnn}{\Pe{n+1}}
\begin{align*}
	E_1 \left[\mat{D}\right] &\equiv \tr\left( \mat{T} \mat{D} + \mat{V}_0 \mat{D} \right), \\
	\mat{G}\left[ \mat{D} \right] &\equiv
		\mat{F}\left[ \mat{D} \right] + \mat{K}\left[ \mat{D} \right] \\
\intertext{and}
	E_2 \left[\mat{D}\right] &\equiv
		\frac12 \tr\left( \mat{D} \, \mat{G}\left[ \mat{D} \right] \right).
\end{align*}
For all matrices $\mat{D}_1, \mat{D}_2 \in \tilde{\mathcal{P}}$
we can show the properties~\cite{Cances2000a}
\begin{align}
	\label{eqn:TermGsymmetry}
	\tr \left( \mat{D}_1 \mat{G}\left[ \mat{D}_2 \right] \right)
		&= \tr \left( \mat{D}_2 \mat{G}\left[ \mat{D}_1 \right] \right) \\
	\label{eqn:GmatTrace}
	\tr\left( \mat{F}\left[ \mat{D}_1 \right] \mat{D}_2 \right)
	&= E_1[\mat{D}_2] + \tr\left( \mat{D}_1 \Gfct{\mat{D}_2} \right)
\end{align}
These imply for $E_2$ and arbitrary $\alpha, \beta \in \R$
\begin{equation}
	\begin{aligned}
	E_2[\alpha \mat{D}_1 + \beta\mat{D}_2]
	&= \frac 1 2 \tr\left( \alpha^2  \mat{D}_1 \Gfct{\mat{D_1}} \right)
		+ \frac 1 2 \tr\left( \alpha\beta \mat{D_1} \Gfct{\mat{D}_2} \right) \\
		\nonumber
		&\hspace{30pt}
		+ \frac 1 2 \tr\left( \alpha\beta \mat{D}_2 \Gfct{\mat{D_1}} \right)
		+ \frac 1 2 \tr\left( \beta^2 \mat{D}_2 \Gfct{\mat{D}_2} \right) \\
	 &\stackrel{\eqref{eqn:GmatTrace}}{=}
	 \alpha^2 E_2[\mat{D_1}] + \beta^2 E_2[\mat{D}_2]
	 + \alpha\beta \tr\left( \mat{D_1} \Gfct{\mat{D}_2} \right),
	\end{aligned}
	\label{eqn:HFE2}
\end{equation}
whereas $E_1$ is linear
\begin{equation}
	E_1[\alpha \mat{D}_1 +\beta \mat{D}_2] = \alpha E_1[\mat{D}_1] + \beta E_1[\mat{D}_2].
	\label{eqn:HFE1}
\end{equation}
These results allow to expand
the \HF energy for a member $\Pet{n+1}$
of the density matrix segment
$\text{Seg}\left[\Pet{n}, \Pe{n+1}\right]$
as
\begin{equation}
\begin{aligned}
	\EHFD{\Pet{n+1}} &= \EHFD{\ila{n+1} \Pet{n} + \la{n+1} \Pe{n+1}} \\
	&= \EHFD{\Pet{n} + \la{n+1} \left( \Pe{n+1}  - \Pet{n} \right)} \\
	&= E_1\left[ \Pet{n} + \la{n+1} \left( \Pe{n+1}  - \Pet{n} \right) \right] \\
		&\hspace{50pt}+E_2\left[\Pet{n} + \la{n+1} \left( \Pe{n+1}  - \Pet{n} \right) \right] \\
	&= E_1[\Pet{n}] + \la{n+1} E_1\left[\Pe{n+1}  - \Pet{n}\right] + E_2[\Pet{n}]\\
		&\hspace{50pt} + \la{n+1} \tr \left( \Pet{n} \Gfct{\Pe{n+1}-\Pet{n}} \right) \\
		&\hspace{50pt}
		+ \left( \la{n+1} \right)^2 E_2[\Pe{n}-\Pet{n}] \\
	&= \EHFD{\Pet{n}}
	+ \la{n+1} \underbrace{\tr\left( \Pet{n} \mat{F} \left[ \Pe{n+1}-\Pet{n} \right] \right)}_{=s} \\
	&\hspace{50pt}
	+ \left( \la{n+1} \right)^2 \underbrace{E_2[\Pe{n}-\Pet{n}]}_{=c} \\
	&= \EHFD{ \Pet{n} } + \la{n+1} s + \left( \la{n+1} \right)^2 c
\end{aligned}
\label{eqn:ODAquadratic}
\end{equation}
The coefficients $s$ and $c$ can alternatively be written as
\begin{equation}
\begin{aligned}
	s &= \tr\left( \Fot{n} \left( \Pe{n+1} - \Pet{n} \right) \right) \\
	&= \tr \left( \Fot{n} \Pe{n+1} \right) - E_\text{HF}[\Pet{n}] - E_2 [\Pet{n}] \\
	&= \tr \left( \Fot{n} \Pe{n+1} \right) - E_1[\Pet{n}] - 2 E_2 [\Pet{n}]  \\
\end{aligned}
\label{eqn:ODAs}
\end{equation}
and\footnote{Note that the original paper \cite{Cances2000a} uses a deviating formalism %
which causes an extra factor of $2$ to appear in their expression for $c$.}
\begin{equation}
\begin{aligned}
	c &= E_2\left[ \Pe{n+1} - \Pet{n} \right] \\
	&\stackrel{\eqref{eqn:HFE2}}{=}
		E_2 [\Pe{n+1}] - \tr\left( \Gfct{\Pet{n}} \Pe{n+1} \right) + E_2 [\Pet{n}] \\
	&= E_2 [\Pe{n+1}] - \tr \left( \Fot{n} \Pe{n+1} \right) + E_1[\Pe{n+1}] + E_2 [\Pet{n}]
\end{aligned}
\label{eqn:ODAc}
\end{equation}
Now the stationary point along the density matrix segment
can be determined by differentiating \eqref{eqn:ODAquadratic} resulting in
\begin{align*}
	\frac{\partial \EHFD{\Pet{n+1}}}{\partial \la{n+1}}
	&= s + 2 \la{n+1} c
	&&\text{and}&
	\frac{\partial^2 \EHFD{\Pet{n+1}}}{\partial \left(\la{n+1}\right)^2} &= 2c
\end{align*}
Due to $E_2[\mat{D}] \geq 0$~\cite{Cances2000a} for all $\mat{D} \in \tilde{\mathcal{P}}$
one easily deduces $c \geq 0$,
such that the stationary point of the above expression is always a minimum.
Since $\la{n+1} \in [0,1]$ the minimiser is
\begin{equation}
	\label{eqn:ODALambdaOpt}
	\la{n+1}_\text{min} = \left\{
	\begin{array}{cl}
		1 & \text{if $2c \leq -s$} \\
		- \frac{s}{2c} & \text{else}
	\end{array}
	\right.,
\end{equation}
where the cases $c=0$ and $s=0$ have been ignored, since they only occur at convergence.
This closes the missing link and allows to implement a \ODA
in as a density-based \SCF.

Let $\alpha, \beta \in \R$ and $\mat{D}_1, \mat{D}_2 \in \tilde{\mathcal{P}}$.
Since $\matFfullD = \mat{T} + \mat{V}_0 + \matJfullD + \matKfullD$
and the two-electron terms are linear in the density matrix, we have
\begin{equation}
	\mat{F}\!\left[\alpha \mat{D}_1 + \beta\mat{D}_2\right]
	= \alpha \mat{F}\!\left[\mat{D}_1\right] + \beta \mat{F}\!\left[\mat{D}_2\right]
	\label{eqn:FockLinearCombination}
\end{equation}
iff $\alpha + \beta = 1$. Defining
\begin{align*}
	\Fot{n} &\equiv  \mat{F}\left[ \Pet{n} \right] &
	\Fo{n} &\equiv \mat{F}\left[ \Pe{n} \right]
\end{align*}
this allows to rewrite \eqref{eqn:ODAupFock} as
\begin{equation}
	\label{eqn:ODAdamping}
	\Fot{n}
	= \mat{F}\left[ \ila{n} \Pet{n-1} + \la{n} \Pe{n} \right]
	= \ila{n} \Fot{n-1} + \la{n} \Fo{n},
\end{equation}
where the ``$\text{min}$'' subscripts were dropped.
Comparing with equation \eqref{eqn:FockDamping}
one can identify with $\la{n}$ the damping factor $\alpha$.
Since $\la{n}$ is optimal in the sense of minimising the energy along the line segment
spanned by $\Pe{n}$ and $\Pet{n-1}$,
the optimal damping algorithm can be described by repetitively finding the optimal damping
parameter from \SCF step to \SCF step.
Notice that its construction guarantees that the \SCF energy will always decrease.
It is hence guaranteed to converge to a local minimum of the \HF problem
\eqref{eqn:HFOptDens}~\cite{Cances2000,Cances2000a}.
The \ODA is only a particularly simple example from a whole family
of density-based \SCF algorithms called relaxed constraints algorithms,
which are discussed in detail in \cite{Cances2000a}.

\noindent
Using \eqref{eqn:ODAdamping} one can show by induction that
\begin{align}
	\label{eqn:ODAFock}
	\Fot{n} &= \sum_{j = 0}^n \Fo{j} \la{j} \prod_{i=j+1}^n \ila{i}, \\
	\label{eqn:ODADens}
	\Pet{n} &= \sum_{j = 0}^n \Pe{j} \la{j} \prod_{i=j+1}^n \ila{i},
\end{align}
where we set $\la{0} \equiv 1$. Since
\begin{align*}
	\Fo{j} &= \matFnfull \\
	\Pe{j} &= \mat{C}^{(j)} \tp{\left(\mat{C}^{(j)}\right)} \\
\end{align*}
these results in theory allow to express the complete \ODA
in terms of the coefficients
such that expressions like \eqref{eqn:ExchangeApply}
or \eqref{eqn:ApplicationKcs} could be used for a \FE-based
or a \CS-based discretisation respectively.

In practice this is usually not a fruitful approach for two reasons.
Firstly it requires to store a growing list of coefficients,
namely one for each \SCF step.
Especially for a \FE approach this becomes increasingly costly in terms of memory.
Secondly for a \contract-based ansatz
we especially want to avoid storing the Fock matrices $\Fo{j}$
in favour of \contract expression like \eqref{eqn:ExchangeApply}
and \eqref{eqn:ApplicationKcs}.
In other words each application of $\Fot{n}$ to a vector $\vec{x}$
would need to be performed by first computing $\Fo{j} \vec{x}$ for each $j$
and then adding the results.
This procedure is roughly $n$ times as expensive as a single apply.
Even though the \contract expressions formally scale better,
the increasing number of times they need to be invoked
should make this ansatz rather expensive.

Overall the \ODA is very suitable for {\cGTO} and \CS-based discretisations,
since for these density-based \SCF schemes are fine,
but this algorithm is not suitable for solving the \HF problem
with a \FE-based discretisation without further modifications.

\defineabbr{tODA}{tODA\xspace}{Truncated optimal damping algorithm}
\subsection{Truncated optimal damping algorithm}
\label{sec:tODA}

Let us again consider \eqref{eqn:ODAFock}.
Due to $\la{i} \in [0,1]$ the Fock matrix prefactor
\begin{equation}
	\label{eqn:ODAFockCoeff}
	\lambda^{(j)} \prod_{i=j+1}^{n} \left(1- \lambda^{(i)}\right) \in [0,1]
\end{equation}
is a product of factors,
which are all between $0$ and $1$.
Therefore this prefactor may become
rather small for small values of $j$ as $n$ increases.
In other words in the later \SCF steps the $\Fo{j}$ terms
which were produced at the beginning of the \SCF procedure
may be accompanied with a small prefactor and hence can at some point
be neglected in \eqref{eqn:ODAFock}.
This is the justification for the truncated optimal damping algorithm~(\tODA),
which approximates the \ODA by artificially restricting the number
of terms in \eqref{eqn:ODAFock} to the $m$ most recently
obtained Fock matrices.
If we define
\[
	j_0(n) \equiv n-m+1
\]
this allows to write the approximated sums as
\begin{align}
	\label{eqn:ODAFockApprox}
	\Fot{n} &= \frac{1}{\la{j_0(n)}} \sum_{j = j_0(n)}^n \Fo{j} \la{j} \prod_{i=j+1}^n \ila{i}, \\
\intertext{and analogously for the density matrices}
	\label{eqn:ODADensApprox}
	\Pet{n} &= \frac{1}{\la{j_0(n)}} \sum_{j = j_0(n)}^n \Pe{j} \la{j} \prod_{i=j+1}^n \ila{i}.
\end{align}
The factor $1/\la{j_0(n)}$ is required to make sure that
the Fock matrix prefactors sum to $1$,
\ie to make sure that the condition for
the linear combination of Fock matrices
\eqref{eqn:FockLinearCombination} is fulfilled.

The simplest case of this class of approximations is $m=1$.
This implies $j_0(n) = n$ such that \eqref{eqn:ODAFockApprox}
and \eqref{eqn:ODADensApprox} simplify to read
\begin{align*}
	\Fot{n} &= \Fo{n} & \Pet{n} &= \Pe{n}
\end{align*}
In other words this 2-step \tODA is equivalent to an \textit{adhoc} modification
of the exact ODA where we replace
$\tilde{\mat{D}}^{(n)}$ by $\mat{D}^{(n)}$,
the density of the previous \SCF step.
Taking this into account the expressions \eqref{eqn:ODAs} and \eqref{eqn:ODAc}
may be written as
\begin{align}
	\label{eqn:tODAs}
	\nonumber
	s &= \tr \left( \Fot{n} \Pe{n+1} \right) - E_1[\Pet{n}] - 2 E_2 [\Pet{n}] \\
	  &= \tr \left( \tp{\left(\mat{C}^{(n+1)}\right)} \Fo{n} \mat{C}^{(n+1)} \right)
	  - E_1\!\left[\mat{C}^{(n)} \tp{\left(\mat{C}^{(n)}\right)}\right]
	  - 2 E_2\! \left[\mat{C}^{(n)} \tp{\left(\mat{C}^{(n)}\right)}\right] \\
\intertext{and}
	\label{eqn:tODAc}
	\nonumber
c &= E_2[\Pe{n+1}] - \tr \left( \Fot{n} \Pe{n+1} \right) + E_1[\Pe{n+1}]
		+ E_2 [\Pet{n}] \\
	\nonumber
	&= E_2\!\left[\mat{C}^{(n+1)} \tp{\left(\mat{C}^{(n+1)}\right)}\right]
		- \tr \left( \tp{\left(\mat{C}^{(n+1)}\right)} \Fo{n} \mat{C}^{(n+1)} \right) \\
		&\hspace{50pt}
		+ E_1\!\left[\mat{C}^{(n+1)} \tp{\left(\mat{C}^{(n+1)}\right)}\right]
		+ E_2\!\left[\mat{C}^{(n)} \tp{\left(\mat{C}^{(n)}\right)}\right]
\end{align}
In contrast to the exact \ODA
this yields a coefficient-based \SCF algorithm.
Starting from an initial set of coefficients $\mat{C}^{(0)}$
with corresponding initial Fock matrix $\Fot{0} =
\mat{F}\!\!\left[\mat{C}^{(0)}\!\tp{\left(\mat{C}^{(0)}\right)}\right]$
we proceed for $n=1,2,3,\ldots$ as follows.
\begin{itemize}
	\item Diagonalise $\Fot{n-1}$ in order to obtain coefficients
		$\mat{C}_F^{(n)}$.
	\item According to the Aufbau principle select $\mat{C}^{(n)}$
		and build $\Fo{n} = \matFnfull$.
	\item Evaluate the Pulay error $\mat{e}^{(n)}$ \eqref{eqn:PulayError}
		and end the process if $\norm{\mat{e}^{(n)}}_\text{frob} \leq \epsilonconv$.
	\item Compute $s$, $c$ and $\lambda^{(n)}$
		according to \eqref{eqn:tODAs}, \eqref{eqn:tODAc}
		and \eqref{eqn:ODALambdaOpt}.
	\item Set
		\[ \Fot{n} = \ila{n} \Fo{n-1} + \la{n} \Fo{n} \]
		and repeat.
\end{itemize}
In this process one only needs the history of two Fock matrices $\Fo{n-1}$ and
$\Fo{n}$, such that $\Fot{n}$ can be applied when needed.
This in turn implies that only the coefficient matrices $\mat{C}^{(n-1)}$
and $\mat{C}^{(n)}$ are required, such that $\Fo{n}$ and $\Fo{n-1}$
can be applied whenever needed.

Compared to the Roothaan algorithm~(see section \ref{sec:RoothaanRepeatedDiag})
the \tODA only roughly doubles
the cost of each diagonalisation, since two Fock matrices need to be applied.
Additionally one needs to evaluate the trace
\[
	\tr \left( \tp{\left(\mat{C}^{(n+1)}\right)} \Fo{n} \mat{C}^{(n+1)} \right)
\]
and compute the energies
$E_1[\mat{D}^{(n)}]$ and $E_2[\mat{D}^{(n)}]$ in order to obtain $c$ and $s$
for each iteration.
The former step costs about as much as a single matrix-vector product
and the latter is usually done during the \SCF anyways
to display the progress to the user,
thus represents no extra cost.

Even though about twice as expensive as the Roothaan algorithm
if a \contract-based \SCF is performed,
the advantage of the \tODA is,
that it automatically finds the damping coefficient $\la{n}$,
which reduces the energy at each iteration as much as possible.
This amounts to break the oscillatory behaviour
of the standard Roothaan repeated diagonalisation scheme
in a slightly improved manner than the default damping
or level-shifting modifications.

One should mention, however, that the \tODA
does not inherit all of the nice mathematical properties from the \ODA.
For example it is no longer guaranteed that the \tODA
converges to a stationary point of the \HF problem \eqref{eqn:HFOptCoeff}%
~\cite{CancesODAprivate}.
Especially close to convergence it may for example happen,
that $\lambda^{(n)} \not\in [0,1]$ since both $c$ and $s$
become rather small, thus $2c \leq s$ ill-defined.
One can get around this by explicitly setting $\lambda^{(n)} = 1$ in the cases,
where $\abs{c}$ and $\abs{s}$ become small.
The \tODA is thus best used in the initial \SCF steps in order to
effectively prevent the Roothaan oscillations from happening.

\subsection{Direct inversion of the iterative subspace}
\label{sec:DIIS}



Standard commutator DIIS by Pulay~\cite{Pulay1982,Hamilton1986}

Other ways~\cite{Shepard2007}
LSIIS (least-squares commutator DIIS)~\cite{Li2016}



$\tilde{\mat{F}}^{(n)}$ gradient of $\mathcal{E}_D(\mat{D})$
wrt. $\mat{D}$~\cite{Lions1988,Cances2000}.
Can define~\cite{Lions1988,Cances2000}
\[
	\mat{C}^{(n+1)} = \arginf_{\mat{C} \in \mathcal{C}}
\left\{ \tr \tilde{\mat{F}}^{(n)} \mat{C} \tp{\mat{C}} \right\} \]



\subsection{Geometric direct minimisation}
Only brief

\subsection{Second-order \SCF algorithms}
\label{sec:SOSCF}
Only brief
mention that approximate methods exist, too

% https://www.sciencedirect.com/science/article/pii/0301010481851567

Quadratically-convergent scf \cite{Ochsenfeld1997}
Augmented Roothan-Hall \cite{Hoest2008}

\todoil{Check out: Linear scaling SCF as minimisation \cite{Salek2007} --- this is second order
and based on the density, preconditioned CG-like minimisation}


\subsection{Combinations of algorithms}
Ideally want to switch every now and then


DIIS and ODA
