\section{Self-consistent field algorithms}
\label{sec:SCFAlgorithms}

In this section we want to discuss a few standard self-consistent field algorithms
in the light of the various types of basis functions we discussed in the previous section.
Even though it is my hope that the selection of algorithms discussed here
is representative,
the vast number of methods,
which has been developed over the years,
makes it impossible to be exhaustive.

Most \SCF algorithms are designed only with a \cGTO-based
discretisation of the \HF and Kohn-Sham \DFT problem in mind.
The deviating numerical properties of the finite-element method
or a \CS-based discretisation,
therefore often call for minor modifications of the schemes.
For example both finite elements as well as Coulomb-Sturmians
favour \contract-based methods
due to the better scaling of equations like \eqref{eqn:ExchangeApply}
and \eqref{eqn:ApplicationKcs}
compared to building the full matrix.
Therefore the Fock matrix might not be built in memory any more,
which implies that a linear combination of Fock matrices
cannot be computed in memory either.
This does not imply that \SCF schemes,
which form linear combinations of Fock matrices
are completely ruled out,
but they might become less favourable compared to other schemes.
See section \ref{sec:DIIS} below for details.

On the other hand in \FE-based approaches all quantities,
which scale quadratically in $\Nbas$ cannot be stored in memory.
This applies not only to the iterated Fock matrix $\mat{F}^{(n)}$,
but to the density matrix $\mat{D}^{(n)} \in \R^{\Nbas\times \Nbas}$ as well.
Even though clever low-rank approximation methods
like hierarchical matrices~\cite{Hackbusch1999,Hackbusch2002,Grasedyck2003,Hackbusch2015}
or tensor decomposition methods
\todo{cite}
could reduce the memory footprint of the density matrix,
this work will try to indicate ways
by which building the density matrix in an \SCF can be avoided.
Naturally this implies
a focus on coefficient-based \SCF schemes as well,
where the number of iterated parameters
--- the coefficient matrix $\mat{C} \in \R^{\Nbas\times\Norb}$ ---
scales only linearly in $\Nbas$.
Furthermore coefficient-based \SCF schemes have the advantage,
that iterating the density matrix
destroys the possibility to perform the
linear-scaling application of $\mat{K}$ for \CS-based methods,
see equation \eqref{eqn:ApplicationKcs} for details.

\todoil{Pagebreak here?}
\pagebreak
It was already pointed out in section \vref{sec:OverviewSCF}
that focusing on coefficient-based schemes
is hardly a restriction in terms of the number of possible approaches,
since coefficient-based and density-based schemes can be interconverted,
at least approximately.
We will suggest modifications of the
optimal damping algorithm~(ODA)~\cite{Cances2000a} in section \ref{sec:tODA}
and Pulay's direct inversion of the iterative subspace~\cite{Pulay1982}
in section \ref{sec:DIIS},
which bring these methods to the coefficient-based setting.

Most of the \SCF algorithms we will consider here
only converge the \HF equations \eqref{eqn:HFiterated}
until the Pulay error \eqref{eqn:PulayError} vanishes
following our general description in remark \vref{rem:SCFcoeff}.
Regarding the \HF optimisation problem \eqref{eqn:HFOptCoeff}
this is only the necessary condition for a stationary point on the Stiefel
manifold $\mathcal{C}$.
Only some \SCF algorithms
termed \newterm{second-order self-consistent field method}s
guarantee (usually only appropriately)
that the stationary point they find is truly a minimum.
They are briefly considered in section \ref{sec:SOSCF}.

\subsection{Roothaan repeated diagonalisation}
\label{sec:RoothaanRepeatedDiag}

Roothaan's repeated diagonalisation~\cite{Roothaan1951}
approach to the \HF problem \eqref{eqn:HFiterated} is by far the simplest.
In the formalism of remark \vref{rem:SCFcoeff}
this algorithm can be described by building the next Fock matrix
$\tilde{\mat{F}}^{(n)}$ only by considering
the current occupied coefficients $\mat{C}^{(n)}$,
\ie $\tilde{\mat{F}}^{(n)} = \matFnfull$.
The two-step iteration procedure of figure \vref{fig:RoothanRepeatedDiag} results.
\begin{figure}
	\centering
	\missingfigure{Roothaan algorithm diagram}
	\caption{Roothaan algorithm diagram}
	\label{fig:RoothanRepeatedDiag}
\end{figure}

Even though Roothaan's algorithm already works for a few simple cases,
it is far from being reliable.
For example one can show~\cite{Cances2000,Cances2000b} that it
either converges to a stationary point of the
discretised \HF problem \eqref{eqn:HFOptCoeff}
or alternatively it numerically oscillates between two states,
where none of them are a stationary point of \eqref{eqn:HFOptCoeff}.
In practice which of these cases occurs is dependent on the system
as well as the basis set.
Furthermore there is no guarantee that the resulting stationary point
the Roothaan's algorithm finds is the \HF ground state.
All these cases are already found for \HF calculations
on the first three periods of the periodic table~\cite{Cances2000}.

\subsection{Level-shifting modification}
If one uses essentially the same \SCF scheme as \ref{fig:RoothanRepeatedDiag}
but instead diagonalises the matrix
\[ \tilde{\mat{F}}^{(n)} = \matFnfull
	- b \mat{S} \, \mat{C}^{(n)}\!\tp{\left(\mat{C}^{(n)}\right)} \, \mat{S} \]
where $b > 0$,
already a much better convergence is achieved.
This modification is called \newterm{level shifting}%
~\cite{Saunders1973,Guest1974},
where $b$ is the \textbf{level-shifting parameter},
typically chosen in the range between $0.1$ and $0.5$.
Effectively this modification increases the energy gap between occupied and virtual
orbital energies.
To see this, let us consider the converged case, where
\[ \mat{F} \mat{C}_F = \mat{S} \mat{C}_F \mat{E} \]
exactly and let us partition the full coefficient matrix%
\footnote{We assume \RHF here and furthermore only consider the $\alpha$ block.
	For \UHF the analysis is exactly the same with
	the relevant equations just replicated in $\alpha$ and $\beta$ block.}
\[
	\mat{C}_F = \mm{\mat{C} & \mat{C}_\text{virt}}
\]
into occupied and virtual parts.
Now let $\tilde{\mat{F}} = \mat{F} - b \mat{S} \, \mat{C}\tp{\mat{C}} \, \mat{S}$
such that
\begin{align*}
	\tilde{\mat{F}} \mat{C}_F
	&= \left( \mat{F} - b \mat{S} \, \mat{C}\tp{\mat{C}} \, \mat{S} \right) \mat{C}_F \\
	&= \mat{S} \mat{C}_F \mat{E} - b \mat{S} \mm{
		\mat{C}\tp{\mat{C}} \mat{S} \mat{C} &
		\mat{C}\tp{\mat{C}} \mat{S} \mat{C}_\text{virt}
	} \\
	&= \mat{S} \mat{C}_F \mat{E} - b \mat{S} \mm{ \mat{C} & 0 } \\
	&= \mat{S} \mat{C}_F \mat{E} + \mat{S} \mat{C}_F \mm{ -b  & 0 } \\
	&= \mat{S} \mat{C}_F \tilde{\mat{E}}
\end{align*}
where
\[ \tilde{\mat{E}} = \diag\left(\varepsilon_1 - b, \varepsilon_2 - b, \ldots,
	\varepsilon_{\Nelec} - b, \varepsilon_{\Nelec+1}, \ldots, \varepsilon_{\Norb}\right).
\]
In other words the virtual orbitals are unaffected
whereas the occupied orbitals are shifted downwards in energy by an amount $b$.

The effect of this is that coupling between both orbital spaces is reduced,
which tends to lead to faster convergence
especially if the gap between $\varepsilon_{\Nelec}$ and $\varepsilon_{\Nelec+1}$ is small.
This empirical observation is backed up by more sophisticated
mathematical analysis by \citet{Cances2000b}.
Their result shows that for large enough $b$,
the level-shifted Roothaan procedure is guaranteed to converge to a stationary
point of the \HF problem \eqref{eqn:HFOptCoeff}.
The also provide an expression for the lower bound of $b$.
In this manner convergence to a stationary point
can be forced even for cases where the original \HF equations \eqref{eqn:HFMO}
have no solution (like the negative ions with $N > 2Z + M$).
In such a case the result is no physical ground state, however.

One can show~\cite{Saunders1973}
that the level-shifting modification is mathematically equivalent
to another modification of Roothaan's repeated diagonalisation,
called \newterm{damping}.
In this procedure one chooses a \newterm{damping factor} $0 < \alpha < 1$
and sets
\begin{equation}
	\tilde{\mat{F}}^{(n)} = (1-\alpha) \tilde{\mat{F}}^{(n-1)} + \alpha \matFnfull,
	\label{eqn:FockDamping}
\end{equation}
such that the new Fock matrix to diagonalise contains still a share
of the old Fock matrix.

\defineabbr{ODA}{ODA\xspace}{Optimal damping algorithm}
\subsection{Optimal damping algorithm}
\label{sec:ODA}
The optimal damping algorithm~(\ODA) was proposed by \citet{Cances2000a}
based on their analysis of the Roothaan algorithm
including the level-shifting modification.

\begin{figure}
	\centering
	\missingfigure{Optimal damping algorithm}
	\caption{Optimal damping algorithm}
	\label{fig:OptimalDampingAlgorithm}
\end{figure}
In unmodified form~\cite{Cances2000,Cances2000a} it is a density-based \SCF algorithm.
Starting from an initial density $\tilde{\mat{D}}^{(0)} = \mat{D}^{(0)}$,
the procedure is roughly~(compare figure \ref{fig:OptimalDampingAlgorithm})
for $n=1, 2, 3, \ldots$
\begin{itemize}
	\item Build the Fock matrix
	\begin{equation}
	\tilde{\mat{F}}^{(n-1)} = \mat{F}\left[ \tilde{\mat{D}}^{(n)} \right]
		\label{eqn:ODAupFock}
	\end{equation}
	and diagonalise it to obtain the new coefficient $\mat{C}_F^{(n)}$.
	Form the new density $\mat{D}^{(n)}$ according to the Aufbau principle from these
	as
	\[ \mat{D}^{(n)} = \mat{C}^{(n)} \tp{\left( \mat{C}^{(n)} \right)}. \]
	%
	\item Evaluate the Pulay error $\mat{e}^{(n)}$ \eqref{eqn:PulayError}
		and end the process if $\norm{\mat{e}^{(n)}}_\text{frob} \leq \epsilonconv$.
	%
	\item Solve the line search problem
		\begin{equation}
			\tilde{\mat{D}}^{(n+1)}
			= \arginf_{\tilde{\mat{D}} \in
			\text{Seg}\left[\tilde{\mat{D}}^{(n)}, \mat{D}^{(n+1)}\right]}
			\mathcal{E}_D^\text{HF}[\tilde{\mat{D}}]
			\label{eqn:ODAupDens}
		\end{equation}
		where
		\[
			\text{Seg}\left[\mat{D}_1, \mat{D}_2\right]
				= \left\{ (1-\lambda) \mat{D}_1 + \lambda \mat{D}_2 \, \Big|\,
				\lambda \in [0,1] \right\}
		\]
		is a line segment of density matrices
		and the energy functional $\mathcal{E}_D^\text{HF}$
		is defined as in \eqref{eqn:HFEnergyFunctionalDens}.
		Repeat the process thereafter.
\end{itemize}
One can show~\cite{Cances2000} that the \ODA
\emph{always} converges to a local minimum of \eqref{eqn:HFOptDens}.

The remaining question to complete the picture of the \ODA
from a computational point of view
is to find a way to obtain the minimal density $\tilde{\mat{D}}^{(n+1)}$.
First notice that in general the density matrix segment
\[\text{Seg}\left[\mat{D}_1, \mat{D}_2\right] \not\subset \mathcal{P}\]
even if $\mat{D}_1, \mat{D}_2 \in \mathcal{P}$.
Much rather this line segment is fully contained only in a superset
$\tilde{\mathcal{P}} \supset \mathcal{P}$,
where we relax the constraint $\mat{D}^2 = \mat{D}$ to%
\footnote{Let $\mat{A}, \mat{B} \in \R^{n\times n}$,
then $\mat{A} \leq \mat{B}
\Leftrightarrow \forall \vec{x} \in \R^n \
\tp{\vec{x}}\mat{A} \vec{x} \leq \tp{\vec{x}}\mat{B}\vec{x}$}
$\mat{D}^2 \leq \mat{D}$.
See \cite{Cances2000} for details.
For ease of notation let us define
%
\newcommand{\Gfct}[1]{  \mat{G}\left[ #1 \right]   }
\newcommand{\Pet}[1]{\textcolor{dkred}{\tilde{\mat{D}}^{(#1)}}}
\newcommand{\Pe}[1]{\textcolor{dkblue}{\mat{D}^{(#1)}}}
\newcommand{\Fot}[1]{\textcolor{dkred}{\tilde{\mat{F}}^{(#1)}}}
\newcommand{\Fo}[1]{\textcolor{dkblue}{\mat{F}^{(#1)}}}
\newcommand{\la}[1]{\lambda^{(#1)}}
\newcommand{\ila}[1]{\left( 1 - \la{#1} \right)}
\newcommand{\EHFD}[1]{\mathcal{E}_D^\text{HF}\left[#1\right]}
%
\newcommand{\Pn}{\Pet{n}}
\newcommand{\Pnn}{\Pe{n+1}}
\begin{align*}
	E_1 \left[\mat{D}\right] &\equiv \tr\left( \mat{T} \mat{D} + \mat{V}_0 \mat{D} \right), \\
	\mat{G}\left[ \mat{D} \right] &\equiv
		\mat{F}\left[ \mat{D} \right] + \mat{K}\left[ \mat{D} \right] \\
\intertext{and}
	E_2 \left[\mat{D}\right] &\equiv
		\frac12 \tr\left( \mat{D} \, \mat{G}\left[ \mat{D} \right] \right).
\end{align*}
For all matrices $\mat{D}_1, \mat{D}_2 \in \tilde{\mathcal{P}}$
we can show the properties~\cite{Cances2000a}
\begin{align}
	\label{eqn:TermGsymmetry}
	\tr \left( \mat{D}_1 \mat{G}\left[ \mat{D}_2 \right] \right)
		&= \tr \left( \mat{D}_2 \mat{G}\left[ \mat{D}_1 \right] \right) \\
	\label{eqn:GmatTrace}
	\tr\left( \mat{F}\left[ \mat{D}_1 \right] \mat{D}_2 \right)
	&= E_1[\mat{D}_2] + \tr\left( \mat{D}_1 \Gfct{\mat{D}_2} \right)
\end{align}
These imply for $E_2$ and arbitrary $\alpha, \beta \in \R$
\begin{equation}
	\begin{aligned}
	E_2[\alpha \mat{D}_1 + \beta\mat{D}_2]
	&= \frac 1 2 \tr\left( \alpha^2  \mat{D}_1 \Gfct{\mat{D_1}} \right)
		+ \frac 1 2 \tr\left( \alpha\beta \mat{D_1} \Gfct{\mat{D}_2} \right) \\
		\nonumber
		&\hspace{30pt}
		+ \frac 1 2 \tr\left( \alpha\beta \mat{D}_2 \Gfct{\mat{D_1}} \right)
		+ \frac 1 2 \tr\left( \beta^2 \mat{D}_2 \Gfct{\mat{D}_2} \right) \\
	 &\stackrel{\eqref{eqn:GmatTrace}}{=}
	 \alpha^2 E_2[\mat{D_1}] + \beta^2 E_2[\mat{D}_2]
	 + \alpha\beta \tr\left( \mat{D_1} \Gfct{\mat{D}_2} \right),
	\end{aligned}
	\label{eqn:HFE2}
\end{equation}
whereas $E_1$ is linear
\begin{equation}
	E_1[\alpha \mat{D}_1 +\beta \mat{D}_2] = \alpha E_1[\mat{D}_1] + \beta E_1[\mat{D}_2].
	\label{eqn:HFE1}
\end{equation}
These results allow to expand
the \HF energy for a member $\Pet{n+1}$
of the density matrix segment
$\text{Seg}\left[\Pet{n}, \Pe{n+1}\right]$
as
\begin{equation}
\begin{aligned}
	\EHFD{\Pet{n+1}} &= \EHFD{\ila{n+1} \Pet{n} + \la{n+1} \Pe{n+1}} \\
	&= \EHFD{\Pet{n} + \la{n+1} \left( \Pe{n+1}  - \Pet{n} \right)} \\
	&= E_1\left[ \Pet{n} + \la{n+1} \left( \Pe{n+1}  - \Pet{n} \right) \right] \\
		&\hspace{50pt}+E_2\left[\Pet{n} + \la{n+1} \left( \Pe{n+1}  - \Pet{n} \right) \right] \\
	&= E_1[\Pet{n}] + \la{n+1} E_1\left[\Pe{n+1}  - \Pet{n}\right] + E_2[\Pet{n}]\\
		&\hspace{50pt} + \la{n+1} \tr \left( \Pet{n} \Gfct{\Pe{n+1}-\Pet{n}} \right) \\
		&\hspace{50pt}
		+ \left( \la{n+1} \right)^2 E_2[\Pe{n}-\Pet{n}] \\
	&= \EHFD{\Pet{n}}
	+ \la{n+1} \underbrace{\tr\left( \Pet{n} \mat{F} \left[ \Pe{n+1}-\Pet{n} \right] \right)}_{=s} \\
	&\hspace{50pt}
	+ \left( \la{n+1} \right)^2 \underbrace{E_2[\Pe{n}-\Pet{n}]}_{=c} \\
	&= \EHFD{ \Pet{n} } + \la{n+1} s + \left( \la{n+1} \right)^2 c
\end{aligned}
\label{eqn:ODAquadratic}
\end{equation}
The coefficients $s$ and $c$ can alternatively be written as
\begin{equation}
\begin{aligned}
	s &= \tr\left( \Fot{n} \left( \Pe{n+1} - \Pet{n} \right) \right) \\
	&= \tr \left( \Fot{n} \Pe{n+1} \right) - E_\text{HF}[\Pet{n}] - E_2 [\Pet{n}] \\
	&= \tr \left( \Fot{n} \Pe{n+1} \right) - E_1[\Pet{n}] - 2 E_2 [\Pet{n}]  \\
\end{aligned}
\label{eqn:ODAs}
\end{equation}
and\footnote{Note that the original paper \cite{Cances2000a} uses a deviating formalism %
which causes an extra factor of $2$ to appear in their expression for $c$.}
\begin{equation}
\begin{aligned}
	c &= E_2\left[ \Pe{n+1} - \Pet{n} \right] \\
	&\stackrel{\eqref{eqn:HFE2}}{=}
		E_2 [\Pe{n+1}] - \tr\left( \Gfct{\Pet{n}} \Pe{n+1} \right) + E_2 [\Pet{n}] \\
	&= E_2 [\Pe{n+1}] - \tr \left( \Fot{n} \Pe{n+1} \right) + E_1[\Pe{n+1}] + E_2 [\Pet{n}]
\end{aligned}
\label{eqn:ODAc}
\end{equation}
Now the stationary point along the density matrix segment
can be determined by differentiating \eqref{eqn:ODAquadratic} resulting in
\begin{align*}
	\frac{\partial \EHFD{\Pet{n+1}}}{\partial \la{n+1}}
	&= s + 2 \la{n+1} c
	&&\text{and}&
	\frac{\partial^2 \EHFD{\Pet{n+1}}}{\partial \left(\la{n+1}\right)^2} &= 2c
\end{align*}
Due to $E_2[\mat{D}] \geq 0$~\cite{Cances2000a} for all $\mat{D} \in \tilde{\mathcal{P}}$
one easily deduces $c \geq 0$,
such that the stationary point of the above expression is always a minimum.
Since $\la{n+1} \in [0,1]$ the minimiser is
\begin{equation}
	\label{eqn:ODALambdaOpt}
	\la{n+1}_\text{min} = \left\{
	\begin{array}{cl}
		1 & \text{if $2c \leq -s$} \\
		- \frac{s}{2c} & \text{else}
	\end{array}
	\right.,
\end{equation}
where the cases $c=0$ and $s=0$ have been ignored, since they only occur at convergence.
This closes the missing link and allows to implement a \ODA
in as a density-based \SCF.

Let $\alpha, \beta \in \R$ and $\mat{D}_1, \mat{D}_2 \in \tilde{\mathcal{P}}$.
Since $\matFfullD = \mat{T} + \mat{V}_0 + \matJfullD + \matKfullD$
and the two-electron terms are linear in the density matrix, we have
\begin{equation}
	\mat{F}\!\left[\alpha \mat{D}_1 + \beta\mat{D}_2\right]
	= \alpha \mat{F}\!\left[\mat{D}_1\right] + \beta \mat{F}\!\left[\mat{D}_2\right]
	\label{eqn:FockLinearCombination}
\end{equation}
iff $\alpha + \beta = 1$. Defining
\begin{align*}
	\Fot{n} &\equiv  \mat{F}\left[ \Pet{n} \right] &
	\Fo{n} &\equiv \mat{F}\left[ \Pe{n} \right]
\end{align*}
this allows to rewrite \eqref{eqn:ODAupFock} as
\begin{equation}
	\label{eqn:ODAdamping}
	\Fot{n}
	= \mat{F}\left[ \ila{n} \Pet{n-1} + \la{n} \Pe{n} \right]
	= \ila{n} \Fot{n-1} + \la{n} \Fo{n},
\end{equation}
where the ``$\text{min}$'' subscripts were dropped.
Comparing with equation \eqref{eqn:FockDamping}
one can identify with $\la{n}$ the damping factor $\alpha$.
Since $\la{n}$ is optimal in the sense of minimising the energy along the line segment
spanned by $\Pe{n}$ and $\Pet{n-1}$,
the \ODA can be described by repetitively finding the optimal damping
parameter from \SCF step to \SCF step.
Notice that its construction guarantees that the \SCF energy will always decrease.
It is hence guaranteed to converge to a local minimum of the \HF problem
\eqref{eqn:HFOptDens}~\cite{Cances2000,Cances2000a}.
The \ODA is only a particularly simple example from a whole family
of density-based \SCF algorithms called relaxed constraints algorithms,
which are discussed in detail in \cite{Cances2000a}.

\noindent
Using \eqref{eqn:ODAdamping} one can show by induction that
\begin{align}
	\label{eqn:ODAFock}
	\Fot{n} &= \sum_{j = 0}^n \Fo{j} \la{j} \prod_{i=j+1}^n \ila{i}, \\
	\label{eqn:ODADens}
	\Pet{n} &= \sum_{j = 0}^n \Pe{j} \la{j} \prod_{i=j+1}^n \ila{i},
\end{align}
where we set $\la{0} \equiv 1$. Since
\begin{align*}
	\Fo{j} &= \matFnfull \\
	\Pe{j} &= \mat{C}^{(j)} \tp{\left(\mat{C}^{(j)}\right)} \\
\end{align*}
these results in theory allow to express the complete \ODA
in terms of the coefficients
such that expressions like \eqref{eqn:ExchangeApply}
or \eqref{eqn:ApplicationKcs} could be used for a \FE-based
or a \CS-based discretisation respectively.

In practice this is usually not a fruitful approach for two reasons.
Firstly it requires to store a growing list of coefficients,
namely one for each \SCF step.
Especially for a \FE approach this becomes increasingly costly in terms of memory.
Secondly for a \contract-based ansatz
we especially want to avoid storing the Fock matrices $\Fo{j}$
in favour of \contract expression like \eqref{eqn:ExchangeApply}
and \eqref{eqn:ApplicationKcs}.
In other words each application of $\Fot{n}$ to a vector $\vec{x}$
would need to be performed by first computing $\Fo{j} \vec{x}$ for each $j$
and then adding the results.
This procedure is roughly $n$ times as expensive as a single apply.
Even though the \contract expressions formally scale better,
the increasing number of times they need to be invoked
should make this ansatz rather expensive.

Overall the \ODA is very suitable for {\cGTO} and \CS-based discretisations,
since for these density-based \SCF schemes are fine,
but this algorithm is not suitable for solving the \HF problem
with a \FE-based discretisation without further modifications.

\defineabbr{tODA}{tODA\xspace}{Truncated optimal damping algorithm}
\subsection{Truncated optimal damping algorithm}
\label{sec:tODA}

Let us again consider \eqref{eqn:ODAFock}.
Due to $\la{i} \in [0,1]$ the Fock matrix prefactor
\begin{equation}
	\label{eqn:ODAFockCoeff}
	\lambda^{(j)} \prod_{i=j+1}^{n} \left(1- \lambda^{(i)}\right) \in [0,1]
\end{equation}
is a product of factors,
which are all between $0$ and $1$.
Therefore this prefactor may become
rather small for small values of $j$ as $n$ increases.
In other words in the later \SCF steps the $\Fo{j}$ terms
which were produced at the beginning of the \SCF procedure
may be accompanied with a small prefactor and hence can at some point
be neglected in \eqref{eqn:ODAFock}.
This is the justification for the truncated optimal damping algorithm~(\tODA),
which approximates the \ODA by artificially restricting the number
of terms in \eqref{eqn:ODAFock} to the $m$ most recently
obtained Fock matrices.
If we define
\[
	j_0(n) \equiv n-m+1
\]
this allows to write the approximated sums as
\begin{align}
	\label{eqn:ODAFockApprox}
	\Fot{n} &= \frac{1}{\la{j_0(n)}} \sum_{j = j_0(n)}^n \Fo{j} \la{j} \prod_{i=j+1}^n \ila{i}, \\
\intertext{and analogously for the density matrices}
	\label{eqn:ODADensApprox}
	\Pet{n} &= \frac{1}{\la{j_0(n)}} \sum_{j = j_0(n)}^n \Pe{j} \la{j} \prod_{i=j+1}^n \ila{i}.
\end{align}
The factor $1/\la{j_0(n)}$ is required to make sure that
the Fock matrix prefactors sum to $1$,
\ie to make sure that the condition for
the linear combination of Fock matrices
\eqref{eqn:FockLinearCombination} is fulfilled.

The simplest case of this class of approximations is $m=1$.
This implies $j_0(n) = n$ such that \eqref{eqn:ODAFockApprox}
and \eqref{eqn:ODADensApprox} simplify to read
\begin{align*}
	\Fot{n} &= \Fo{n} & \Pet{n} &= \Pe{n}
\end{align*}
In other words this 2-step \tODA is equivalent to an \textit{adhoc} modification
of the exact ODA where we replace
$\tilde{\mat{D}}^{(n)}$ by $\mat{D}^{(n)}$,
the density of the previous \SCF step.
Taking this into account the expressions \eqref{eqn:ODAs} and \eqref{eqn:ODAc}
may be written as
\begin{align}
	\label{eqn:tODAs}
	\nonumber
	s &= \tr \left( \Fot{n} \Pe{n+1} \right) - E_1[\Pet{n}] - 2 E_2 [\Pet{n}] \\
	  &= \tr \left( \tp{\left(\mat{C}^{(n+1)}\right)} \Fo{n} \mat{C}^{(n+1)} \right)
	  - E_1\!\left[\mat{C}^{(n)} \tp{\left(\mat{C}^{(n)}\right)}\right]
	  - 2 E_2\! \left[\mat{C}^{(n)} \tp{\left(\mat{C}^{(n)}\right)}\right] \\
\intertext{and}
	\label{eqn:tODAc}
	\nonumber
c &= E_2[\Pe{n+1}] - \tr \left( \Fot{n} \Pe{n+1} \right) + E_1[\Pe{n+1}]
		+ E_2 [\Pet{n}] \\
	\nonumber
	&= E_2\!\left[\mat{C}^{(n+1)} \tp{\left(\mat{C}^{(n+1)}\right)}\right]
		- \tr \left( \tp{\left(\mat{C}^{(n+1)}\right)} \Fo{n} \mat{C}^{(n+1)} \right) \\
		&\hspace{50pt}
		+ E_1\!\left[\mat{C}^{(n+1)} \tp{\left(\mat{C}^{(n+1)}\right)}\right]
		+ E_2\!\left[\mat{C}^{(n)} \tp{\left(\mat{C}^{(n)}\right)}\right]
\end{align}
In contrast to the exact \ODA
this yields a coefficient-based \SCF algorithm.
Starting from an initial set of coefficients $\mat{C}^{(0)}$
with corresponding initial Fock matrix $\Fot{0} =
\mat{F}\!\!\left[\mat{C}^{(0)}\!\tp{\left(\mat{C}^{(0)}\right)}\right]$
we proceed for $n=1,2,3,\ldots$ as follows.
\begin{itemize}
	\item Diagonalise $\Fot{n-1}$ in order to obtain coefficients
		$\mat{C}_F^{(n)}$.
	\item According to the Aufbau principle select $\mat{C}^{(n)}$
		and build $\Fo{n} = \matFnfull$.
	\item Evaluate the Pulay error $\mat{e}^{(n)}$ \eqref{eqn:PulayError}
		and end the process if $\norm{\mat{e}^{(n)}}_\text{frob} \leq \epsilonconv$.
	\item Compute $s$, $c$ and $\lambda^{(n)}$
		according to \eqref{eqn:tODAs}, \eqref{eqn:tODAc}
		and \eqref{eqn:ODALambdaOpt}.
	\item Set
		\[ \Fot{n} = \ila{n} \Fo{n-1} + \la{n} \Fo{n} \]
		and repeat.
\end{itemize}
In this process one only needs the history of two Fock matrices $\Fo{n-1}$ and
$\Fo{n}$, such that $\Fot{n}$ can be applied when needed.
This in turn implies that only the coefficient matrices $\mat{C}^{(n-1)}$
and $\mat{C}^{(n)}$ are required, such that $\Fo{n}$ and $\Fo{n-1}$
can be applied whenever needed.

Compared to the Roothaan algorithm~(see section \ref{sec:RoothaanRepeatedDiag})
the \tODA only roughly doubles
the cost of each diagonalisation, since two Fock matrices need to be applied.
Additionally one needs to evaluate the trace
\[
	\tr \left( \tp{\left(\mat{C}^{(n+1)}\right)} \Fo{n} \mat{C}^{(n+1)} \right)
\]
and compute the energies
$E_1[\mat{D}^{(n)}]$ and $E_2[\mat{D}^{(n)}]$ in order to obtain $c$ and $s$
for each iteration.
The former step costs about as much as a single matrix-vector product
and the latter is usually done during the \SCF anyways
to display the progress to the user,
thus represents no extra cost.

Even though about twice as expensive as the Roothaan algorithm
if a \contract-based \SCF is performed,
the advantage of the \tODA is,
that it automatically finds the damping coefficient $\la{n}$,
which reduces the energy at each iteration as much as possible.
This amounts to break the oscillatory behaviour
of the standard Roothaan repeated diagonalisation scheme
in a slightly improved manner than the default damping
or level-shifting modifications.

One should mention, however, that the \tODA
does not inherit all of the nice mathematical properties from the \ODA.
For example it is no longer guaranteed that the \tODA
converges to a minimum of the \HF problem \eqref{eqn:HFOptCoeff}%
~\cite{CancesODAprivate}.
Especially close to convergence it may for example happen,
that $\lambda^{(n)} \not\in [0,1]$ since both $c$ and $s$
become rather small, thus $2c \leq s$ ill-defined.
One can get around this by explicitly setting $\lambda^{(n)} = 1$ in the cases,
where $\abs{c}$ and $\abs{s}$ become small.
The \tODA is thus best used in the initial \SCF steps in order to
effectively prevent the Roothaan oscillations from happening.

\subsection{Direct inversion in the iterative subspace}
\label{sec:DIIS}
\defineabbr{DIIS}{DIIS\xspace}{Direct inversion in the iterative subspace}

In his celebrated \citeyear{Pulay1982} paper \citeauthor{Pulay1982}
not only introduced the aforementioned Pulay error \eqref{eqn:PulayError},
but also improved upon his previously introduced
\SCF convergence acceleration scheme~\cite{Pulay1980}.
This effort resulted in the procedure now widely known
by the term \newterm{direct inversion in the iterative subspace}~(\DIIS).
In his variant of the DIIS procedure the next Fock matrix was found
as a linear combination
\begin{equation}
	\tilde{\mat{F}}^{(n)} = \sum_{i=0}^{m-1} c_i \mat{F}^{(n-i)}
	\label{eqn:DIISfock}
\end{equation}
of Fock matrices from the $m$ most recent \SCF steps, \ie
\[ \mat{F}^{(j)}
	= \mat{F}\!\!\left[\mat{C}^{(j)}\!\tp{\left(\mat{C}^{(j)}\right)}\right].
\]
The coefficients $\{c_i\}_{i=1,\ldots,m}$ are to be determined such that
the norm of the corresponding linear combination of Pulay errors
\begin{equation}
	f_\text{DIIS}(c_0, c_1, \ldots, c_{m-1})
	= \norm{\sum_{i=0}^{m-1} c_i \mat{e}^{(n-i)}}_\text{frob}^2 
	= \sum_{i=0}^{m-1} \sum_{j=0}^{m-1} c_i c_j \tr\left( \mat{e}^{(n-i)} \mat{e}^{(n-j)}  \right)
\end{equation}
is minimal.
Defining a real-symmetric matrix $\mat{B} \in \R^{m\times m}$ with elements
\begin{equation}
	B_{ij} = \tr\left( \mat{e}^{(n-i)} \mat{e}^{(n-j)} \right)
	\label{eqn:BMatElem}
\end{equation}
we can alternatively write
\begin{equation}
	f_\text{DIIS}(\vec{c}) = \tp{\vec{c}} \mat{B} \vec{c}.
	\label{eqn:DIISobjective}
\end{equation}
In agreement with what was discussed in equation \eqref{eqn:FockLinearCombination},
we need to additionally impose the constraint
\[
	\sum_{i=0}^{m-1} c_i = 1,
\]
such that the resulting Fock matrix $\tilde{\mat{F}}^{(n)}$ is physically sensible.
In other words Pulay's \DIIS scheme can be expressed
as the quadratic programming problem
\[
	\vec{c} = \argmin\left\{
		\tp{\vec{c}} \mat{B} \vec{c}
		\, \middle| \,
		\sum_{i=0}^{m-1} c_i = 1
	\right\},
\]
which has corresponding Euler-Lagrange equations
\begin{equation}
	\mm{
		\mat{B} & \vec{1} \\
		\tp{\vec{1}} & 0
	}
	\mm{\vec{c} \\ \lambda}
	= \mm{\vec{0} \\ 1},
	\label{eqn:DIISlinsys}
\end{equation}
where $\vec{1}, \vec{0} \in \R^m$ are column vectors of $m$ ones and $m$ zeros
and $\lambda$ is the Lagrange multiplier corresponding to the constraint
$\sum_{i=0}^{m-1} c_i = 1$.
Typically one takes $m$ between $2$ and $10$,
such that the linear system \eqref{eqn:DIISlinsys} can be solved by either dense
or iterative methods rather fast.

There are a few issues,
which typically occur close to self-consistency,
where the error matrices $\{\mat{e}^{(n-i)}\}_{i=0,\ldots,m-1}$
will be almost identical.
This causes multiple rows in $\mat{B}$ to be extremely similar
and thus gives rise to an ill-conditioned linear system \eqref{eqn:DIISlinsys}.
There are a couple of remedies typically used in practice~\cite{Kudin2002}.
For example one may drop the Fock matrices $\mat{F}^{(n-i)}$
where the coupling to the most recent Fock matrix $\mat{F}^{(n)}$,
\ie the matrix element $B_{0i}$ is smaller than a certain threshold.
Alternatively one may artificially bias the lowest-energy solution,
by multiplying all other diagonal entries $B_{ii}$ by a penalty
factor slightly larger than $1$.
Last but not least one may always drop the oldest Fock matrix
$\mat{F}^{(n-m+1)}$ if an ill-conditioned linear system is detected.
Alternatively one can remove linear dependencies by a singular-value
decomposition of $\mat{B}$.
Another point worth noting is that the \DIIS procedure is an \emph{extrapolation}
technique.
In other words there is no guarantee,
that the Fock matrix $\tilde{\mat{F}}^{(n)}$ is of any physical
or mathematical significance.
It could lead into totally the wrong direction causing the \SCF
procedure to eventually diverge.

Since there is no reason to build the density matrix
in Pulay's \DIIS procedure
it is suitable for both density-based as well as coefficent-based \SCF settings.
Moreover given that the $m$ coefficients $\{ \mat{C}^{(n-i)} \}_{i=0,\ldots,m-1}$
are stored,
the Fock matrix terms of \eqref{eqn:DIISfock}
can be applied using expressions like \eqref{eqn:ExchangeApply}
or \eqref{eqn:ApplicationKcs} without any problems,
making the \DIIS suitable for a \contract-based \SCF as well.

Let us briefly summarise the procedure in a \contract-based \SCF.
Start from an initial set of occupied coefficients $\mat{C}^{(0)} \in \mathcal{C}$.
Set $B_{00} = 1$ and run for $n=1, 2, 3, \ldots$
\begin{itemize}
	\item Use the overlaps $\mat B$ to setup and solve \eqref{eqn:DIISlinsys}
		for the new set of \DIIS coefficients $\vec{c}$.
	\item Build the Fock matrix $\tilde{\mat{F}}^{(n)}$
		according to \eqref{eqn:DIISfock}.
		In this process skip coefficients $c_i$ below a certain threshold
		in order to save some matrix-vector products when
		$\tilde{\mat{F}}$ is contracted with trial vectors during
		the diagonalisation.
		If one entry $c_i$ is very large, say $>10$,
		then only keep this matrix in the expression.
		In all cases be sure to renormalise,
		such that all coefficients still sum to $1$.
	\item Diagonalise $\tilde{\mat{F}}$
		to obtain $\mat{C}_F^{(n)}$.
		Select $\mat{C}$ by the Aufbau principle.
	%
	\item Evaluate the Pulay error $\mat{e}^{(n)}$ \eqref{eqn:PulayError}
		and end the process if $\norm{\mat{e}^{(n)}}_\text{frob} \leq \epsilonconv$.
	%
	\item Calculate the new error overlaps
		$B_{0i} = \braket{\mat{e}^{(n)}}{\mat{e}^{(n-i)}}$.
		Note that only one row of $\mat{B}$ has to be calculated,
		since the others can be kept from the previous \SCF iteration.
		Drop coefficients, which are beyond the $m$ Fock matrices to keep
		and repeat the process thereafter.
\end{itemize}
The Pulay \DIIS scheme outlined here is rather general
and has been frequently applied to problems
other than solving the \HF or Kohn-Sham equations~\cite{Hamilton1986}.
For example it can be applied to accelerate
the convergence of fixed-point problems
as they occur for example in coupled-cluster theory,
see section \vref{sec:CC}.
In the optimisation community the \DIIS technique is known as
\newterm{Anderson acceleration}\todo{cite Nocedal Wright?} in this context.

Last but not least one should mention that a few
improvements to the \DIIS have been suggested recently.
This includes the \textbf{energy DIIS}~\cite{Kudin2002},
which effectively interpolates between densities resulting
from the \ODA accelerating \ODA convergence
whilst showing mathematically highly desirable properties.
\citet{Shepard2007} have suggested ways to improve the \DIIS
by reformulating the original problem into a least squares
or a linear least squares problem.
The augmented Roothaan-Hall \DIIS~\cite{Hoest2008} is another take
to yield a linear-scaling method with improved convergence.
The
\textbf{least-squares commutator in the Iterative Subspace}~\cite{Li2016} approach
can also be seen as a variant of the \DIIS
trying to correct some of its issues.

\subsection{Second-order \SCF algorithms}
\label{sec:SOSCF}

To conclude our review of a few common \SCF algorithms,
this section will briefly touch upon so-called \newterm{second-order \SCF algorithms}.
The general idea of such methods is
to consider not only the \HF equations \eqref{eqn:HFDiscreteEquations},
which effectively set the gradient of the minimisation problem \eqref{eqn:HFOptCoeff}
to zero,
but to also incorporate the Hessian of \eqref{eqn:HFOptCoeff} as well.
The aim is to both achieve faster convergence
and to furthermore ensure that convergence to a true \SCF minimum is guaranteed
and other stationary points on $\mathcal{C}$ are avoided.
Since forming the Hessian of \eqref{eqn:HFOptCoeff} can become rather expensive,
many approaches use approximate Hessians instead.
Typically it is also observed
that convergence only becomes quadratic
once the \SCF procedure is already reasonably
close to the minimiser of \eqref{eqn:HFOptCoeff}.

The first approach
which falls in this category
was the \textbf{quadratically-convergent \SCF}~(QCSCF)~\cite{Bacskay1981},
which minimised the \SCF energy by considering a CI expansion
based on the current set of \SCF orbitals
up to second order.
Similar to most methods discussed in this section,
QCSCF employs normalised basis functions $\{\varphi_\mu\}_{\mu\in\Ibas}$.
These have the advantage, that the occupied \SCF coefficients $\mat{C}^{(n)}$
can be alternatively parametrised~\cite{Helgaker2013} as
\[ \mat{C}_{(n)} = \mat{U}^{(n)} \mat{C}^{(0)} = \exp(-\mat{K}^{(n)}) \mat{C}^{(0)}, \]
where $\mat{U}^{(n)} \in \R^{\Nbas \times \Nbas}$ is a unitary rotation matrix
and $\mat{K}^{(n)}$ is an anti-Hermitian matrix.
Since the entries of  $\mat{K}^{(n)}$ effectively rotate the coefficients
$\mat{C}^{(0)}$,
this matrix is typically called \newterm{orbital rotation matrix}.
Further details about the orbital rotation ansatz for \SCF methods
can be found in \cite{Helgaker2013}.

\begin{figure}
	\centering
	\missingfigure{Geometric direct minimisation}
	\caption{Geometric direct minimisation}
	\label{fig:GeometricDirectMinimisation}
\end{figure}
Related to this \SCF ansatz is the \textbf{geometric direct minimisation}
method~\cite{Voorhis2002}.
This approach tries to directly
optimise the coefficients $\mat{C}_{(n)}$ in the sense of \eqref{eqn:HFOptCoeff}
on the Stiefel manifold $\mathcal{C}$.
The algorithm determines the energy gradient $\mat{g}$ at the current
coefficient set $\mat{C}^{(n)}$
and then follows the geodesic defined by $\mat{g}$
to find a new set $\mat{C}^{(n+1)}$.
The step size is determined by a Newton-Raphson-like step,
where the Hessian is constructed
approximately using the Broyden-Fletcher-Goldfarb-Shanno
update scheme.
The algorithm is sketched in figure \vref{fig:GeometricDirectMinimisation}
as well.

Last but not least one should mention the recent linear scaling
\SCF approach by \citet{Salek2007}
as well as the augmented Roothaan-Hall method~\cite{Hoest2008}
which can be seen as approximate second-order \SCF methods
based on a minimisation of the coefficients as well.

\subsection{Combinations of algorithms}
Ideally want to switch every now and then

%TODO Tynsh: Energy diis, scf benchmark



$\tilde{\mat{F}}^{(n)}$ gradient of $\mathcal{E}_D(\mat{D})$
wrt. $\mat{D}$~\cite{Lions1988,Cances2000}.
Can define~\cite{Lions1988,Cances2000}
\[
	\mat{C}^{(n+1)} = \arginf_{\mat{C} \in \mathcal{C}}
\left\{ \tr \tilde{\mat{F}}^{(n)} \mat{C} \tp{\mat{C}} \right\} \]
