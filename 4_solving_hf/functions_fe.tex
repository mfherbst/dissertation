\subsection{Finite-element based discretisation}
\label{sec:FE}
\newcommand{\Nquadc}{\ensuremath N_\text{quadc}}
\newcommand{\Ncell}{\ensuremath N_\text{cell}}

The Slater-type orbitals and Gaussian-type orbitals
we introduced in the previous sections
are examples for so-called atom-centred basis functions
or \newterm{atom-centred orbital}s~({\AO}s).
A different ansatz in many respects
are grid-based methods,
where the underlying idea is to partition three-dimensional real space
into smaller parts using a structured grid.
The problem is then solved by grid interpolation and numerical integration
instead of analytical evaluation of integrals.
The example we want to consider in this work, are \newterm{finite elements},
which are specifically constructed piecewise polynomials,
often employed in structural mechanics or engineering
for solving partial differential equations~\cite{Johnson1987}.
Multiple approaches for
solving the \HF problem or the Kohn-Sham equations
using finite-element-based discretisations
have been performed over the years%
~\cite{Tsuchida1995,Soler2002,Lehtovaara2009,Alizadegan2010,Avery2011PhD,Davydov2015,Lee2015,Boffi2016}.
This section only gives a short overview of the finite-element method
in the light of the \HF problem
with special focus on the things I have tried during my doctoral studies.
For more details the reader is referred
to the literature~\cite{Johnson1987,Grossmann1992,Bangerth2003,Brenner2008}.

\subsubsection{Construction of a \FE grid}
Compared to atom-centred basis functions,
where a discretisation based on the complete domain $\R^3$ is possible,
any grid-based method can only achieve this
on a subset $\Omega \subset \R^3$.
Typically $\Omega$ is taken to be open.
At the boundary $\partial\Omega$ one needs
to impose a boundary condition in order for the solution to be unique.
Ideally we would like to model the problem as close to the complete $\R^3$
as possible, \ie one would like to make the domain $\Omega$ as large as possible.
In practice one needs to make a compromise,
raising the question what kind of conditions to impose on the boundary.
We will discuss this in more detail later in the context
of solving the Poisson equation,
see equations \eqref{eqn:PoissonHartreeBCDirichlet} to \eqref{eqn:PoissonHartreeBCRobin}.
For now let us assume that $\Omega$ is large enough,
such that the \SCF orbitals are essentially zero on $\partial\Omega$
and we can impose a homogeneous Dirichlet boundary%
\footnote{This implies that the \HF eigenfunctions are forced
	to be exactly zero at the boundary $\partial\Omega$.}.
Using this approximation as well as the inner product
\[ \braket{\psi}{\chi}_1 \equiv \int_\Omega \psi(\vec{r}) \chi(\vec{r})  \D \vec{r} \]
the spin-free, real-valued \HF equations \eqref{eqn:HFequations} can be adapted to read
%
\begin{equation}
\label{eqn:HFequationsFE}
\begin{aligned}
	\Op{F}_{\Theta^0} \psi_i^0(\vec{r}) &= \varepsilon_i \psi_i^0(\vec{r}) && \vec{r} \in \Omega \\
	\psi_i^0(\vec{r}) &=0 &&\vec{r} \in \partial \Omega \\
	\text{where}\qquad \braket{\psi_i^0}{\psi_j^0}_1 &= \delta_{ij},
\end{aligned}
\end{equation}
for $\Theta^0 = (\psi_1^0, \psi_2^0, \ldots, \psi_{\Nelec}^0) \in \left( H^2(\Omega,\R) \right)^{\Nelec}$
being the minimiser to the \HF problem \eqref{eqn:HFMO}.
The corresponding sesquilinear form
\[ a_{\Theta^0}(\psi, \chi)
	\equiv \int_\Omega \psi(\vec{r}) \Op{F}_{\Theta^0} \chi(\vec{r}) \D \vec{r} \]
is defined in analogy to \eqref{eqn:SesquilinearFormFock}.
By partial integration it can be seen that this form
is defined on the domain $Q(\Op{F}_{\Theta^0}) = H^1(\Omega, \R)$.
In the \newterm{finite-element method}
the aim is to solve \eqref{eqn:HFequationsFE} variationally
in the sense of remark \vref{rem:DiscreteFormulation}
employing a hierarchy of approximation spaces $S_n$.
Such an attempt is of course only sensible
if such spaces are more and more accurate
approximations of the form domain $H^1(\Omega, \R)$
in the sense of \eqref{eqn:CondSubspaces}.

To outline the construction the spaces $S_n$,
let us consider at first a (fictitious) one-dimensional chemical system,
where $\Omega = (a,b)$ with $a,b \in \R$.
This domain can be subdivided into $\Ncell$ parts
\[ a = x_0 < x_1 < x_2 < \cdots < x_{\Ncell} = b, \]
which do not need to be of equal size~(see figure \vref{fig:FEoneDimLin}).
The open intervals $c_j = (x_j, x_{j+1})$ for $j=0, 1, \ldots, \Ncell-1$
are called grid \newterm{cells}
and the set $\mathcal{M}_h = \{c_j \, | \, j=0, 1, \ldots, \Ncell-1 \}$
of all grid cells is called a \newterm{mesh} or a \newterm{triangulation}.
In this set the index $h$ stands for the maximal size of a grid cell
defined as
\[ h \equiv \max_{c \in \mathcal{M}_h} \big| \max(c) - \min(c) \big|. \]
Using the vector space
\[
	\set{P}_k^1 \equiv \left\{ u \in C^\infty(\R) \,\middle|\, u(x) = \sum_{i=0}^k c_i x^i, c_i \in \R \right\}
\]
of all real polynomials of order at most $k$, we can define
\begin{equation}
	P_k(\mathcal{M}_h)
	\equiv \left\{ u \in C^0(\overline{\Omega}) \ \middle| \
	\forall c \in \mathcal{M}_h: \
	u|_{\bar{c}} \in \set{P}_k^1 \right\},
	\label{eqn:PiecewisePolynomialsOne}
\end{equation}
the set of piecewise polynomials of at most degree $k$.
The elements of $P_k(\mathcal{M}_h)$
are at least continuous on the complete domain $\Omega$
and inside the grid cells they are completely smooth.
It can be shown~\cite[Lemma 4.1]{Grossmann1992} that this implies
$P_k(\mathcal{M}_h) \subset H^1(\Omega, \R)$.
As $h \to 0$ such approximations become more exact,
which make $P_k(\mathcal{M}_h)$ the desired approximation
spaces of $H^1(\Omega, \R)$ for a one-dimensional problem.

\begin{figure}
	\centering
	\includeimage{4_solving_hf/fe_grid_linear}
	\caption[Examples of linear finite elements in one dimension]
	{
		A few examples of linear finite elements
		on a one-dimensional grid
		with unevenly spread grid points
		$x_0$ to $x_{N_\text{cell}}$.
		The cells are split from another
		by vertical dashed grey lines
		and the nodal points are indicated as by ticks
		on the $x$ axis.
	}
	\label{fig:FEoneDimLin}
\end{figure}
\begin{figure}
	\centering
	\includeimage{4_solving_hf/fe_grid_quadratic}
	\caption[Examples of quadratic finite elements in one dimension]
	{
		Same as figure \ref{fig:FEoneDimLin},
		but showing quadratic finite elements in one dimension.
	}
	\label{fig:FEoneDimQuad}
\end{figure}
\newcommand{\Ibash}{\mathcal{I}_{\text{bas},h}}
For representing $P_k(\mathcal{M}_h)$
one typically chooses a \newterm{Lagrange basis} $\{\varphi_\mu\}_{\mu\in \Ibash}$,
consisting of basis functions $\varphi_\mu$ with $0 \leq \mu \leq k \Ncell$,
defined as
\begin{align*}
	\varphi_\mu &\in P_k(\mathcal{M}_h), &
	\varphi_\mu(\tilde{x}_\nu) &= \delta_{\mu\nu}.
\end{align*}
In this expression%
\footnote{In equation \eqref{eqn:FEnodalPointsOneD} $\nu/k$ denotes integer division
without remainder.}
\begin{equation}
	\tilde{x}_\nu = x_{\nu/k} + \frac{\nu \!\! \mod k}{k} \left( x_{(\nu/k) +1} - x_{\nu/k} \right)
	\label{eqn:FEnodalPointsOneD}
\end{equation}
where $0 \leq \nu \leq k \Ncell$ are the \newterm{nodal points}.
An alternative term to refer to such
basis functions $\varphi_\mu$
is \textbf{finite element of order $k$},
the order $k$ being a reference to the maximal polynomial degree inside the cells $c$.
Examples for linear and quadratic finite elements are
illustrated in figures \ref{fig:FEoneDimLin} and \ref{fig:FEoneDimQuad}, respectively.
In each case the finite element functions
are either $1$ or $0$ at the nodal points and only
non-zero on a few cells, which are always direct neighbours.

In the following this construction is generalised
towards a three-dimensional domain $\Omega$.
In the most general form a mesh can be defined as
\begin{defn}[Mesh]
	Let $\Omega$ be a domain in $\R^3$.
	A mesh is a finite set $\mathcal{M}_h = {c_0, c_1, \ldots, c_{\Ncell-1}}$
	of $\Ncell$ domains $c_i$ with sufficiently regular boundary%
	\footnote{The boundary of the domains has to be Lipschitz.},
	such that
	\begin{align*}
		\overline{\Omega} &= \bigcup_{i=0}^{\Ncell-1} \overline{c_i}
		&&\text{and}
		&c_i \cap c_j = \emptyset \quad \forall i \neq j,
	\end{align*}
	\ie such that these domains completely partition $\Omega$.
	Furthermore we set for each $c \in \mathcal{M}_h$
	the \newterm{cell diameter}
	\[ h(c) = \max_{\vec{x},\vec{y} \in \bar{c}} \norm{\vec{x}-\vec{y}}_2 \]
	and call
	\[ h = \max_{c \in \mathcal{M}_h} h(c) \]
	the \newterm{mesh size}.
\end{defn}
\noindent
Usually one only considers so-called \newterm{affine} meshes.
\begin{defn}
	A mesh is called affine if a reference cell $c_0$
	and for each cell $c_i \in \mathcal{M}_h$
	affine transformations%
	\footnote{A transformation $\tau$ is affine iff $\tau(\vec{x}) = \mat{A}\vec{x} + \vec{b}$ with $\mat{A}$ being a transformation matrix and $\vec{b}$ being a constant shift vector.}
	$\tau_{c_i}$ exist,
	such that $\overline{c_i} = \tau_{c_i}(c_0)$.
\end{defn}
In other words a mesh is affine exactly if each
grid cell can be generated from the reference cell $c_0$
by a linear transformation followed by a shift.
This work only considers \textbf{cuboidal meshes},
where the reference cell $c_0 = [0,1]^3$ is the unit cube.
Similar to the construction of the grid cells,
the finite elements of order $k$ themselves
can be constructed by applying the affine transformations
to a set of template polynomials of the same order $k$.
Typically one defines these so-called
\newterm{shape function}s ${e_i}_i$
on the reference cell $c_0$
and uses the affine transformation $\tau_c$
to generate the finite elements on each cell
via $\tau_c(e_i)$.
%
\begin{figure}
	\centering
	\includeimage{4_solving_hf/shape_functions_1d}
	\caption[Examples for shape functions in one dimension]{
		The shape functions for polynomial orders $k=1$, $k=2$ and $k=3$
		in one dimension.
	}
	\label{fig:ShapeFunctionsOneD}
\end{figure}
\begin{figure}
	\centering
	\includeimage{4_solving_hf/shape_functions_2d}
	\caption[Examples for shape functions in two dimensions]
	{
		Examples for shape functions in two dimensions.
		The upper left shape function is for $k=1$,
		all others for $k=2$.
	}
	\label{fig:ShapeFunctionsTwoD}
\end{figure}
%
A few examples of shape functions in one and two dimensions are illustrated
in figure \ref{fig:ShapeFunctionsOneD} and \vref{fig:ShapeFunctionsTwoD}.
Notice that the shape functions in two dimensions
have been constructed as tensor products from the one-dimensional ones.
This construction is a special property of so-called $Q_k$ finite elements,
which are typically used in cuboidal meshes.
Via the tensor product ansatz $Q_k$ elements
in three and higher dimensions can be constructed as well.

Let us denote with $S_h$ the
space spanned by all $Q_k$ finite elements $\{\varphi_\mu\}_{\mu\in\Ibash}$
on a cuboidal mesh $\mathcal{M}_h$,
which have been constructed by applying
appropriate affine maps to a set of shape functions.
Even though we always have $S_h \subset H^1(\Omega, \R)$,
condition \eqref{eqn:CondSubspaces}
is \emph{not} necessarily satisfied as $h \to 0$.
In other words to ensure convergence
of the Ritz-Galerkin procedure in three dimensions
a vanishing mesh size is not sufficient.
The further required conditions are that the mesh
is \textbf{uniform} and \textbf{shape-regular}.
Roughly speaking, these conditions ensure that
all grid cells have the same size
and their shape is closer to being a ball than to being a needle.

If those conditions are taken into account,
an initial mesh can be refined more and more
until the \HF problem \eqref{eqn:HFequationsFE}
is solved up to the desired accuracy.
Furthermore, \textit{a priori} and \textit{a posteriori} error estimates
can be derived for regular meshes.
From these estimates, grid cells
which contribute most to the estimated error,
can be identified and refined
--- typically by splitting them into four equal-sized parts.
This refinement strategy%
\footnote{%
In practice there is a bit more to it,
since the meshes should stay uniform and shape-regular.
}
is called \newterm{adaptive refinement}.
Since \FE grids do not need to be equally spaced,
one may well start from a crude initial grid
and refine the grid adaptively
whilst solving the problem \eqref{eqn:HFequationsFE}
until the desired accuracy is achieved.
In this manner, the density of grid points
is lower where the electron density does not change a lot and is higher
where more grid cells are needed to represent the problem properly.
Notice that such a process can be automated as well.
Compared to a \cGTO-based discretisation
the finite-element method is therefore truly back box.

There are a couple of drawbacks to the finite-element method,
which should not go unmentioned.
First of all the tensor product construction of the $Q_k$ finite elements
in two and three dimensions
implies that
the three-dimensional \FE basis $\{\varphi_\mu\}_{\mu\in\Ibash}$
is similarly only non-zero in a few cells.
Therefore for a proper description of the \HF orbitals
on the full domain $\Omega$
\emph{many} finite elements are required,
typically $10^5$ to $10^6$~\cite{Davydov2015}.
All approaches for solving the numerical problems arising from a \FE
discretisation may therefore scale at most linearly to be feasible.
On the other hand the strict locality of the \FE basis functions
typically leads to very sparse matrices, such that this is usually
no problem if appropriate algorithms are devised.

Secondly, the electron-nuclear cusp tends to be an issue
for finite elements as well.
For most problems the cell-wise error contains
a term involving the gradient of the approximate solution.
See the Kelly error estimator~\cite{KellyError} for a very simple example.
The adaptive refinement process
will therefore place a larger amount of grid points
--- and thus a larger amount of finite-element basis functions ---
around the regions,
where the gradient of the approximated function is large.
Both the wave function as well as the \HF orbitals
have large gradients around the electron-nuclear cusp~\cite{Kato1957},
which is furthermore the only discontinuity of these functions~\cite{Kato1951}.
Even though for most applications the region around the core
is not very interesting from a chemical point of view,
it thus consumes a large number of \FE basis functions
for proper representation.
In the light of the previous paragraph this is not at all ideal.
As a remedy most \FE-based approaches to quantum chemistry employ pseudo-potentials
to represent the core region~\cite{Davydov2015},
leaving only the regions of smaller gradients
to be represented by finite elements.
Overall this significantly reduces the number of finite elements required,
but in turn introduces an empirical element ruining the black-box nature.
For simplicity we will not consider pseudo-potentials
in the remaining discussion about finite elements,
but our expressions can be easily modified to incorporate such.

\subsubsection{Evaluating the discretised Fock matrix}
Let us now consider a particular cuboidal mesh $\mathcal{M}_h$
at some stage during
the process of solving \eqref{eqn:HFequationsFE} up to desired accuracy.
On $\mathcal{M}_h$ we can construct
a set of $\Nbas$ $Q_k$ finite elements $\{\varphi_\mu\}_{\mu\in\Ibash}$
following the procedure outlined above.
In a completely analogous procedure to section \vref{sec:DiscreteHF}
we use these to discretise \eqref{eqn:HFequationsFE},
which results in the non-linear eigenproblem
\begin{equation}
	\begin{aligned}
		\matFnfull \mat{C}_F^{(n+1)} &= \mat{S} \mat{C}_F^{(n+1)} \mat{E}^{(n+1)} \\
		\tp{\mat{C}} \mat{S} \mat{C} &= \mat{I}_{\Nelec},
	\end{aligned}
	\label{eqn:SCFproblemFE}
\end{equation}
where
\begin{align*}
	\matFnfull &= \mat{T} + \mat{V}_0 + \matJnfull + \matKnfull, \\
	\mat{E}^{(n+1)}
	&= \text{diag}\left(\varepsilon_1^{(n+1)},
	\varepsilon_2^{(n+1)}, \ldots,
	\varepsilon_{\Norb}^{(n+1)}\right) \in \R^{\Norb \times \Norb}.
\end{align*}
By the Aufbau principle
the occupied coefficients $\mat{C}^{(n+1)} \in \R^{\Nbas \times \Nelec}$
are as usual the first $\Nelec$ columns of the full coefficient matrix
$\mat{C}_F^{(n+1)} \in \R^{\Nbas \times \Norb}$.
The individual terms of the Fock matrix and the overlap matrix
are given by expressions \eqref{eqn:Tbas} to \eqref{eqn:Sbas}
just with the integration over $\R^3$ replaced by
an integration over $\Omega$.
Naturally problem \eqref{eqn:SCFproblemFE} can in principle be solved
by the self-consistent field procedure of remark \vref{rem:SCFcoeff}.

For evaluating the Fock matrix $\matFnfull$,
let us first consider the terms
$\mat{T}$ and $\mat{V}_0$ as well as the overlap matrix $\mat{S}$.
This amounts to evaluating integrals
\begin{align*}
O_{\mu\nu} &=  \int_\Omega \varphi_\mu(\vec{r}) \, \Op{O} \, \varphi_\nu(\vec{r}) \D\vec{r}
&&\text{where} \quad \Op{O} = \Op{T}, \Op{V}_0 \text{ or } \id_{H^1(\Omega,\R)}.
\intertext{
With reference to the grid $\mathcal{M}_h$ we can write this as a
sum of cell contributions $O^c_{\mu\nu}$
}
O_{\mu\nu} &= \sum_{c\in\mathcal{M}_h} O^c_{\mu\nu}
&&\text{where} \quad O^c_{\mu\nu} = \int_c \varphi_\mu(\vec{r})\, \Op{O}\, \varphi_\nu(\vec{r}) \D\vec{r}.
\end{align*}
All of the operators $\Op{T}$, $\Op{V}_0$ or $\id$
are so-called \newterm{local operators},
which implies
\[ \forall \nu\in\Ibash: \quad \text{Supp}\left(\Op{O}\, \varphi_\nu\right) \subseteq \text{Supp}\left(\varphi_\nu\right), \]
where
\[ \text{Supp}(\chi) \equiv \left\{ \vec{r} \in \Omega \,\middle|\, \chi(\vec{r}) \neq 0 \right\} \]
denotes the \newterm{support} of a function $\chi$.
In other words $\left(\Op{O} \varphi_\nu\right)(\vec{r})$
is non-zero only if $\varphi_\nu(\vec{r})$ is non-zero,
which implies
\[ c \not\subset \text{Supp}(\varphi_\mu) \cap \text{Supp}(\varphi_\nu)
	\quad \Rightarrow \quad
	O^c_{\mu\nu} = 0.
\]
Conversely, to build the matrix $\mat{O}$
we only need to consider those elements $O_{\mu\nu}$
where $\text{Supp}(\varphi_\mu) \cap \text{Supp}(\varphi_\nu) \neq \emptyset$.
A particular $\varphi_\mu$ only has support in up to $2^3 = 8$ cells.
In each cell at most $(k+1)^3$ finite elements have support,
such that for a particular $\mu\in\Ibash$,
$O_{\mu\nu}$ can only be non-zero
for at most $8 (k+1)^3$ values of $\nu\in\Ibash$.
Using a clever ordering of the finite-element functions,
one can determine the set of finite elements $\varphi_\nu$,
which couple with a given element $\varphi_\mu$ immediately~\cite{CuthillMcKee},
such that $\mat{O}$ can be evaluated by only considering
a number of pairs $(\mu,\nu) \in \Ibash \times \Ibash$,
which scales linearly with the number of finite elements $\Nbas$.
Furthermore, the cell contributions $O^c_{\mu\nu}$ to $\mat{O}$
are independent from one another,
such that $\mat{O}$ can be determined
by an embarrassingly parallel MapReduce step,
\ie one first distributes the computation of the $O^c_{\mu\nu}$ in
batches over a number of workers~(Map)
and then accumulates the result of each in one place~(Reduce).

By construction for each finite element
$\varphi_\mu$ one can find a shape function $e_i$
such that on a particular cell $c \in \mathcal{M}_h$
\[
	\varphi_\mu\big|_c(\vec{r}) = e_i\left( \tau_c^{-1}(\vec{r}) \right).
\]
Let similarly $e_j$ be the shape function corresponding to $\varphi_\nu$
and further let $J_c(\vec{\xi})$ denote the Jacobian matrix
of the mapping $\vec{r} = \tau_c(\vec{\xi})$,
defined as
\[
	\forall \alpha,\beta \in \{x,y,z\}:
	\left( J_c (\vec{\xi}) \right)_{\alpha\beta} = \frac{\partial \left( \tau_c(\vec{\xi})  \right)_\alpha}{\partial \xi_\beta}.
\]
Then we can evaluate $O^c_{\mu\nu}$ as
\begin{equation}
\begin{aligned}
	O^c_{\mu\nu} &= \int_c \varphi_\mu(\vec{r})\, \Op{O} \, \varphi_\nu(\vec{r}) \D\vec{r} \\
	&= \int_c e_i\left( \tau_c^{-1}(\vec{r}) \right) \, \Op{O} \,
		e_j\left( \tau_c^{-1}(\vec{r}) \right) \\
		&= \int_{c_0} e_i(\vec{\xi}) \, \Op{O} \, e_j(\vec{\xi}) \det\left( J_c(\vec{\xi}) \right) \D \vec{\xi} \\
	&= \sum_{q=1}^{\Nquadc} e_i(\vec{\xi}_q) \, \Op{O} \, e_j(\vec{\xi}_q) \det\left( J_c(\vec{\xi}_q) \right) w_q,
\end{aligned}
\label{eqn:FEintegralLocal}
\end{equation}
where in the last step we introduced a quadrature for the integration
using $\Nquadc$ quadrature points
$\vec{\xi}_1, \vec{\xi}_2, \ldots, \vec{\xi}_{\Nquadc} \in c_0$
with quadrature weights $w_1, w_2, \ldots, w_{\Nquadc}$.
Provided that the operator $\Op{O}$ acting on $e_j$ returns a polynomial%
\footnote{This is the case for example for electrostatic Coulomb potentials
or the kinetic energy operator.},
the numerical integration in the last step of \eqref{eqn:FEintegralLocal}
can be made exact using large enough $\Nquadc$,
since $e_i$ and $e_j$ are only polynomials of order $k$.
Notice that the only quantities in the above sum depending
on the cell $c$ are the Jacobian and perhaps the operator $\Op{O}$.
In other words the quadrature itself only needs to be defined
with respect to the reference cell
and as a result the only required values of the shape function are those
at the quadrature points.
For a particular combination of quadrature and type of shape function,
this could for example be stored in a lookup-table
and used for the evaluation of many integrals.
Together with the guaranteed linear scaling
in the number of matrix elements $O_{\mu\nu}$
which need to be computed
as well as the embarrassingly parallel procedure
this makes the computation of the matrices $\mat{T}$,
$\mat{V}_0$ and $\mat{S}$ extremely efficient despite
the large number of basis functions $\Nbas$ for a
finite-element-based discretisation.
Since we know the sparsity pattern of
pairs of finite elements $(\varphi_\mu, \varphi_\nu)$
with common support
already \emph{before} any computation,
we can already set-up sensible storage schemes
for these matrices and thus avoid storing the known zeros.
This leads to linear scaling in storage with respect to the number
of finite elements as well.

For the evaluation of the Coulomb term $\matJ$
and the exchange term $\matK$ this na\"{i}ve approach does not work,
unfortunately, since neither $\Op{J}$ nor $\Op{K}$ are local operators.
Let us first treat the Coulomb term.
With reference to \eqref{eqn:OperatorCoulomb} and \eqref{eqn:Jbas}
we can write
for all $\mu, \nu \in \Ibash$
\begin{equation}
	\label{eqn:CoulombFE}
	J_{\mu\nu}\!\left[\mat{C}^{(n)}
		\left(\mat{C}^{(n)}\right)^\dagger\right]
	= \int_\Omega \varphi_\mu(\vec{r}_1)
	\left(\int_\Omega \frac{\rho^{(n)}(\vec{r}_2)}
		{ r_{12} }\D \vec{r}_2 \right)
	\varphi_\nu(\vec{r}_1) \D \vec{r}_1,
\end{equation}
where we introduced the discretised electron density
\begin{equation}
	\rho^{(n)}(\vec{r}) \equiv \sum_{i\in\Iocc} \abs{\sum_{\mu\in\Ibash} C^{(n)}_{\mu i} \varphi_\mu(\vec{r})}^2.
	\label{eqn:ElectronDensityFE}
\end{equation}
Following classical electrostatics~\cite{Jackson1999}
such an electron density gives rise to a potential $V^{(n)}_H(\vec{r})$
defined by a Poisson equation
\begin{equation}
\label{eqn:PoissionFE}
\begin{aligned}
	-\Delta V^{(n)}_H(\vec{r}) &= 4\pi \rho^{(n)}(\vec{r}) &&\vec{r} \in \Omega
\end{aligned}
\end{equation}
with suitable boundary condition on $\partial\Omega$
--- see discussion below.
In this case $V^{(n)}_H(\vec{r})$ is called the \newterm{Hartree potential} as well.
Assuming \eqref{eqn:PoissionFE} can be solved,
we can rewrite \eqref{eqn:CoulombFE} to give
\[
	J_{\mu\nu}\!\left[\mat{C}^{(n)}\left(\mat{C}^{(n)}\right)^\dagger\right]
	= \int_\Omega \varphi_\mu(\vec{r}_1) V^{(n)}_H(\vec{r}_1) \varphi_\nu(\vec{r}_1) \D \vec{r}_1.
\]
Since the Hatree potential $V^{(n)}_H$ is a local operator,
this latter integral can be evaluated in $\bigO(\Nbas)$
time and space using the cell-wise numerical integration scheme discussed above.

Solving the Poisson equation \eqref{eqn:PoissionFE}
is a well-understood problem in numerical mathematics
consisting of just solving a linear system of equations.
Using a combination of multigrid preconditioning~\cite{Hackbusch1985}
and a conjugate-gradient linear solver~\cite{Grossmann1992},
this problem can be solved in $\bigO(\Nbas)$.%
% TODO OPTIONAL
%\footnote{
%	The freely available
%	example program \url{http://dealii.org/developer/doxygen/deal.II/step_16.html}
%	distributed alongside
%	the \texttt{deal.ii} finite-element library~\cite{Arndt2017,Bangerth2007}
%	demonstrates this.
%}.

Let us now address the pending question,
which boundary condition to use in equations
\eqref{eqn:SCFproblemFE} as well as \eqref{eqn:PoissionFE}.
First we note, that for non-equally spaced grids,
one may take the cells close to the boundary to be rather large.
Given that both the Hartree potential $V_H^{(n)}$ as well as the
SCF orbitals decay asymptotically,
there is less and less change in their values to be expected.
In other words a coarse grid will be sufficient in these regions
and we can in fact take $\Omega$ quite large,
say $[-100,100]^3$ or even larger.
If we impose a homogeneous Dirichlet boundary on such a domain,
this is still a sensible choice for the \SCF orbitals,
but it can lead to issues for the Hartree potential, which only falls off as $-1/r$.
So even at distances of $10^6$ Bohr from the nucleus the potential
will is around $10^{-6}$.
The situation can be improved by approximating the density $\rho^{(n)}(\vec{r})$
at large distances by a point charge in the sense of a multipole expansion.
The solution to the Poisson equation in this case is trivial,
yielding the Coulomb potential
\[ V_P(\vec{r}) = \frac{\Nelec -1}{r}. \]
For the complete \SCF problem \eqref{eqn:SCFproblemFE}
one could similarly employ a multipole approximation
to yield an approximate solution at the boundary,
related to what we already did in remark
\vref{rem:PhysicalProperties}.
In both cases such approximate solutions can be enforced
using appropriate boundary conditions.
The options are
\begin{itemize}
	\item Dirichlet boundary conditions:
		\begin{align}
			V_H^{(n)}(\vec{r}) &= V_P(\vec{r}) \qquad \vec{r} \in \partial\Omega
			\label{eqn:PoissonHartreeBCDirichlet}
		\end{align}
	\item Neumann boundary conditions:
		\begin{align}
			\partial_n V_H^{(n)}(\vec{r}) &= \partial_n V_P(\vec{r}) \qquad \vec{r} \in \partial\Omega,
			\label{eqn:PoissonHartreeBCNeumann}
		\end{align}
		where $\partial_n V_H^{(n)}$ denotes the normal derivative at the boundary $\partial\Omega$.
	\item Robin boundary conditions:
		\begin{align}
			\label{eqn:PoissonHartreeBCRobin}
			\alpha(\vec{r}) \tilde{V}_H^{(n)}(\vec{r})
				&= \partial_n \tilde{V}_H(\vec{r}) \qquad \vec{r} \in \partial\Omega \\
		\intertext{where $\alpha(\vec{r})$ is determined from}
			\nonumber
			\alpha(\vec{r}) V_P(\vec{r}) &= \partial_n  V_P(\vec{r}) 
		\end{align}
\end{itemize}
For the Poisson equation Robin boundary conditions
\eqref{eqn:PoissonHartreeBCRobin} usually work best in practice,
since they enforce resemblance of the gradient and the value
of $V_P(\vec{r})$ at the same time.

In theory there is no reason why
one should use the same discretisation
for solving the Poisson equation \eqref{eqn:PoissionFE}
and for solving the \HF equations \eqref{eqn:HFequationsFE}.
Using different meshes is possible,
but leads to complications when projecting the Hartree potential $V_H^{(n)}$
onto the grid used for solving the \HF equations.
The use of different polynomial orders, for example,
has been investigated by \citet{Davydov2015}
in the context of the related Kohn-Sham equations.
Their results suggest to use twice the polynomial order for solving the Poisson equation
compared to the polynomials used for the \HF problem.
This can be rationalised by looking at the expression \eqref{eqn:ElectronDensityFE}
for the discretised density.
If $\varphi_\mu$ and $\varphi_\nu$ denote two $Q_k$ finite elements,
which are used for the discretisation of \eqref{eqn:HFequationsFE},
solving the Poisson equation \eqref{eqn:PoissionFE} requires the representation of
the density $\rho^{(n)}(\vec{r})$,
which consists of products $\varphi_\mu \cdot \varphi_\nu$.
These can only be represented exactly if at least $Q_{2k}$ elements
are used to discretise \eqref{eqn:PoissionFE}.

%
% -------------
%

Now we consider the exchange term.
Using \eqref{eqn:OperatorExchange} and \eqref{eqn:Kbas} we can
deduce an expression of the exchange matrix elements.
For all $\mu, \nu \in \Ibash$ we get
\begin{equation}
	\label{eqn:ExchangeFE}
	K^{(n)}_{\mu\nu} \equiv
	K_{\mu\nu}\!\left[\mat{C}^{(n)} \left(\mat{C}^{(n)}\right)^\dagger\right]
	%
	= -\int_\Omega \int_\Omega
		\varphi_\mu(\vec{r}_1) \frac{\gamma^{(n)}(\vec{r}_1, \vec{r}_2)}
		{ r_{12} }\varphi_\nu(\vec{r}_2) \D \vec{r}_2 \D \vec{r}_1,
\end{equation}
where we introduced the discretised one-particle reduced density matrix
\begin{equation}
	\gamma^{(n)}(\vec{r}_1, \vec{r}_2)
	\equiv \sum_{i\in\Iocc} \, \sum_{\mu,\nu\in\Ibash}
	C^{(n)}_{\mu i} \varphi_\mu(\vec{r}_1) \, C^{(n)}_{\nu i} \varphi_\nu(\vec{r}_2).
	\label{eqn:DensityMatrixFE}
\end{equation}
The double integral \eqref{eqn:ExchangeFE}
can be split into a sum of contributions from each grid cell pair
$(c,d) \in \left( \mathcal{M}_h \right)^2$
\begin{align*}
	K^{(n)}_{\mu\nu}
	&= -\sum_{c,d \in \mathcal{M}_h}
		\int_{c} \int_{d}
		\frac{\varphi_\mu(\vec{r}_1) \, \gamma^{(n)}(\vec{r}_1, \vec{r}_2)
			\,\varphi_\nu(\vec{r}_2)}
		{r_{12}} \D \vec{r}_2 \D \vec{r}_1 \\
	&= -\sum_{c,d \in \mathcal{M}_h}
		\int_{c_0} \int_{c_0}
		\frac{e_i(\vec{\xi}_1)\,
			\gamma^{(n)}\!\!\left(\tau_c(\vec{\xi}_1), \tau_d(\vec{\xi}_2)\right)
			\,e_j(\vec{\xi}_2)}
		{\norm{\tau_c(\vec{\xi}_1) - \tau_d(\vec{\xi}_2)}_2}
		\D \vec{\xi}_1 \D \vec{\xi}_2,
\end{align*}
where $e_i$ is the shape function corresponding to $\varphi_\mu$
and $e_j$ the one corresponding to $\varphi_\nu$.
Introducing two quadratures $\mathcal{Q}$ and $\mathcal{Q}'$
with quadrature points $\vec{\xi}_q$, $\vec{\xi}_r'$
and corresponding weights $w_q$ and $w_r'$ yields
\begin{align*}
	K^{(n)}_{\mu\nu}
		&\simeq -\sum_{c,d \in \mathcal{M}_h}
		\sum_{q=1}^{\Nquadc}
		\sum_{r=1}^{\Nquadc'}
		\frac{
			e_i(\vec{\xi}_q) \,
			\gamma^{(n)}\!\!\left(\tau_c(\vec{\xi}_q), \tau_d(\vec{\xi}'_r)\right)
			\,e_j(\vec{\xi}'_r)}
		{\norm{\tau_c(\vec{\xi}_q) - \tau_d(\vec{\xi}'_r)}_2}
		w_q w_r'.
\end{align*}
Notice that in contrast to \eqref{eqn:FEintegralLocal}
the right-hand side expression is \emph{not} exactly equal to the matrix element
$K^{(n)}_{\mu\nu}$,
since there are a couple of issues.
First of all there is the $1/r_{12}$ singularity,
which becomes $\norm{\tau_c(\vec{\xi}_q) - \tau_d(\vec{\xi}'_r)}_2^{-1}$
after the introduction of a numerical quadrature.
If the resulting matrix elements should be numerically meaningful
one needs to at least make sure that the quadrature
points $\vec{\xi}_q$ and $\vec{\xi}'_r$ are rather different for both
quadratures in order to avoid divergence.
More properly one needs to use a particular quadrature scheme,
suitable for integrating this singularity
or one needs a lot of quadrature points.
Already this aspect makes the construction of $\mat{K}$ more challenging
than the other matrices.
Additionally, the non-local nature of \HF exchange really comes
into play as well.
Unlike the previous Fock matrix terms
no immediate criterion for excluding some
pairs of finite element indices $(\mu, \nu)$ can be found
from the derived expression.

Each element $K^{(n)}_{\mu\nu}$
can be evaluated in $\bigO(k^6 \Nquadc \Nquadc' \Nelec)$
computational time.
To see this, let us first consider the evaluation of
$\gamma^{(n)}\!\left(\tau_c(\vec{\xi}_q), \tau_d(\vec{\xi}'_r)\right)$
on one cell pair $(c,d)$ and for one pair
of quadrature points $(\vec{\xi}_q, \vec{\xi}'_r)$.
With each cell $c$ only $\bigO(k^3)$ finite element functions share support.
Therefore,
there will be at most $\bigO(\Nelec (k^3)^2)$
terms in \eqref{eqn:DensityMatrixFE} which are non-zero.
In other words evaluating
$\gamma^{(n)}\!\left(\tau_c(\vec{\xi}_q), \tau_d(\vec{\xi}'_r)\right)$
takes $\bigO(\Nelec k^6)$,
which gives rise to a cost of $\bigO(k^6 \Nquadc \Nquadc' \Nelec)$
to evaluate a single element $K^{(n)}_{\mu\nu}$.
Overall the computational scaling for the na\"{i}ve
procedure outlined above is therefore $\bigO(\Nbas^2)$.
In storage we would expect the same quadratic scaling,
which is highly undesirable.

Theoretically one would expect
that this can be improved by considering some distance cut-off.
The physical justification for this is the exponential decay of the wave function,
which causes the density matrix $\gamma^{(n)}(\vec{r}_1, \vec{r}_2)$
to decay exponentially with $\norm{\vec{r}_1 - \vec{r}_2}_2$
as well,
provided that the discretisation is sensible for describing the problem.
In combination with the additional decay of $1/r_{12}$
it should be possible to \textit{a priori} exclude some index pairs $(\mu, \nu)$
and thus reduce the scaling.

\begin{figure}
	\centering
	\includeimage[width=\textwidth]{4_solving_hf/fock_fe}\\[-0.7em]
	\begin{tabularx}{0.67\textwidth}{XcX}
		a) $\mat{T} + \mat{V}_0 + \mat{J}$ & \hspace{0.27\textwidth} &
		\hspace{0.02\textwidth}b) $\mat{F}$
	\end{tabularx}
	\caption[Local terms and Fock matrix for a finite-element-based Hartree-Fock]
		{Structure of the local terms $\mat{T}+\mat{V}_0 + \mat{J}$
		and the Fock matrix $\mat{F}$
		for a finite-element-based \HF treatment
		of the beryllium atom in three dimensions
		in a small \FE basis of around 7000 basis functions.
		The Pulay error of the depicted Fock matrix
		is around $0.1$.
		The colouring depends on the absolute value of the entries
		with entries smaller than $10^{-10}$ being shown in white.
	}
	\label{fig:StructureFiniteElementFock}
\end{figure}

From the preliminary results I obtained,
I would not expect this attempt to be beneficial by its own
and that further strategies are required.
To illustrate this, consider figure \ref{fig:StructureFiniteElementFock},
where both the structure of the local terms $\mat{T}+\mat{V}_0 + \mat{J}$
and the structure of the complete Fock matrix
\[ \mat{F} = \mat{T}+\mat{V}_0 + \mat{J} + \mat{K} \]
is depicted for a closed-shell treatment of the beryllium atom.
Notice that we only used a rather small finite-element basis with
around 7000 finite element functions.
The colouring depends on the absolute value of the entries,
where white indicates values less than $10^{-10}$.
From the figure it is immediately visible
that the extra exchange term $\mat{K}$
seems to play a major role only in some blocks of the Fock matrix,
but not so much in others.
Whilst this probably allows for neglecting to evaluate some blocks of $\mat{K}$,
still a large amount of elements cannot be ignored.
Notice that even in the upper left corner of $\mat{F}$
some elements originating from $\mat{K}$ are larger than $10^{-10}$.
Compared to the structure of the local terms $\mat{T}+\mat{V}_0 + \mat{J}$,
even a clever reordering scheme
will probably not improve the sparsity structure very much.
I therefore believe it to be challenging if not impossible
to achieve an $\bigO(\Nbas)$-scaling in the number
of matrix entries $K^{(n)}_{\mu\nu}$,
which have to be computed.

An alternative strategy
is to avoid building and storing the matrix $\mat{K}$ at all
and instead recompute its elements whenever needed.
At first sight this does not seem to make the problem any easier,
yet it even appears to lead to more computations rather than less.
But as will be demonstrated for the example of the
application of the exchange matrix $\mat{K}$
to an arbitrary vector $\vec{x} \in \R^{\Nbas}$,
this is not always true.
The trick is usually that changing the order of summation and integration
often allows to compute the elements of a matrix like $\mat{K}$
in a more efficient way,
reducing the overall computational scaling to $\bigO(\Nbas)$.
For easier writing of the following algebra, let
\[ \psi_i^{(n)}(\vec{r}) = \sum_{\mu\in\Ibash} C^{(n)}_{\mu i} \varphi_\mu(\vec{r}) \]
such that
\[
	\gamma^{(n)}(\vec{r}_1, \vec{r}_2)
	= \sum_{i\in\Iocc} \psi_i^{(n)}(\vec{r}_1) \psi_i^{(n)}(\vec{r}_2)
\]
Using expression \eqref{eqn:ExchangeFE} we can write
the application of $\matK$ to a vector $\vec{x}$ as:
\begin{equation}
\label{eqn:ExchangeApply}
\begin{aligned}
	\left( \mat{K}^{(n)} \vec{x}  \right)_\mu
	&= -\sum_{\nu\in\Ibash} x_\nu K^{(n)}_{\mu\nu} \\
	&= -\sum_{\nu\in\Ibash}
		x_\nu
		\int_\Omega \int_\Omega
		\varphi_\mu(\vec{r}_1)
		\sum_{i\in\Iocc}
		\frac{
			\psi_i^{(n)}(\vec{r}_1) \psi_i^{(n)}(\vec{r}_2)
		}{r_{12}}
		\varphi_\nu(\vec{r}_2) \D \vec{r}_2 \D \vec{r}_1 \\
	&= -\sum_{i\in\Iocc}
		\int_\Omega
		\varphi_\mu(\vec{r}_1) \psi_i^{(n)}(\vec{r}_1)
		\left(
		\int_\Omega
		\sum_{\nu\in\Ibash}
			\frac{\psi_i^{(n)}(\vec{r}_2) \, x_\nu \varphi_\nu(\vec{r}_2)}
			{r_{12}}
		\D \vec{r}_2
		\right) \D \vec{r}_1 \\
	&= -\sum_{i\in\Iocc} \sum_{\kappa\in\Ibash}
		\int_\Omega
			\varphi_\mu(\vec{r}_1) \, C^{(n)}_{\kappa i} \varphi_\kappa(\vec{r}_1)
		\left(
		\int_\Omega \frac{\rho_{\vec{x},i}^{(n)}(\vec{r}_2)}{r_{12}} \D \vec{r}_2
		\right) \D \vec{r}_1,
\end{aligned}
\end{equation}
where we introduced the \newterm{exchange contraction densities}
\begin{equation}
	\label{eqn:ExchangeContrDens}
	\rho_{\vec{x},i}^{(n)}(\vec{r}) = \sum_{\nu,\lambda\in\Ibash}
	C^{(n)}_{\lambda i} \varphi_\lambda(\vec{r}_2) \, x_\nu \varphi_\nu(\vec{r}_2).
\end{equation}
For each $i\in\Iocc$ we can solve a Poisson equation in analogy to \eqref{eqn:PoissionFE}
\begin{equation}
\label{eqn:ExchangePoissonEqns}
\begin{aligned}
	- \Delta V_{\vec{x}, i}(\vec{r}) &= 4 \pi \rho_{\vec{x},i}^{(n)}(\vec{r})
		&& \vec{r} \in \Omega,
	%	\\ V^{(n)}_{\vec{x}, i}(\vec{r}) &= 0 && \vec{r} \in \partial\Omega
\end{aligned}
\end{equation}
where the boundary is fixed using one of \eqref{eqn:PoissonHartreeBCDirichlet}
to \eqref{eqn:PoissonHartreeBCRobin}.
Solving \eqref{eqn:ExchangePoissonEqns} defines
implicitly the \newterm{exchange contraction potentials} $V^{(n)}_{\vec{x}, i}$.
With these potentials \eqref{eqn:ExchangeApply} becomes
\begin{equation}
	\left( \mat{K}^{(n)} \vec{x}  \right)_\mu
	= -\sum_{i\in\Iocc} \sum_{\kappa\in\Ibash}
		\int_\Omega
		\varphi_\mu(\vec{r}) \, C^{(n)}_{\kappa i} \varphi_\kappa(\vec{r})
		V^{(n)}_{\vec{x}, i}(\vec{r}) \D \vec{r}.
	\label{eqn:ExchangeFromPotentials}
\end{equation}
Since $V^{(n)}_{\vec{x}, i}$ is a local operator,
the integrals in \eqref{eqn:ExchangeFromPotentials}
can be evaluated in $\bigO(k^3 \Nbas \Nquadc)$
once the potentials $V^{(n)}_{\vec{x}, i}$ are known.
Consequently, the complete expression \eqref{eqn:ExchangeFromPotentials}
can be computed in $\bigO(k^3 \Nquadc \Nelec \Nbas)$ time.

Assuming the same grid and polynomial order are used%
\footnote{Similar to the results of \cite{Davydov2015} for
the Poisson-solves in \eqref{eqn:PoissionFE},
it seems reasonable that one might need to go to \emph{twice}
the polynomial order for solving \eqref{eqn:ExchangePoissonEqns} as well.
This does not change our analysis, however,
since it just introduces a constant factor.}
for solving the Poisson equations \eqref{eqn:ExchangePoissonEqns}
and for discretising \eqref{eqn:HFequationsFE},
each of the exchange contraction densities
\eqref{eqn:ExchangeContrDens}
can be evaluated on the grid in $\bigO(k^3 \Nquadc \Nbas)$.
Solving each Poisson equation \eqref{eqn:ExchangePoissonEqns}
is again $\bigO(\Nbas)$,
such that overall obtaining the potentials $V^{(n)}_{\vec{x}, i}$
takes $\bigO(k^3 \Nquadc \Nelec \Nbas)$ time.
Even though computing the complete matrix $\mat{K}$ is quadratic
in $\Nbas$,
the application to a vector $\vec{x}$
can be done in $\bigO(\Nbas)$ computational time
with this scheme.
% TODO OPTIONALA
%  Clearify it and check whether its true.
% When it comes to storage,
% the most costly step is the discretisation
% of the Laplace operator $\Delta$ for solving the Poisson equations
% \eqref{eqn:ExchangePoissonEqns},
% which is after all still linear in $\Nbas$.

Many iterative diagonalisation algorithms,
like the Lanczos method~(see section \ref{sec:Arnoldi})
or Davidson's method~(see section \ref{sec:Davidson})
do not make explicit reference the elements of the matrix to be diagonalised.
Much rather they only require a way to perform the matrix-vector product.
In this manner obtaining a few selected eigenpairs
is possible \emph{without} having the complete matrix in memory.
In the context of the finite-element method
so-called \newterm{matrix-free methods}
have been developed recently~\cite{Kronbichler2012}.
These follow this strategy and avoid
building any finite-element-discretised form of any operator in memory.
This includes local operators like $\Op{T}$, $\Op{V}_0$ or $\Op{J}$ in our case.
If properly preconditioned iterative linear solvers are used,
such methods tend to perform better~\cite{Kronbichler2012}
than their traditional counterparts.
For reasons which will become more clear in the next chapter,
I will refer to efforts,
where storing matrix data is avoided
in favour of a matrix-vector contraction expression
as contraction-based methods instead.

For achieving a finite-element-based \HF
this seems to be a very promising ansatz as well.
Already lifting the requirement to build the Fock matrix $\mat{F}$
and intstead only employ
iterative diagonalisation algorithms
allows to formally reduce the scaling from $\bigO(\Nbas^2)$ to $\bigO(\Nbas)$
--- in both storage and time.
In our preliminary implementation of such a scheme,
we were, however, not able to implement such an ansatz successfully.
The biggest challenge is
the application of the exchange term $\mat{K}$.
Even though the formal scaling is linear in $\Nbas$,
one still needs to solve the Poisson equations \eqref{eqn:ExchangePoissonEqns}
a lot of times.
Already for each matrix-vector application
$\Nelec$ Poisson-solves are required.
For both Davidson and Lanczos one usually needs around 50 iterations,
with a couple of hundred matrix-vector products to be computed.
If we further need around 30 \SCF steps,
this alltogether makes some 1000 Poisson-solves
already for very small chemical systems.
The only way this can be achieved is by
proper approximations, proper preconditioning and
the caching of important intermediate results.
Beyond the multigrid preconditioning we already mentioned
in the context of the Coulomb term \eqref{eqn:PoissionFE},
other options for preconditioning include
an incomplete Choleski factorisation~\cite{Lin1999}
of the discretised form of $\Delta$
or potentially even an exact sparse inversion
using libraries like UMFPACK~\cite{Davis2004}
Even though such approaches are comparatively costly,
they are extremely good preconditioners up to the point
where solving the Poisson equations \eqref{eqn:ExchangePoissonEqns}
reduces to a few manageable matrix-vector products.
Since $\Delta$ is essentially equivalent to $\mat{T}$
and occurs both is the computation of $\matJ$
as well as the equations \eqref{eqn:ExchangePoissonEqns}
for applying $\matK$ to a vector, the costs could amortise overall.
If one accepts storing the discretised form of $\Delta$
as well as its sparse Choleski factorisation
or its exact sparse inverse~\cite{Davis2004},
one can reduce the costs even further,
since these quantities only need to be computed
once for each discretisation grid and not once per \SCF cycle.
Another problem in the proposed scheme for \FE-based \HF is numerical stability.
In integrals like \eqref{eqn:ExchangeFromPotentials}
the integrand is no longer a simple polynomial function,
but could have a rather complicated functional form,
such that higher quadrature orders than usual
could be necessary.
Furthermore, our experiments suggest,
that the Poisson equations \eqref{eqn:ExchangePoissonEqns}
need to be solved to high numerical accuracy
in order to result in meaningful eigenpairs
in the iterative diagonalisation method.
Due to these challenges a practically useful implementation
of the presented ansatz is still pending.

Despite these numerical challenges
finite-element-based \HF is a promising approach.
Once clear relationships between the quadrature orders
and the required accuracies between the iterative solvers are known,
the error is completely controlled by the discretisation itself.
An adaptively refined grid should therefore allow to solve
the \HF problem up to arbitrary precision.
In contrast to the {\cGTO} discretisation,
where a basis set has to be selected \emph{prior} to the calculation,
the finite-element method amounts to build
an appropriate basis as it solves the problem.
Even though no further results from \FE-based quantum chemistry
will be presented in this work,
many decisions that lead to the program and algorithm design
of \molsturm~(see chapter \vref{ch:Molsturm})
keep the numerical requirements of finite elements in mind as well.
