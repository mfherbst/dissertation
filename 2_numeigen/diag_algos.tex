\section{Diagonalisation algorithms}
\label{sec:DiagAlgos}

This section discusses the key ideas of a few algorithms for obtaining
approximations to the eigenpairs of a matrix $\mat{A}$.
Whilst the regime of quantum mechanics is a complex-valued Hilbert space,
in this thesis
we will only consider combinations of operators and discretisation bases
$\{\varphi_\mu\}_{\mu \in \Ibas}$,
which have the property that
\[ \forall \mu, \nu \in \Ibas: \quad a(\varphi_\mu, \varphi_\nu) \in \R. \]
As a result all matrices in \eqref{eqn:DiscretisedEigenproblem} will be real and symmetric.
In this section we will therefore only consider eigenproblems of the type
\[ \mat{A} \vec{u}_i = \lambda_i \vec{u}_i \]
where $\mat{A} \in \R^{\Nbas \times \Nbas}$,
$\lambda_i \in \R$ and $\vec{u}_i \in \R^{\Nbas}$.

\subsection{Direct methods}
One way this can be done are so-called \newterm{direct diagonalisation methods}.
These methods directly attempt to perform a transformation
\[ \rtp{\mat{O}} \mat{A} \mat{O} = \mat{L} = \diag(\lambda_1, \lambda_2, \ldots, \lambda_{\Nbas}), \]
where $\mat{O} \in \R^{\Nbas\times\Nbas}$ is an orthogonal matrix.
Typically this is performed in steps by inspecting the elements of $\mat{A}$
and gradually building both $\mat{O}$ as well as the matrix-matrix product
$\rtp{\mat{O}} \mat{A} \mat{O}$
using techniques such as Householder reflectors~\cite{Arbenz2010}
or Givens rotation~\cite{Arbenz2010}.
In either case this requires random access into the memory of $\mat{A}$.
This is one of the reasons why
direct methods are typically only suitable for either small matrices,
where $\Nbas$ is at most on the order of $1000$,
or matrices with special structure,
like being tridiagonal or banded.
Important dense diagonalisation methods include
QR factorisation~\cite{Arbenz2010,Saad2003}
as well as Cuppen's divide and conquer algorithm~\cite{Arbenz2010,Saad2003}.
They generally yield \emph{all} eigenvalues of a matrix at once
and little or no
extra work is required to additionally obtain all eigenvectors as well.

\subsection{Iterative diagonalisation methods}
In contrast to direct methods are \newterm{iterative diagonalisation methods},
where, abstractly speaking, the matrix $\mat{A}$ is probed iteratively
in order to gather more and more information
about the eigenpairs of interest.
The way this is done in practice is to repetitively form
the matrix-vector product
\[ \vec{y} = \mat{A}\vec{x} \]
of the problem matrix $\mat{A}$
with suitably constructed trial vectors $\vec{x}$.
The resulting vector $\vec{y}$ is then used
to improve upon the approximation for the eigenpairs as well as to
build the $\vec{x}$ for the next step.
This implies that random access into $\mat{A}$
is not required for such methods and thus specific storage schemes
or well-crafted algorithms going beyond a typical matrix-vector product
can be employed for forming $\vec{y}$.
The latter aspect is most important in the context of this thesis,
see chapter \vref{ch:NumSolveHF} and chapter \vref{ch:LazyMatrices}.

Iterative methods are typically not ideal for
computing many or all eigenpairs of a matrix $\mat{A}$,
which is in contrast to direct methods.
They do perform, however, much better than direct methods
if only few eigenpairs are desired
and it is well-known \emph{where} in the spectrum they are located.
Examples for cases where they tend to work well
is if one requires some of the largest eigenvalues of $\mat{A}$
or some of those, which are closest to an estimated value $\sigma$.
Some important iterative methods are sketched in the following sections.

\subsection{The power method}
\label{sec:Power}
The simplest iterative approach to obtain a single extremal eigenvalue
from a particular matrix $\mat{A}$ is the power method.
Starting from a random initial vector $\vec{v}^{(0)} \in \R^{\Nbas}$,
the algorithm only consists of applying the matrix $\mat{A}$
repetitively to the current vector, \ie
\begin{equation}
\begin{aligned}
	\vec{v}^{(1)} &= \mat{A} \vec{v}^{(0)}, \\
	\vec{v}^{(2)} &= \mat{A} \vec{v}^{(1)} = \mat{A}^2 \vec{v}^{(0)}, \\
	\vec{v}^{(3)} &= \mat{A} \vec{v}^{(2)} = \mat{A}^3 \vec{v}^{(0)}, \\
	&\vdots \\
	\vec{v}^{(j)} &= \mat{A} \vec{v}^{(j-1)} = \mat{A}^j \vec{v}^{(0)}.
\end{aligned}
	\label{eqn:PowerScheme}
\end{equation}
In each step we may compute an estimate $\theta^{(j)}$ for the eigenvalue by
the expression
\begin{equation}
	\theta^{(j)} = \rho_R\left(\vec{v}^{(j)}\right) \equiv
	\frac{\vec{v}^T \mat{A} \vec{v}}{\vec{v}^T \vec{v}},
	\label{eqn:RayleighQuotient}
\end{equation}
where $\rho_R$ is the \newterm{Rayleigh quotient},
the discretised version of \eqref{eqn:CourantFischer}.
In well-behaved cases this algorithm will find an approximation for
the largest eigenvalue in $\theta^{(i)}$
and an approximation for the corresponding eigenvector as
\[ \frac{\vec{v}^{(i)}}{\norm{\vec{v}^{(i)}}_2}. \]
To understand this
let us write $\vec{v}^{(0)}$ as an expansion in the exact
eigenvectors \linebreak $\vec{u}_1, \vec{u}_2, \ldots, \vec{u}_{\Nbas}$:
\begin{equation}
	\vec{v}^{(0)} = \sum_{i=1}^{\Nbas} \alpha_i \vec{u}_i = \alpha_{\Nbas} \vec{u}_{\Nbas} + \sum_{i=1}^{\Nbas-1} \alpha_i \vec{u}_i
	\label{eqn:Vexpansion}
\end{equation}
Without loss of generality%
\footnote{The case $\alpha_{\Nbas} = 0$ is handled by the limited precision
floating point arithmetic. After a single application of $\mat{A}$,
this is cured and we are back to the case we consider here.}
we can normalise $\vec{v}^{(0)}$ such that $\alpha_{\Nbas} = 1$.
Keeping this in mind the application of $\mat{A}$ to \eqref{eqn:Vexpansion}
results in
\[
	\mat{A} \vec{v}^{(0)} = \lambda_{\Nbas} \left( \vec{u}_{\Nbas} + \sum_{i=1}^{\Nbas}
	\frac{\lambda_k}{\lambda_{\Nbas}}
	\alpha_i \vec{u}_i \right).
\]
After the $j$-th step and subsequent normalisation we hence get
\[
	\frac{\vec{v}^{(j)}}{\norm{\vec{v}^{(j)}}_2} =
	\vec{u}_{\Nbas} + \bigO\left(
	\left( \frac{\lambda_{\Nbas-1}}{\lambda_{\Nbas}} \right)^j
	\right).
\]
Provided that $\abs{\lambda_{\Nbas-1}} \neq \abs{\lambda_{\Nbas}}$,
\ie that the largest eigenvalue (by magnitude) is single,
the iterate $\vec{v}^{(j)}$
therefore converges linearly against the eigenvector corresponding to this
largest eigenvalue $\lambda_{\Nbas}$.
Similarly $\theta^{(j)}$ converges against $\lambda_{\Nbas}$ in this case.

% TODO OPTIONAL
% Gerschgorin's circles

\subsection{Spectral transformations}
\label{sec:ShiftInvert}
With the power method at hand to obtain the largest eigenvalue,
the question is now,
how one could generalise this approach
for getting the lowest eigenvalue or even one directly from the middle
of the spectrum.
This is the purpose of so-called \newterm{spectral transformations}.

\begin{prop}
	\label{prop:Spectral}
	Given a symmetric matrix $\mat{A} \in \R^{\Nbas \times \Nbas}$,
	the following holds for each eigenpair
	$(\lambda_i, \vec{u}_i) \in \R \times \R^{\Nbas}$:
	\begin{enumerate}[label=(\alph*)]
		\item If $\mat{A}$ is invertible,
			$\vec{u}_i$ is an eigenvector of $\mat{A}^{-1}$ with eigenvalue
			$1 / \lambda_i$.
		\item For every $\sigma \in \R$, $\vec{u}_i$ is an eigenvector
			of the matrix $\mat{A} - \sigma \mat{I}_{\Nbas}$
			with eigenvalue $\lambda_i - \sigma$.
		\item If $\sigma \in \R$ is chosen such that
			$\mat{A} - \sigma \mat{I}_{\Nbas}$
			is invertible, than  $\vec{u}_i$ is an eigenvector
			of $\left( \mat{A} - \sigma \mat{I}_{\Nbas} \right)^{-1}$
			with eigenvalue $1 / (\lambda_i - \sigma)$.
	\end{enumerate}
	\begin{proof}
		All can be shown in a single line:
		\begin{enumerate}[label=(\alph*)]
			\item By definition $\mat{I}_{\Nbas} = \mat{A}^{-1} \mat{A}$
				and thus we have
				\[ \frac{1}{\lambda_i} \vec{u}_i
					= \frac{1}{\lambda_i} \mat{I}_{\Nbas} \vec{u}_i
					= \frac{1}{\lambda_i} \mat{A}^{-1} \mat{A} \vec{u}_i
					= \frac{1}{\lambda_i} \mat{A}^{-1} \lambda_i \vec{u}_i
					= \mat{A}^{-1} \vec{u}_i.
				\]
			\item Direct calculation shows
				\[ \left( \mat{A} - \sigma \mat{I}_{\Nbas} \right) \vec{u}_i
					= \mat{A} \vec{u}_i - \sigma \vec{u}_i
					= \lambda_i \vec{u}_i - \sigma \vec{u}_i
					= \left( \lambda_i - \sigma  \right) \vec{u}_i.
				\]
			\item Follows from (a) and (b).
		\end{enumerate}
	\end{proof}
\end{prop}

Proposition \ref{prop:Spectral} provides us with a toolbox for
changing the spectrum of a matrix in a desired way without changing its eigenvectors.
For example if we are interested in obtaining
the lowest eigenvalue of a matrix $\mat{A}$ using the power method,
we essentially only need to apply the scheme \eqref{eqn:PowerScheme}
to the inverse%
\footnote{Usually the inverse is computed iteratively as well, see discussion
	in section \ref{sec:GeneralisedEigenvalueProblem}.}
$\mat{A}^{-1}$ instead of $\mat{A}$.
Since the largest eigenvector of $\mat{A}^{-1}$ will be the smallest of $\mat{A}$,
this yields the required result.
Similarly by proposition \ref{prop:Spectral}(c)
we can tune the power method into a particular eigenvalue of interest
by guessing an appropriate shift $\sigma$.
Such spectral transformations are not restricted to the Power method,
since the equivalent effect can be achieved for other iterative
methods by passing them an appropriate matrix.

% TODO OPTIONAL
% Wilkinson's method

\subsection{Krylov subspace based methods}
\label{sec:Arnoldi}
Applying the power method
effectively amounts to generating a sequence of vectors
\begin{equation}
	\vec{v}, \, \mat{A} \vec{v}, \, \mat{A}^2 \vec{v}, \, \ldots,
	\label{eqn:PowerSequence}
\end{equation}
starting from an initial guess $\vec{v}$.
Given that the eigenvalue of largest magnitude of $\mat{A}$ is not degenerate,
the above sequence will approach the eigenvector corresponding to this
extremal eigenvalue~(see discussion in section \ref{sec:Power}).
In each iteration the power method does, however,
only keep a single of the vectors in \eqref{eqn:PowerSequence}
and throws away all information encoded in the history of the iteration.
An alternative approach which avoids doing so, is
to explicitly keep all vectors in \eqref{eqn:PowerSequence}.
This leads to the construction of a Krylov subspace~\cite{Arbenz2010}
\begin{equation}
	\mathcal{K}_j = \left\{\vec{v},\, \mat{A} \vec{v},\, \mat{A}^2 \vec{v},
	\ldots, \mat{A}^j \vec{v} \right\}.
	\label{eqn:KrylovSubspace}
\end{equation}
A large number of iterative methods both for solving eigenproblems
as well as linear problems
can be boiled down to an iterative construction of such a Krylov subspace.
Once or while it is found the original problem matrix $\mat{A}$
is projected onto this subspace, yielding $\tilde{\mat{A}} \in \R^{j \times j}$.

A key step to exploit the notion of Krylov subspaces
is the construction of an orthogonal basis for $\mathcal{K}_j$.
The Arnoldi algorithm~\cite{Arnoldi1951}
was devised to achieve this in a very efficient manner.
It exploits the fact that each vector \eqref{eqn:KrylovSubspace}
is related to its predecessor by an application of the problem matrix $\mat{A}$
to produce a simple recursion scheme minimising the work needed in each step.
Alongside with the construction of the basis the Arnoldi algorithm
\emph{at the same time} constructs $\tilde{\mat{A}}$,
the projection of $\mat{A}$ into the Krylov subspace.
Since $\tilde{\mat{A}}$ is both smaller and has a much simpler form%
\footnote{It is a so-called upper Hessenberg matrix,
	\ie only the upper triangle and a single subdiagonal in the lower
	triangle are non-zero.}
it can be diagonalised by a shifted QR factorisation,
a direct method.
This leads to the Arnoldi method for diagonalising
non-symmetric real matrices,
where one first uses the Arnoldi procedure to construct a sufficiently
good Krylov subspace%
\footnote{Some error estimates exist to judge this without performing
the next step of actually diagonalising the upper Hessenberg matrix.},
followed by a dense diagonalisation of the subspace matrix
to yield estimates for the eigenpairs.

A modification of the Arnoldi method for symmetric matrices $\mat{A}$
is the Lanczos method~\cite{Lanczos1950},
which implicitly exploits the fact,
that the subspace matrix has to be tridiagonal%
\footnote{Since $\mat{A}$ is symmetric, so is the subspace matrix
and a symmetric upper Hessenberg matrix is tridiagonal.}
already while constructing the Krylov subspace basis.

Even though the basic idea of Arnoldi and Lanczos are comparatively easy,
the implementation is still involved
due to a range of subtleties.
For example one can show~\cite{Arbenz2010} that the unmodified
Lanczos procedure
leads to an Arnoldi basis of poor numerical quality
with potentially linearly dependent vectors
roughly speaking
exactly when achieving convergence for an eigenpair.
Similarly both Arnoldi and Lanczos tend to have difficulties
when reporting multiplicities.
So if $\mat{A}$ has a triply degenerate eigenvalue $\lambda_i$
it can happen that these algorithms only find it twice,
even though the eigenspaces for $\lambda_{i-1}$
and $\lambda_{i+1}$,
\ie of the next smallest and next largest eigenvalue,
are completely described.
For such issues a large range of remedies have been proposed
over the years~\cite{Arbenz2010,Saad2003},
stressing the importance of Arnoldi methods in numerical linear algebra.
Examples include block modifications ---
where not a single vector, but a collection of vectors is iterated
in the Arnoldi procedure ---
or concepts such as implicit restart, deflation or locking.

\subsection{The Jacobi-Davidson algorithm}
\label{sec:Davidson}
Related to the Krylov subspace methods
sketched above, the Jacobi-Davidson approach
finds approximations to the eigenpairs of \eqref{eqn:DiscretisedEigenproblem}
by constructing suitable small subspaces
and solving the projected problem with dense methods.
The algorithm used for constructing the subspace is, however,
somewhat different%
\footnote{One should mention that similarities to the Lanczos procedure
	can be found, however. See \cite{Arbenz2010} for details.}.
Let us sketch the procedure for a matrix $\mat{A} \in \R^{\Nbas \times \Nbas}$,
where an approximation to the unknown, exact eigenpair $(\lambda_i, \vec{u}_i)$
is desired.
Following \citet{Davidson1975} we define the residual
\begin{equation}
	\vec{r}^{(j} = \mat{A} \vec{v}^{(j)} - \lambda_i \vec{v}^{(j)}
	\label{eqn:DavidsonResidual}
\end{equation}
of our current approximation $\vec{v}^{(j)}$ to the eigenvector $\vec{u}_i$.
In order to correct we employ the
\newterm{Jacobi orthogonal component correction},
\ie we want to add a vector $\vec{t}^{(j)} \perp \vec{v}^{(j)}$ to our
subspace, such that
\[
	\mat{A} \left(\vec{v}^{(j)} + \vec{t}^{(j)}\right) = \lambda_i \left(\vec{v}^{(j)} + \vec{t}^{(j)}\right).
\]
In other words we attempt to find the vector missing from the subspace,
such that it is able to span the exact solution,
which implies that it would be able to find it the next time
we solve the projected problem in the subspace.

Since $\lambda_i$ is in general not known at the $j$-th step of the algorithm,
$\vec{t}^{(j)}$ cannot be found exactly in practice.
Instead one employs the value returned by the Rayleigh quotient
\eqref{eqn:RayleighQuotient}
instead of $\lambda_i$ to make progress.
Incorporating the condition $\vec{t}^{(j)} \perp \vec{v}^{(j)}$ leads to the
\newterm{correction equation}
\begin{equation}
	\left(\mat{I}_{\Nbas} - \vec{v}^{(j)} \vec{v}^{(j)\,\ast} \right)
	\left(\mat{A} \vec{v}^{(j)} - \theta^{(j)} \mat{I}_{\Nbas}\right)
	\left(\mat{I}_{\Nbas} - \vec{v}^{(j)} \vec{v}^{(j)\,\ast}\right)
	\vec{t}^{(j)}
	= - \vec{r}^{(j)}.
	\label{eqn:JacobiDavidsonCorrectionEquation}
\end{equation}
Since a vector $\vec{t}^{(j)}$ is required
in each iteration, it needs to be solved many times.
Fortunately it does, however, not need to be solved exactly.
In practice one therefore
employs preconditioning techniques~\cite{Saad2003,Saad2011,Arbenz2010,Grossmann1992}
to speed up the performance of the iterative procedures
needed to solve \eqref{eqn:JacobiDavidsonCorrectionEquation}.
An alternative is to avoid using the \emph{exact} matrix $\mat{A}$
in favour of an approximation,
which makes solving \eqref{eqn:JacobiDavidsonCorrectionEquation} easier.
A combination of both is possible as well.

In the original paper \citet{Davidson1975}
assumed $\mat{A}$ to be diagonal-dominant
and thus only used the diagonal
\[ \mat{D}_A = \diag\left( A_{11} A_{22} \ldots A_{\Nbas,\Nbas} \right) \]
instead of the full $\mat{A}$
for the correction in \eqref{eqn:JacobiDavidsonCorrectionEquation}.
This leads to the identification
\[
	\vec{t}^{(j)} = \left( \mat{D}_A - \theta^{(j)} \mat{I}_{\Nbas} \right)^{-1} \vec{r}^{(j)},
\]
which is trivially computed elementwise as
\[
	\left(t^{(j)}\right)_i = \frac{\left(r^{(j)}\right)_i}{A_{ii} - \theta^{(j)}}.
\]
This is the basis of many diagonalisation routines
employed in quantum-chemistry packages nowadays.

\subsection{Generalised eigenvalue problems}
\label{sec:GeneralisedEigenvalueProblem}
Many eigenproblems occurring in quantum chemistry are in fact
not of the form \eqref{eqn:DiscretisedEigenproblem},
but are so-called \newterm{generalised eigenproblems},
\begin{equation}
	\mat{A} \vec{u}_i = \lambda_i \mat{S} \vec{u}_i
	\label{eqn:GeneralisedEigenproblem}
\end{equation}
where the right-hand side contains a
real, positive-definite matrix $\mat{S} \in \R^{\Nbas\times\Nbas}$ as well.
These typically arise because the
basis set $\{\varphi_\mu\}_{\mu \in \Ibas}$
used for the discretisation is not orthogonal.
For the typical basis sets employed to numerically solve the Hartree-Fock
problem,
one of the central aspects of this thesis,
this is the usual case~(see section \vref{sec:BasisTypes}).

One way to deal with \eqref{eqn:GeneralisedEigenproblem} is to reduce
it to a normal eigenproblem by formally inverting $\mat{S}$
and multiplying from the right-hand side.
This leads to
\[
	\left( \mat{S}^{-1} \mat{A} \right) \vec{u}_i = \lambda_i \vec{u}_i,
\]
a normal eigenproblem with the problem matrix $\mat{S}^{-1} \mat{A}$.
In iterative methods this amounts to replacing all occurrences
of the matrix-vector product $\mat{A}\vec{x}$ by the expression
\[ \vec{y} = \mat{S}^{-1} \mat{A} \vec{x}. \]
In this expression the vector $\vec{y}$ can be computed by solving the linear system
\[ \mat{S} \vec{y} = \mat{A} \vec{x} \]
using an inner preconditioned iterative method.
Whilst this would work,
this approach is hardly ever followed in practice.
The reason is that even for a real symmetric, positive-definite $\mat{S}$
and a real symmetric $\mat{A}$,
the matrix $\mat{S}^{-1} \mat{A}$ might not be symmetric,
which would imply that less advantageous solution algorithms need to be employed.

% TODO OPTIONAL
% 
%Another way is to exploit the fact that $\mat{S}$
%is symmetric and compute the Choleski factorisation~\cite{Arbenz2010} of $\mat{S}$,
%\ie to find lower-triangular matrix $\mat{L}$ which satisfy
%\[ \mat{S} = \mat{L} \rtp{\mat{L}}. \]
%Since $\mat{S}$ is non-singular, so is $\mat{L}$ and
%we can write \eqref{eqn:GeneralisedEigenproblem} as
%\[
%	\left( \mat{L}^{-1} \mat{A} \left(\rtp{\mat{L}}\right)^{-1} \right)
%		\left( \rtp{\mat{L}} \vec{u}_i \right)
%		= \lambda_i \left( \rtp{\mat{L}} \vec{u}_i \right).
%\]
%At first this does not look like a big improvement,
%but since triangular matrices are cheap to invert~\to do{cite}
%
%such that the
%symmetric matrix $\left( \mat{L}^{-1} \mat{A} \left(\rtp{\mat{L}}\right)^{-1} \right)$
%can be diagonalised using a normal diagonalisation algorithm.

An alternative approach to avoid this
is to try to modify the iterative procedures towards supporting the generalised
eigenproblems straight away.
By properly following the algebra one finds that
appropriate formulations of the algorithms in the setting of generalised
eigenproblems can be achieved by replacing the explicit or implicit
occurrences of the orthonormality condition
\[ \rtp{\vec{u}}_i \vec{u}_j = \delta_{ij} \]
by
\[  \rtp{\vec{u}}_i \vec{u}_j = S_{ij}. \]
In other words only the way the orthonormalisation of the subspace vectors
is performed as well as some expressions in which the identity matrix
occurs,
like in \eqref{eqn:JacobiDavidsonCorrectionEquation},
need to be changed.

Yet another option is to orthogonalise the basis before performing
the discretisation and thus avoid the appearence of the generalised
eigenproblem all together.


% TODO OPTIONAL
%\subsection{Other algorithms and further approaches}
%See \cite{Saad2003,Arbenz2010}
%
%Some generalisations based on forming polynomials over the matrix $\mat{A}$
%exist as well.
%For example Chebychev polynomial filtering works similar to this
%\to doil{Check}


% https://www.math.tu-berlin.de/fachgebiete_ag_modnumdiff/fg_modellierung_simulation_und_optimierung_in_natur_und_ingenieurswissenschaften/v_menue/mitarbeiter/prof_dr_reinhold_schneider/publikationen/


